import os
import sentencepiece as spm
from pathlib import Path

def test_turkish_tokenizer():
    """TÃ¼rkÃ§e tokenizer'Ä± kapsamlÄ± test et"""
    
    # Model dosyasÄ±nÄ± bul
    model_file = Path('tokenizer_train/turkish_tokenizer/turkish_tokenizer.model')
    
    if not model_file.exists():
        print("âŒ Model dosyasÄ± bulunamadÄ±!")
        print(f"   Beklenen konum: {model_file}")
        print("   Ã–nce tokenizer'Ä± eÄŸitin: python train_turkish_tokenizer.py")
        return
    
    # Model yÃ¼kle
    sp = spm.SentencePieceProcessor()
    sp.load(str(model_file))
    
    print("ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e SentencePiece Tokenizer Test")
    print("=" * 60)
    print(f"ğŸ“ Model: {model_file}")
    print(f"ğŸ“ Vocabulary boyutu: {sp.vocab_size()}")
    print(f"ğŸ”¤ UNK token ID: {sp.unk_id()}")
    print(f"ğŸ BOS token ID: {sp.bos_id()}")
    print(f"ğŸ”š EOS token ID: {sp.eos_id()}")
    print(f"ğŸ“„ PAD token ID: {sp.pad_id()}")
    print()
    
    # Ã–zel tokenlarÄ± kontrol et
    special_tokens = [
        '<s>', '</s>', '<pad>', '<mask>', '<cls>', '<sep>',
        '<turkish>', '<TR>', '<question>', '<answer>',
        '<thinking>', '</thinking>', '<reasoning>', '</reasoning>',
        '<|system|>', '<|user|>', '<|assistant|>',
        '<instruction>', '</instruction>', '<Ã§eviri>', '</Ã§eviri>'
    ]
    
    print("â­ Ã–zel Token Kontrol:")
    print("-" * 30)
    for token in special_tokens:
        token_id = sp.piece_to_id(token)
        if token_id != sp.unk_id():
            print(f"   âœ… {token} -> ID: {token_id}")
        else:
            print(f"   âŒ {token} -> BulunamadÄ±")
    print()
    
    # Temel test cÃ¼mleleri
    print("ğŸ” Temel Tokenizasyon Testleri:")
    print("-" * 40)
    
    test_sentences = [
        "Merhaba dÃ¼nya! TÃ¼rkÃ§e tokenizer nasÄ±l Ã§alÄ±ÅŸÄ±yor?",
        "TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r ve en bÃ¼yÃ¼k ÅŸehri Ä°stanbul'dur.",
        "Ã–ÄŸrencilerimiz sÄ±navlarÄ±nda Ã§ok baÅŸarÄ±lÄ± oldular.",
        "Evlerimizde, okullarÄ±mÄ±zda, iÅŸ yerlerimizde hep birlikte yaÅŸÄ±yoruz.",
        "Teknolojik geliÅŸmeler hayatÄ±mÄ±zÄ± kolaylaÅŸtÄ±rÄ±yor.",
        "ArkadaÅŸlarÄ±mÄ±zla birlikte gÃ¼zel anÄ±lar oluÅŸturuyoruz."
    ]
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"Test {i}: {sentence}")
        
        # Encode (tokenize)
        tokens = sp.encode(sentence, out_type=str)
        token_ids = sp.encode(sentence, out_type=int)
        
        print(f"   ğŸ“‹ Tokenlar: {tokens[:8]}{'...' if len(tokens) > 8 else ''}")
        print(f"   ğŸ”¢ Token sayÄ±sÄ±: {len(tokens)}")
        print(f"   ğŸ“Š SÄ±kÄ±ÅŸtÄ±rma oranÄ±: {len(sentence) / len(tokens):.2f} karakter/token")
        
        # Decode (detokenize)
        decoded = sp.decode(token_ids)
        is_correct = sentence == decoded
        print(f"   âœ… Decode {'baÅŸarÄ±lÄ±' if is_correct else 'baÅŸarÄ±sÄ±z'}")
        if not is_correct:
            print(f"   âš ï¸  Orijinal: {sentence}")
            print(f"   âš ï¸  Decode:   {decoded}")
        print()
    
    # TÃ¼rkÃ§e morfoloji testi
    print("ğŸ” TÃ¼rkÃ§e Morfoloji Testi:")
    print("-" * 30)
    
    morphology_examples = [
        ("ev", "evler", "evlerimiz", "evlerimizde"),
        ("Ã¶ÄŸretmen", "Ã¶ÄŸretmenler", "Ã¶ÄŸretmenlerimiz", "Ã¶ÄŸretmenlerimizin"),
        ("Ã§ocuk", "Ã§ocuklar", "Ã§ocuklarÄ±mÄ±z", "Ã§ocuklarÄ±mÄ±zÄ±n"),
        ("kitap", "kitaplar", "kitaplarÄ±mÄ±z", "kitaplarÄ±mÄ±zdan")
    ]
    
    for base, plural, possessive, locative in morphology_examples:
        base_tokens = sp.encode(base, out_type=str)
        plural_tokens = sp.encode(plural, out_type=str)
        poss_tokens = sp.encode(possessive, out_type=str)
        loc_tokens = sp.encode(locative, out_type=str)
        
        print(f"ğŸ“š {base} -> {plural} -> {possessive} -> {locative}")
        print(f"   Token sayÄ±larÄ±: {len(base_tokens)} -> {len(plural_tokens)} -> {len(poss_tokens)} -> {len(loc_tokens)}")
        print(f"   {base}: {base_tokens}")
        print(f"   {plural}: {plural_tokens}")
        print(f"   {possessive}: {poss_tokens}")
        print(f"   {locative}: {loc_tokens}")
        print()
    
    # Instruction Tuning token testi
    print("ğŸ¤– Instruction Tuning Token Testi:")
    print("-" * 40)
    
    instruction_example = """<|system|>Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e asistansÄ±n.</|system|>
<|user|><instruction>Bu metni Ã¶zetle:</instruction>
<input>TÃ¼rkiye Cumhuriyeti Anadolu ve DoÄŸu Trakya'da kurulmuÅŸ bir Ã¼lkedir.</input></|user|>
<|assistant|><thinking>KullanÄ±cÄ± Ã¶zet istiyor.</thinking>
<Ã¶zet>TÃ¼rkiye, Anadolu ve DoÄŸu Trakya'da kurulmuÅŸ cumhuriyettir.</Ã¶zet></|assistant|>"""
    
    print("Instruction Ã¶rneÄŸi:")
    print(instruction_example)
    print()
    
    inst_tokens = sp.encode(instruction_example, out_type=str)
    inst_ids = sp.encode(instruction_example, out_type=int)
    
    print(f"ğŸ“‹ Toplam token sayÄ±sÄ±: {len(inst_tokens)}")
    print(f"ğŸ“Š Ortalama token uzunluÄŸu: {len(instruction_example) / len(inst_tokens):.2f} karakter/token")
    
    # Decode kontrolÃ¼
    decoded_inst = sp.decode(inst_ids)
    print(f"âœ… Instruction decode {'baÅŸarÄ±lÄ±' if instruction_example == decoded_inst else 'baÅŸarÄ±sÄ±z'}")
    print()
    
    # Performans testi
    print("âš¡ Performans Testi:")
    print("-" * 20)
    
    import time
    
    test_text = "Bu bir performans testidir. " * 100  # 100 kez tekrarla
    
    # Encoding performance
    start_time = time.time()
    for _ in range(100):
        tokens = sp.encode(test_text, out_type=int)
    encoding_time = time.time() - start_time
    
    # Decoding performance
    start_time = time.time()
    for _ in range(100):
        decoded = sp.decode(tokens)
    decoding_time = time.time() - start_time
    
    print(f"ğŸ“ Test metni uzunluÄŸu: {len(test_text)} karakter")
    print(f"ğŸ”¢ Token sayÄ±sÄ±: {len(tokens)}")
    print(f"âš¡ Encoding: {encoding_time:.4f}s (100 iÅŸlem)")
    print(f"âš¡ Decoding: {decoding_time:.4f}s (100 iÅŸlem)")
    print(f"ğŸ“Š Encoding hÄ±zÄ±: {len(test_text) * 100 / encoding_time:.0f} karakter/saniye")
    print()
    
    print("ğŸ‰ TÃ¼m testler tamamlandÄ±!")
    print("âœ… Tokenizer baÅŸarÄ±yla Ã§alÄ±ÅŸÄ±yor!")

def interactive_test():
    """Ä°nteraktif test modu"""
    
    model_file = Path('turkish_tokenizer/turkish_tokenizer.model')
    
    if not model_file.exists():
        print("âŒ Model dosyasÄ± bulunamadÄ±!")
        return
    
    sp = spm.SentencePieceProcessor()
    sp.load(str(model_file))
    
    print("ğŸ”§ Ä°nteraktif Tokenizer Test Modu")
    print("=" * 40)
    print("Ã‡Ä±kmak iÃ§in 'exit' yazÄ±n")
    print()
    
    while True:
        try:
            text = input("ğŸ‡¹ğŸ‡· Test metni girin: ").strip()
            
            if text.lower() in ['exit', 'Ã§Ä±k', 'quit']:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                break
            
            if not text:
                continue
            
            # Tokenize
            tokens = sp.encode(text, out_type=str)
            token_ids = sp.encode(text, out_type=int)
            
            print(f"ğŸ“‹ Tokenlar: {tokens}")
            print(f"ğŸ”¢ Token IDs: {token_ids}")
            print(f"ğŸ“Š Token sayÄ±sÄ±: {len(tokens)}")
            print(f"ğŸ“ SÄ±kÄ±ÅŸtÄ±rma oranÄ±: {len(text) / len(tokens):.2f}")
            
            # Decode
            decoded = sp.decode(token_ids)
            print(f"âœ… Decode: {decoded}")
            print(f"ğŸ¯ DoÄŸru: {'Evet' if text == decoded else 'HayÄ±r'}")
            print("-" * 40)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
            break
        except Exception as e:
            print(f"âŒ Hata: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        interactive_test()
    else:
        test_turkish_tokenizer() 