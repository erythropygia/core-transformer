import os
import json
import time
import logging
import argparse
import sentencepiece as spm
from pathlib import Path
from typing import List, Dict, Optional

# Logging ayarlarƒ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tokenizer_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TurkishTokenizerTrainer:
    def __init__(self, 
                 vocab_size: int = 32000,
                 model_type: str = 'bpe',
                 character_coverage: float = 0.9995,
                 output_dir: str = 'turkish_tokenizer'):
        """
        T√ºrk√ße tokenizer eƒüiticisi
        
        Args:
            vocab_size: Vocabulary boyutu (32K T√ºrk√ße i√ßin optimal)
            model_type: Model tipi ('bpe', 'unigram', 'word', 'char')
            character_coverage: Karakter kapsamƒ± (T√ºrk√ße i√ßin 0.9995)
            output_dir: √áƒ±ktƒ± dizini
        """
        self.vocab_size = vocab_size
        self.model_type = model_type
        self.character_coverage = character_coverage
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # T√ºrk√ße i√ßin optimal SentencePiece parametreleri
        self.sp_params = {
            'model_type': model_type,
            'vocab_size': vocab_size,
            'character_coverage': character_coverage,
            'normalization_rule_name': 'nfkc',  # T√ºrk√ße karakterler i√ßin
            'remove_extra_whitespaces': True,
            'split_by_unicode_script': True,    # T√ºrk√ße morfoloji i√ßin √∂nemli
            'split_by_whitespace': True,
            'split_by_number': True,
            'split_digits': True,
            'treat_whitespace_as_suffix': False,
            'allow_whitespace_only_pieces': True,
            'max_sentence_length': 8192,
            'shuffle_input_sentence': True,
            'input_sentence_size': 10000000,    # 10M c√ºmle
            'seed_sentencepiece_size': 1000000,
            'shrinking_factor': 0.75,
            'num_threads': os.cpu_count() or 4,
            'max_sentencepiece_length': 16,
            'num_sub_iterations': 2,
        }
        
        # T√ºrk√ße √∂zel tokenlar
        self.special_tokens = [
            '<s>', '</s>', '<pad>',                     # Temel tokenlar (unk otomatik)
            '<mask>', '<cls>', '<sep>',                # BERT-style tokenlar  
            '<turkish>', '<TR>',                       # Dil tokenlarƒ±
            '<question>', '<answer>',                  # QA tokenlarƒ±
            '<news>', '<social>', '<formal>',          # Domain tokenlarƒ±
            '<thinking>', '</thinking>',                # Think tokenlarƒ± (modern AI i√ßin)
            '<thought>', '</thought>',                  # Alternatif d√º≈ü√ºnce tokenlarƒ±
            '<reasoning>', '</reasoning>',              # Mantƒ±k y√ºr√ºtme
            '<analysis>', '</analysis>',                # Analiz b√∂l√ºmleri
            '<|endoftext|>', '<|startoftext|>',        # End of text ve start of text tokenlarƒ±
            
            # Instruction Tuning Tokenlarƒ±
            '<|system|>', '<|user|>', '<|assistant|>', # Chat rolleri (ChatML style)
            '<|im_start|>', '<|im_end|>',              # Instruction message markers
            '<instruction>', '</instruction>',          # Instruction wrapper
            '<input>', '</input>',                     # Input wrapper
            '<output>', '</output>',                   # Output wrapper
            '<context>', '</context>',                 # Context information
            
            # Multi-turn Conversation
            '<turn>', '</turn>',                       # Conversation turn markers
            '<conversation>', '</conversation>',        # Conversation wrapper
            '<history>', '</history>',                 # Chat history
            
            # Function Calling & Tool Use
            '<function_call>', '</function_call>',     # Function call wrapper
            '<tool_use>', '</tool_use>',               # Tool usage marker
            '<json>', '</json>',                       # JSON structured data
            '<code>', '</code>',                       # Code blocks
            
            # Safety & Control
            '<safe>', '<unsafe>',                      # Safety markers
            '<warning>', '</warning>',                 # Warning wrapper
            '<filter>', '</filter>',                   # Content filtering
            
            # Task-Specific (T√ºrk√ße i√ßin)
            '<√ßeviri>', '</√ßeviri>',                   # Translation task
            '<√∂zet>', '</√∂zet>',                       # Summarization task
            '<soru>', '</soru>',                       # Question marker
            '<cevap>', '</cevap>',                     # Answer marker
            
            # Multi-modal (gelecek i√ßin)
            '<image>', '</image>',                     # Image content
            '<audio>', '</audio>',                     # Audio content
            '<video>', '</video>',                     # Video content
        ]

    def get_corpus_file(self) -> str:
        """
        Mevcut OSCAR corpus dosyasƒ±nƒ± kontrol et ve d√∂nd√ºr
        
        Returns:
            Corpus dosyasƒ±nƒ±n yolu
        """
        # Script'in bulunduƒüu dizinden data klas√∂r√ºne eri≈üim
        script_dir = Path(__file__).parent
        corpus_file = script_dir / 'data' / 'oscar_turkish.txt'
        
        if not corpus_file.exists():
            raise FileNotFoundError(f"OSCAR corpus dosyasƒ± bulunamadƒ±: {corpus_file}")
        
        file_size = corpus_file.stat().st_size / (1024*1024*1024)  # GB
        logger.info(f"‚úÖ OSCAR corpus bulundu: {corpus_file}")
        logger.info(f"üìä Dosya boyutu: {file_size:.2f} GB")
        
        return str(corpus_file)

    def preprocess_corpus(self, corpus_file: str) -> str:
        """
        Corpus dosyasƒ±nƒ± √∂ni≈ülemden ge√ßir
        
        Args:
            corpus_file: Girdi corpus dosyasƒ±
            
        Returns:
            √ñni≈ülenmi≈ü corpus dosyasƒ±nƒ±n yolu
        """
        logger.info("üîÑ Corpus √∂ni≈üleme ba≈ülƒ±yor...")
        
        processed_file = self.output_dir / 'processed_corpus.txt'
        total_lines = 0
        valid_lines = 0
        
        with open(processed_file, 'w', encoding='utf-8') as outfile:
            with open(corpus_file, 'r', encoding='utf-8') as infile:
                for line in infile:
                    total_lines += 1
                    line = line.strip()
                    
                    # Basit filtreleme
                    if self._is_valid_text(line):
                        outfile.write(line + '\n')
                        valid_lines += 1
                    
                    if total_lines % 100000 == 0:
                        logger.info(f"  üìù {total_lines} satƒ±r i≈ülendi, {valid_lines} ge√ßerli satƒ±r")
        
        logger.info(f"‚úÖ √ñni≈üleme tamamlandƒ±: {valid_lines}/{total_lines} satƒ±r korundu")
        return str(processed_file)

    def _is_valid_text(self, text: str) -> bool:
        """Basit metin kontrol√º"""
        # √áok kƒ±sa metinleri filtrele
        if len(text.strip()) < 10:
            return False
        
        # √áok uzun metinleri filtrele
        if len(text) > 1000:
            return False
        
        # Sayƒ±sal i√ßerik kontrol√º
        digit_ratio = sum(c.isdigit() for c in text) / len(text)
        if digit_ratio > 0.5:
            return False
        
        return True

    def train_tokenizer(self, corpus_file: str) -> Dict[str, str]:
        """
        SentencePiece tokenizer eƒüit
        
        Args:
            corpus_file: Eƒüitim korpusu dosyasƒ±
            
        Returns:
            Eƒüitilen modelin dosya yollarƒ±
        """
        logger.info("üöÄ SentencePiece tokenizer eƒüitimi ba≈ülƒ±yor...")
        
        model_prefix = str(self.output_dir / 'turkish_tokenizer')
        
        # SentencePiece eƒüitim parametrelerini hazƒ±rla
        sp_params = self.sp_params.copy()
        sp_params.update({
            'input': corpus_file,
            'model_prefix': model_prefix,
            'user_defined_symbols': self.special_tokens,
        })
        
        # Parametreleri logla
        logger.info("üìã Eƒüitim parametreleri:")
        for key, value in sp_params.items():
            logger.info(f"  {key}: {value}")
        
        try:
            # SentencePiece modelini eƒüit
            logger.info("üî• SentencePiece eƒüitimi ba≈ülƒ±yor...")
            start_time = time.time()
            
            spm.SentencePieceTrainer.train(**sp_params)
            
            training_time = time.time() - start_time
            logger.info(f"‚úÖ Tokenizer eƒüitimi tamamlandƒ±! S√ºre: {training_time:.2f}s")
            
            # √áƒ±ktƒ± dosyalarƒ±nƒ± kontrol et
            model_file = f"{model_prefix}.model"
            vocab_file = f"{model_prefix}.vocab"
            
            if os.path.exists(model_file) and os.path.exists(vocab_file):
                logger.info(f"üìÅ Model dosyasƒ±: {model_file}")
                logger.info(f"üìÅ Vocabulary dosyasƒ±: {vocab_file}")
                
                # Model istatistiklerini g√∂ster
                self._show_model_stats(model_file)
                
                return {
                    'model': model_file,
                    'vocab': vocab_file,
                    'training_time': training_time
                }
            else:
                raise FileNotFoundError("Model dosyalarƒ± olu≈üturulamadƒ±")
                
        except Exception as e:
            logger.error(f"‚ùå Tokenizer eƒüitimi ba≈üarƒ±sƒ±z: {e}")
            raise

    def _show_model_stats(self, model_file: str) -> None:
        """Model istatistiklerini g√∂ster"""
        try:
            sp = spm.SentencePieceProcessor()
            sp.load(model_file)
            
            logger.info("üìä Model ƒ∞statistikleri:")
            logger.info(f"  üìù Vocabulary boyutu: {sp.vocab_size()}")
            logger.info(f"  üî§ Model tipi: {self.model_type.upper()}")
            logger.info(f"  üéØ Karakter kapsamƒ±: {self.character_coverage}")
            
            # √ñzel tokenlarƒ± kontrol et
            special_tokens_found = []
            for token in self.special_tokens:
                if sp.piece_to_id(token) != sp.unk_id():
                    special_tokens_found.append(token)
            
            logger.info(f"  ‚≠ê √ñzel tokenlar: {len(special_tokens_found)}/{len(self.special_tokens)}")
            logger.info(f"     {', '.join(special_tokens_found[:5])}...")
            
        except Exception as e:
            logger.warning(f"Model istatistikleri g√∂sterilemedi: {e}")

    def run_training(self) -> Dict[str, str]:
        """Tam eƒüitim s√ºrecini √ßalƒ±≈ütƒ±r"""
        logger.info("üöÄ T√ºrk√ße Tokenizer Eƒüitimi Ba≈ülƒ±yor!")
        logger.info("=" * 50)
        
        try:
            # 1. Corpus dosyasƒ±nƒ± kontrol et
            logger.info("üìä 1. A≈üama: Corpus dosyasƒ± kontrol ediliyor...")
            corpus_file = self.get_corpus_file()
            
            # 2. Corpus'u √∂ni≈üle
            logger.info("üîÑ 2. A≈üama: Corpus √∂ni≈üleme...")
            processed_corpus = self.preprocess_corpus(corpus_file)
            
            # 3. Tokenizer'ƒ± eƒüit
            logger.info("üéì 3. A≈üama: Tokenizer eƒüitimi...")
            result = self.train_tokenizer(processed_corpus)
            
            # 4. √ñzet bilgi
            logger.info("üéâ Eƒüitim Tamamlandƒ±!")
            logger.info("=" * 50)
            logger.info(f"üìÅ Model dosyasƒ±: {result['model']}")
            logger.info(f"üìÅ Vocabulary dosyasƒ±: {result['vocab']}")
            logger.info(f"‚è±Ô∏è  Toplam eƒüitim s√ºresi: {result['training_time']:.2f}s")
            logger.info("")
            logger.info("üöÄ Tokenizer'ƒ±nƒ±z hazƒ±r! Test etmek i√ßin:")
            logger.info("   python test_turkish_tokenizer.py")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Eƒüitim ba≈üarƒ±sƒ±z: {e}")
            raise

def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(description="T√ºrk√ße SentencePiece Tokenizer Eƒüitici")
    parser.add_argument('--vocab-size', type=int, default=32000, 
                       help='Vocabulary boyutu (varsayƒ±lan: 32000)')
    parser.add_argument('--model-type', choices=['bpe', 'unigram', 'word', 'char'], 
                       default='bpe', help='Model tipi (varsayƒ±lan: bpe)')
    parser.add_argument('--coverage', type=float, default=0.9995,
                       help='Karakter kapsamƒ± (varsayƒ±lan: 0.9995)')
    parser.add_argument('--output-dir', type=str, default='turkish_tokenizer',
                       help='√áƒ±ktƒ± dizini (varsayƒ±lan: turkish_tokenizer)')
    
    args = parser.parse_args()
    
    # Trainer'ƒ± ba≈ülat
    trainer = TurkishTokenizerTrainer(
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.coverage,
        output_dir=args.output_dir
    )
    
    # Eƒüitimi √ßalƒ±≈ütƒ±r
    trainer.run_training()

if __name__ == "__main__":
    main() 