import os
import json
import time
import logging
import argparse
import sentencepiece as spm
from pathlib import Path
from typing import List, Dict, Optional

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tokenizer_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TurkishTokenizerTrainer:
    def __init__(self, 
                 vocab_size: int = 32000,
                 model_type: str = 'bpe',
                 character_coverage: float = 0.9995,
                 output_dir: str = 'turkish_tokenizer'):
        """
        130M+ parametre modeller iÃ§in optimize edilmiÅŸ TÃ¼rkÃ§e tokenizer eÄŸiticisi
        
        Args:
            vocab_size: Vocabulary boyutu (32K, 130M model iÃ§in optimal)
            model_type: Model tipi ('bpe' Ã¶nerilen)
            character_coverage: Karakter kapsamÄ± (TÃ¼rkÃ§e iÃ§in 0.9995)
            output_dir: Ã‡Ä±ktÄ± dizini
        """
        self.vocab_size = vocab_size
        self.model_type = model_type
        self.character_coverage = character_coverage
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ SentencePiece parametreleri
        self.sp_params = {
            'model_type': model_type,
            'vocab_size': vocab_size,
            'character_coverage': character_coverage,
            'normalization_rule_name': 'nfkc',  # TÃ¼rkÃ§e karakterler iÃ§in
            'remove_extra_whitespaces': True,
            'split_by_unicode_script': True,    # TÃ¼rkÃ§e morfoloji iÃ§in kritik
            'split_by_whitespace': True,
            'split_by_number': True,
            'split_digits': True,
            'treat_whitespace_as_suffix': False,
            'allow_whitespace_only_pieces': True,
            'max_sentence_length': 8192,
            'shuffle_input_sentence': True,
            'input_sentence_size': 10000000,    # 10M cÃ¼mle limit
            'seed_sentencepiece_size': 1000000,
            'shrinking_factor': 0.75,
            'num_threads': os.cpu_count() or 4,
            'max_sentencepiece_length': 16,
            'num_sub_iterations': 2,
            # Token ID'leri aÃ§Ä±k ÅŸekilde tanÄ±mla
            'unk_id': 0,    # <unk>
            'bos_id': 1,    # <s> 
            'eos_id': 2,    # </s>
            'pad_id': 3,    # <pad>
        }
        
        # Minimal ama gelecek odaklÄ± Ã¶zel token seti
        # Total: 16 Ã¶zel token (vocab'un %0.05'i)
        self.special_tokens = [
            # === Core Functionality (5 tokens) ===
            '<mask>',               # Masked Language Modeling
            '<turkish>',            # Language identifier
            '<instruction>',        # Instruction tuning baÅŸlangÄ±Ã§
            '</instruction>',       # Instruction tuning bitiÅŸ
            '<context>',           # Context marker
            
            # === Chat & Dialog (4 tokens) ===
            '<|system|>',          # System prompt
            '<|user|>',            # User message  
            '<|assistant|>',       # Assistant response
            '<|end|>',             # Turn end marker
            
            # === Safety & Control (2 tokens) ===
            '<safe>',              # Safe content marker
            '<unsafe>',            # Unsafe content marker
            
            # === Future Reserve (5 tokens) ===
            '<reserved1>',         # Gelecek Ã¶zellikler iÃ§in
            '<reserved2>',         # Function calling, tools vs.
            '<reserved3>',         # Multi-modal extensions
            '<reserved4>',         # Task-specific needs
            '<reserved5>',         # Emergency reserve
        ]

    def get_corpus_file(self) -> str:
        """
        Mevcut OSCAR corpus dosyasÄ±nÄ± kontrol et ve dÃ¶ndÃ¼r
        
        Returns:
            Corpus dosyasÄ±nÄ±n yolu
        """
        script_dir = Path(__file__).parent
        corpus_file = script_dir / 'data' / 'oscar_turkish.txt'
        
        if not corpus_file.exists():
            raise FileNotFoundError(f"OSCAR corpus dosyasÄ± bulunamadÄ±: {corpus_file}")
        
        file_size = corpus_file.stat().st_size / (1024*1024*1024)  # GB
        logger.info(f"âœ… OSCAR corpus bulundu: {corpus_file}")
        logger.info(f"ğŸ“Š Dosya boyutu: {file_size:.2f} GB")
        
        return str(corpus_file)

    def preprocess_corpus(self, corpus_file: str) -> str:
        """
        Corpus dosyasÄ±nÄ± Ã¶niÅŸlemden geÃ§ir - 130M model iÃ§in optimize edilmiÅŸ
        
        Args:
            corpus_file: Girdi corpus dosyasÄ±
            
        Returns:
            Ã–niÅŸlenmiÅŸ corpus dosyasÄ±nÄ±n yolu
        """
        logger.info("ğŸ”„ Corpus Ã¶niÅŸleme baÅŸlÄ±yor...")
        
        processed_file = self.output_dir / 'processed_corpus.txt'
        total_lines = 0
        valid_lines = 0
        
        with open(processed_file, 'w', encoding='utf-8') as outfile:
            with open(corpus_file, 'r', encoding='utf-8') as infile:
                for line in infile:
                    total_lines += 1
                    line = line.strip()
                    
                    # KatÄ± filtreleme - 130M model iÃ§in kaliteli veri
                    if self._is_high_quality_text(line):
                        outfile.write(line + '\n')
                        valid_lines += 1
                    
                    if total_lines % 100000 == 0:
                        logger.info(f"  ğŸ“ {total_lines:,} satÄ±r iÅŸlendi, {valid_lines:,} geÃ§erli satÄ±r")
                        
                    # Memory efficiency iÃ§in batch processing
                    if total_lines % 1000000 == 0:
                        logger.info(f"  ğŸ’¾ {total_lines//1000000}M satÄ±r tamamlandÄ±")
        
        retention_rate = (valid_lines / total_lines) * 100
        logger.info(f"âœ… Ã–niÅŸleme tamamlandÄ±:")
        logger.info(f"   ğŸ“Š {valid_lines:,}/{total_lines:,} satÄ±r korundu (%{retention_rate:.1f})")
        return str(processed_file)

    def _is_high_quality_text(self, text: str) -> bool:
        """130M model iÃ§in yÃ¼ksek kaliteli metin kontrolÃ¼"""
        text = text.strip()
        
        # Minimum uzunluk - Ã§ok kÄ±sa cÃ¼mleler iÅŸe yaramaz
        if len(text) < 20:
            return False
        
        # Maximum uzunluk - Ã§ok uzun cÃ¼mleler problematik
        if len(text) > 512:
            return False
        
        # TÃ¼rkÃ§e karakter oranÄ± kontrolÃ¼
        turkish_chars = sum(1 for c in text if c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÄ°Ã–ÅÃœ')
        if len(text) > 0:
            turkish_ratio = turkish_chars / len(text)
            if turkish_ratio < 0.7:  # En az %70 harf olmalÄ±
                return False
        
        # SayÄ± yoÄŸunluÄŸu kontrolÃ¼
        digit_ratio = sum(c.isdigit() for c in text) / len(text)
        if digit_ratio > 0.3:  # %30'dan fazla sayÄ± olmasÄ±n
            return False
        
        # Tekrar eden karakter kontrolÃ¼ (spam filtreleme)
        if any(char * 4 in text for char in set(text)):
            return False
        
        # BÃ¼yÃ¼k harf yoÄŸunluÄŸu (SPAM filtreleme)
        upper_ratio = sum(c.isupper() for c in text) / len(text)
        if upper_ratio > 0.5:  # %50'den fazla bÃ¼yÃ¼k harf
            return False
            
        return True

    def train_tokenizer(self, corpus_file: str) -> Dict[str, str]:
        """
        130M+ model iÃ§in optimize edilmiÅŸ SentencePiece tokenizer eÄŸit
        
        Args:
            corpus_file: EÄŸitim korpusu dosyasÄ±
            
        Returns:
            EÄŸitilen modelin dosya yollarÄ±
        """
        logger.info("ğŸš€ SentencePiece tokenizer eÄŸitimi baÅŸlÄ±yor...")
        logger.info(f"ğŸ¯ Target: 130M parametre model iÃ§in optimize edilmiÅŸ tokenizer")
        
        model_prefix = str(self.output_dir / 'turkish_tokenizer')
        
        # SentencePiece eÄŸitim parametrelerini hazÄ±rla
        sp_params = self.sp_params.copy()
        sp_params.update({
            'input': corpus_file,
            'model_prefix': model_prefix,
            'user_defined_symbols': ','.join(self.special_tokens),
        })
        
        # Kritik parametreleri logla
        logger.info("ğŸ“‹ EÄŸitim parametreleri:")
        logger.info(f"  ğŸ“ Vocabulary boyutu: {sp_params['vocab_size']:,}")
        logger.info(f"  ğŸ”¤ Model tipi: {sp_params['model_type'].upper()}")
        logger.info(f"  ğŸ¯ Karakter kapsamÄ±: {sp_params['character_coverage']}")
        logger.info(f"  â­ Ã–zel token sayÄ±sÄ±: {len(self.special_tokens)}")
        logger.info(f"  ğŸ§µ Thread sayÄ±sÄ±: {sp_params['num_threads']}")
        
        try:
            # SentencePiece modelini eÄŸit
            logger.info("ğŸ”¥ SentencePiece eÄŸitimi baÅŸlÄ±yor...")
            start_time = time.time()
            
            spm.SentencePieceTrainer.train(**sp_params)
            
            training_time = time.time() - start_time
            logger.info(f"âœ… Tokenizer eÄŸitimi tamamlandÄ±! SÃ¼re: {training_time:.2f}s")
            
            # Ã‡Ä±ktÄ± dosyalarÄ±nÄ± kontrol et
            model_file = f"{model_prefix}.model"
            vocab_file = f"{model_prefix}.vocab"
            
            if os.path.exists(model_file) and os.path.exists(vocab_file):
                logger.info(f"ğŸ“ Model dosyasÄ±: {model_file}")
                logger.info(f"ğŸ“ Vocabulary dosyasÄ±: {vocab_file}")
                
                # Model kalitesini analiz et
                self._analyze_tokenizer_quality(model_file)
                
                return {
                    'model': model_file,
                    'vocab': vocab_file,
                    'training_time': training_time
                }
            else:
                raise FileNotFoundError("Model dosyalarÄ± oluÅŸturulamadÄ±")
                
        except Exception as e:
            logger.error(f"âŒ Tokenizer eÄŸitimi baÅŸarÄ±sÄ±z: {e}")
            raise

    def _analyze_tokenizer_quality(self, model_file: str) -> None:
        """Tokenizer kalitesini analiz et ve raporla"""
        try:
            sp = spm.SentencePieceProcessor()
            sp.load(model_file)
            
            logger.info("ğŸ“Š Tokenizer Kalite Analizi:")
            logger.info(f"  ğŸ“ Toplam vocabulary: {sp.vocab_size():,}")
            
            # Ã–zel tokenlarÄ± kontrol et
            special_count = 0
            special_ids = []
            for token in self.special_tokens:
                token_id = sp.piece_to_id(token)
                if token_id != sp.unk_id():
                    special_count += 1
                    special_ids.append(f"{token}:{token_id}")
            
            logger.info(f"  â­ Ã–zel tokenlar: {special_count}/{len(self.special_tokens)} baÅŸarÄ±lÄ±")
            
            # Temel tokenlarÄ± kontrol et
            core_tokens = {
                '<unk>': sp.unk_id(),
                '<s>': sp.bos_id(), 
                '</s>': sp.eos_id(),
                '<pad>': sp.pad_id()
            }
            
            logger.info("  ğŸ”§ Core tokenlar:")
            for token, token_id in core_tokens.items():
                logger.info(f"     {token}: ID {token_id}")
            
            # TÃ¼rkÃ§e test cÃ¼mleleri
            test_sentences = [
                "Merhaba, nasÄ±lsÄ±nÄ±z?",
                "TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r.",
                "Yapay zeka teknolojisi hÄ±zla geliÅŸiyor."
            ]
            
            logger.info("  ğŸ§ª Tokenizasyon testi:")
            for sentence in test_sentences:
                tokens = sp.encode(sentence, out_type=str)
                token_count = len(tokens)
                logger.info(f"     '{sentence}' -> {token_count} token")
            
            # Efficiency score (ortalama token/karakter oranÄ±)
            total_chars = sum(len(s) for s in test_sentences)
            total_tokens = sum(len(sp.encode(s)) for s in test_sentences)
            efficiency = total_chars / total_tokens if total_tokens > 0 else 0
            
            logger.info(f"  âš¡ Encoding efficiency: {efficiency:.2f} karakter/token")
            
            if efficiency >= 3.0:
                logger.info("     âœ… Excellent efficiency for Turkish!")
            elif efficiency >= 2.5:
                logger.info("     âœ… Good efficiency")
            else:
                logger.info("     âš ï¸  Low efficiency - consider retraining")
                
        except Exception as e:
            logger.warning(f"Kalite analizi yapÄ±lamadÄ±: {e}")

    def save_config(self, result: Dict[str, str]) -> str:
        """Tokenizer konfigÃ¼rasyonunu kaydet"""
        config = {
            'model_info': {
                'name': 'Turkish SentencePiece Tokenizer',
                'version': '1.0',
                'target_model_size': '130M+',
                'language': 'Turkish',
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'parameters': {
                'vocab_size': self.vocab_size,
                'model_type': self.model_type,
                'character_coverage': self.character_coverage,
            },
            'special_tokens': {
                'count': len(self.special_tokens),
                'tokens': self.special_tokens
            },
            'files': result,
            'usage': {
                'loading': f"sp.load('{result['model']}')",
                'encoding': "tokens = sp.encode('metin', out_type=str)",
                'decoding': "text = sp.decode(tokens)"
            }
        }
        
        config_file = self.output_dir / 'tokenizer_config.json'
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ KonfigÃ¼rasyon kaydedildi: {config_file}")
        return str(config_file)

    def run_training(self) -> Dict[str, str]:
        """Tam eÄŸitim sÃ¼recini Ã§alÄ±ÅŸtÄ±r"""
        logger.info("ğŸš€ TÃ¼rkÃ§e Tokenizer EÄŸitimi BaÅŸlÄ±yor!")
        logger.info("ğŸ¯ 130M+ Parametre Modeller Ä°Ã§in Optimize EdilmiÅŸ")
        logger.info("=" * 60)
        
        try:
            # 1. Corpus dosyasÄ±nÄ± kontrol et
            logger.info("ğŸ“Š 1. AÅŸama: Corpus dosyasÄ± kontrol ediliyor...")
            corpus_file = self.get_corpus_file()
            
            # 2. Corpus'u Ã¶niÅŸle
            logger.info("ğŸ”„ 2. AÅŸama: YÃ¼ksek kaliteli corpus hazÄ±rlanÄ±yor...")
            processed_corpus = self.preprocess_corpus(corpus_file)
            
            # 3. Tokenizer'Ä± eÄŸit
            logger.info("ğŸ“ 3. AÅŸama: Tokenizer eÄŸitimi...")
            result = self.train_tokenizer(processed_corpus)
            
            # 4. KonfigÃ¼rasyonu kaydet
            logger.info("ğŸ’¾ 4. AÅŸama: KonfigÃ¼rasyon kaydediliyor...")
            config_file = self.save_config(result)
            result['config'] = config_file
            
            # 5. Ã–zet bilgi
            logger.info("ğŸ‰ EÄŸitim BaÅŸarÄ±yla TamamlandÄ±!")
            logger.info("=" * 60)
            logger.info(f"ğŸ“ Model dosyasÄ±: {result['model']}")
            logger.info(f"ğŸ“ Vocabulary dosyasÄ±: {result['vocab']}")  
            logger.info(f"ğŸ“„ KonfigÃ¼rasyon: {result['config']}")
            logger.info(f"â±ï¸  Toplam eÄŸitim sÃ¼resi: {result['training_time']:.2f}s")
            logger.info("")
            logger.info("ğŸš€ 130M model iÃ§in hazÄ±r! Test etmek iÃ§in:")
            logger.info("   python test_turkish_tokenizer.py")
            logger.info("")
            logger.info("ğŸ’¡ Gelecek geniÅŸletmeler iÃ§in 5 rezerve token hazÄ±r!")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ EÄŸitim baÅŸarÄ±sÄ±z: {e}")
            raise

def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(
        description="130M+ Parametre Modeller Ä°Ã§in TÃ¼rkÃ§e Tokenizer EÄŸiticisi"
    )
    parser.add_argument('--vocab-size', type=int, default=32000, 
                       help='Vocabulary boyutu (varsayÄ±lan: 32000)')
    parser.add_argument('--model-type', choices=['bpe', 'unigram'], 
                       default='bpe', help='Model tipi (varsayÄ±lan: bpe)')
    parser.add_argument('--coverage', type=float, default=0.9995,
                       help='Karakter kapsamÄ± (varsayÄ±lan: 0.9995)')
    parser.add_argument('--output-dir', type=str, default='turkish_tokenizer',
                       help='Ã‡Ä±ktÄ± dizini (varsayÄ±lan: turkish_tokenizer)')
    
    args = parser.parse_args()
    
    # Trainer'Ä± baÅŸlat
    trainer = TurkishTokenizerTrainer(
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.coverage,
        output_dir=args.output_dir
    )
    
    # EÄŸitimi Ã§alÄ±ÅŸtÄ±r
    trainer.run_training()

if __name__ == "__main__":
    main() 