import os
import json
import time
import logging
import argparse
import sentencepiece as spm
from pathlib import Path
from typing import List, Dict, Optional

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tokenizer_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TurkishTokenizerTrainer:
    def __init__(self, 
                 vocab_size: int = 32000,
                 model_type: str = 'bpe',
                 character_coverage: float = 0.9995,
                 output_dir: str = 'turkish_tokenizer'):
        """
        TÃ¼rkÃ§e tokenizer eÄŸiticisi
        
        Args:
            vocab_size: Vocabulary boyutu (32K TÃ¼rkÃ§e iÃ§in optimal)
            model_type: Model tipi ('bpe', 'unigram', 'word', 'char')
            character_coverage: Karakter kapsamÄ± (TÃ¼rkÃ§e iÃ§in 0.9995)
            output_dir: Ã‡Ä±ktÄ± dizini
        """
        self.vocab_size = vocab_size
        self.model_type = model_type
        self.character_coverage = character_coverage
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # TÃ¼rkÃ§e iÃ§in optimal SentencePiece parametreleri
        self.sp_params = {
            'model_type': model_type,
            'vocab_size': vocab_size,
            'character_coverage': character_coverage,
            'normalization_rule_name': 'nfkc',  # TÃ¼rkÃ§e karakterler iÃ§in
            'remove_extra_whitespaces': True,
            'split_by_unicode_script': True,    # TÃ¼rkÃ§e morfoloji iÃ§in Ã¶nemli
            'split_by_whitespace': True,
            'split_by_number': True,
            'split_digits': True,
            'treat_whitespace_as_suffix': False,
            'allow_whitespace_only_pieces': True,
            'max_sentence_length': 8192,
            'shuffle_input_sentence': True,
            'input_sentence_size': 10000000,    # 10M cÃ¼mle
            'seed_sentencepiece_size': 1000000,
            'shrinking_factor': 0.75,
            'num_threads': os.cpu_count() or 4,
            'max_sentencepiece_length': 16,
            'num_sub_iterations': 2,
        }
        
        # TÃ¼rkÃ§e Ã¶zel tokenlar
        self.special_tokens = [
            '<s>', '</s>', '<pad>',                     # Temel tokenlar (unk otomatik)
            '<mask>', '<cls>', '<sep>',                # BERT-style tokenlar  
            '<turkish>', '<TR>',                       # Dil tokenlarÄ±
            '<question>', '<answer>',                  # QA tokenlarÄ±
            '<news>', '<social>', '<formal>',          # Domain tokenlarÄ±
            '<thinking>', '</thinking>',                # Think tokenlarÄ± (modern AI iÃ§in)
            '<thought>', '</thought>',                  # Alternatif dÃ¼ÅŸÃ¼nce tokenlarÄ±
            '<reasoning>', '</reasoning>',              # MantÄ±k yÃ¼rÃ¼tme
            '<analysis>', '</analysis>',                # Analiz bÃ¶lÃ¼mleri
            '<|endoftext|>', '<|startoftext|>',        # End of text ve start of text tokenlarÄ±
            
            # Instruction Tuning TokenlarÄ±
            '<|system|>', '<|user|>', '<|assistant|>', # Chat rolleri (ChatML style)
            '<|im_start|>', '<|im_end|>',              # Instruction message markers
            '<instruction>', '</instruction>',          # Instruction wrapper
            '<input>', '</input>',                     # Input wrapper
            '<output>', '</output>',                   # Output wrapper
            '<context>', '</context>',                 # Context information
            
            # Multi-turn Conversation
            '<turn>', '</turn>',                       # Conversation turn markers
            '<conversation>', '</conversation>',        # Conversation wrapper
            '<history>', '</history>',                 # Chat history
            
            # Function Calling & Tool Use
            '<function_call>', '</function_call>',     # Function call wrapper
            '<tool_use>', '</tool_use>',               # Tool usage marker
            '<json>', '</json>',                       # JSON structured data
            '<code>', '</code>',                       # Code blocks
            
            # Safety & Control
            '<safe>', '<unsafe>',                      # Safety markers
            '<warning>', '</warning>',                 # Warning wrapper
            '<filter>', '</filter>',                   # Content filtering
            
            # Task-Specific (TÃ¼rkÃ§e iÃ§in)
            '<Ã§eviri>', '</Ã§eviri>',                   # Translation task
            '<Ã¶zet>', '</Ã¶zet>',                       # Summarization task
            '<soru>', '</soru>',                       # Question marker
            '<cevap>', '</cevap>',                     # Answer marker
            
            # Multi-modal (gelecek iÃ§in)
            '<image>', '</image>',                     # Image content
            '<audio>', '</audio>',                     # Audio content
            '<video>', '</video>',                     # Video content
        ]

    def get_corpus_file(self) -> str:
        """
        Mevcut OSCAR corpus dosyasÄ±nÄ± kontrol et ve dÃ¶ndÃ¼r
        
        Returns:
            Corpus dosyasÄ±nÄ±n yolu
        """
        # Script'in bulunduÄŸu dizinden data klasÃ¶rÃ¼ne eriÅŸim
        script_dir = Path(__file__).parent
        corpus_file = script_dir / 'data' / 'oscar_turkish.txt'
        
        if not corpus_file.exists():
            raise FileNotFoundError(f"OSCAR corpus dosyasÄ± bulunamadÄ±: {corpus_file}")
        
        file_size = corpus_file.stat().st_size / (1024*1024*1024)  # GB
        logger.info(f"âœ… OSCAR corpus bulundu: {corpus_file}")
        logger.info(f"ğŸ“Š Dosya boyutu: {file_size:.2f} GB")
        
        return str(corpus_file)

    def preprocess_corpus(self, corpus_file: str) -> str:
        """
        Corpus dosyasÄ±nÄ± Ã¶niÅŸlemden geÃ§ir
        
        Args:
            corpus_file: Girdi corpus dosyasÄ±
            
        Returns:
            Ã–niÅŸlenmiÅŸ corpus dosyasÄ±nÄ±n yolu
        """
        logger.info("ğŸ”„ Corpus Ã¶niÅŸleme baÅŸlÄ±yor...")
        
        processed_file = self.output_dir / 'processed_corpus.txt'
        total_lines = 0
        valid_lines = 0
        
        with open(processed_file, 'w', encoding='utf-8') as outfile:
            with open(corpus_file, 'r', encoding='utf-8') as infile:
                for line in infile:
                    total_lines += 1
                    line = line.strip()
                    
                    # Basit filtreleme
                    if self._is_valid_text(line):
                        outfile.write(line + '\n')
                        valid_lines += 1
                    
                    if total_lines % 100000 == 0:
                        logger.info(f"  ğŸ“ {total_lines} satÄ±r iÅŸlendi, {valid_lines} geÃ§erli satÄ±r")
        
        logger.info(f"âœ… Ã–niÅŸleme tamamlandÄ±: {valid_lines}/{total_lines} satÄ±r korundu")
        return str(processed_file)

    def _is_valid_text(self, text: str) -> bool:
        """Basit metin kontrolÃ¼"""
        # Ã‡ok kÄ±sa metinleri filtrele
        if len(text.strip()) < 10:
            return False
        
        # Ã‡ok uzun metinleri filtrele
        if len(text) > 1000:
            return False
        
        # SayÄ±sal iÃ§erik kontrolÃ¼
        digit_ratio = sum(c.isdigit() for c in text) / len(text)
        if digit_ratio > 0.5:
            return False
        
        return True

    def train_tokenizer(self, corpus_file: str) -> Dict[str, str]:
        """
        SentencePiece tokenizer eÄŸit
        
        Args:
            corpus_file: EÄŸitim korpusu dosyasÄ±
            
        Returns:
            EÄŸitilen modelin dosya yollarÄ±
        """
        logger.info("ğŸš€ SentencePiece tokenizer eÄŸitimi baÅŸlÄ±yor...")
        
        model_prefix = str(self.output_dir / 'turkish_tokenizer')
        
        # SentencePiece eÄŸitim parametrelerini hazÄ±rla
        sp_params = self.sp_params.copy()
        sp_params.update({
            'input': corpus_file,
            'model_prefix': model_prefix,
            'user_defined_symbols': self.special_tokens,
        })
        
        # Parametreleri logla
        logger.info("ğŸ“‹ EÄŸitim parametreleri:")
        for key, value in sp_params.items():
            logger.info(f"  {key}: {value}")
        
        try:
            # SentencePiece modelini eÄŸit
            logger.info("ğŸ”¥ SentencePiece eÄŸitimi baÅŸlÄ±yor...")
            start_time = time.time()
            
            spm.SentencePieceTrainer.train(**sp_params)
            
            training_time = time.time() - start_time
            logger.info(f"âœ… Tokenizer eÄŸitimi tamamlandÄ±! SÃ¼re: {training_time:.2f}s")
            
            # Ã‡Ä±ktÄ± dosyalarÄ±nÄ± kontrol et
            model_file = f"{model_prefix}.model"
            vocab_file = f"{model_prefix}.vocab"
            
            if os.path.exists(model_file) and os.path.exists(vocab_file):
                logger.info(f"ğŸ“ Model dosyasÄ±: {model_file}")
                logger.info(f"ğŸ“ Vocabulary dosyasÄ±: {vocab_file}")
                
                # Model istatistiklerini gÃ¶ster
                self._show_model_stats(model_file)
                
                return {
                    'model': model_file,
                    'vocab': vocab_file,
                    'training_time': training_time
                }
            else:
                raise FileNotFoundError("Model dosyalarÄ± oluÅŸturulamadÄ±")
                
        except Exception as e:
            logger.error(f"âŒ Tokenizer eÄŸitimi baÅŸarÄ±sÄ±z: {e}")
            raise

    def _show_model_stats(self, model_file: str) -> None:
        """Model istatistiklerini gÃ¶ster"""
        try:
            sp = spm.SentencePieceProcessor()
            sp.load(model_file)
            
            logger.info("ğŸ“Š Model Ä°statistikleri:")
            logger.info(f"  ğŸ“ Vocabulary boyutu: {sp.vocab_size()}")
            logger.info(f"  ğŸ”¤ Model tipi: {self.model_type.upper()}")
            logger.info(f"  ğŸ¯ Karakter kapsamÄ±: {self.character_coverage}")
            
            # Ã–zel tokenlarÄ± kontrol et
            special_tokens_found = []
            for token in self.special_tokens:
                if sp.piece_to_id(token) != sp.unk_id():
                    special_tokens_found.append(token)
            
            logger.info(f"  â­ Ã–zel tokenlar: {len(special_tokens_found)}/{len(self.special_tokens)}")
            logger.info(f"     {', '.join(special_tokens_found[:5])}...")
            
        except Exception as e:
            logger.warning(f"Model istatistikleri gÃ¶sterilemedi: {e}")

    def run_training(self) -> Dict[str, str]:
        """Tam eÄŸitim sÃ¼recini Ã§alÄ±ÅŸtÄ±r"""
        logger.info("ğŸš€ TÃ¼rkÃ§e Tokenizer EÄŸitimi BaÅŸlÄ±yor!")
        logger.info("=" * 50)
        
        try:
            # 1. Corpus dosyasÄ±nÄ± kontrol et
            logger.info("ğŸ“Š 1. AÅŸama: Corpus dosyasÄ± kontrol ediliyor...")
            corpus_file = self.get_corpus_file()
            
            # 2. Corpus'u Ã¶niÅŸle
            logger.info("ğŸ”„ 2. AÅŸama: Corpus Ã¶niÅŸleme...")
            processed_corpus = self.preprocess_corpus(corpus_file)
            
            # 3. Tokenizer'Ä± eÄŸit
            logger.info("ğŸ“ 3. AÅŸama: Tokenizer eÄŸitimi...")
            result = self.train_tokenizer(processed_corpus)
            
            # 4. Ã–zet bilgi
            logger.info("ğŸ‰ EÄŸitim TamamlandÄ±!")
            logger.info("=" * 50)
            logger.info(f"ğŸ“ Model dosyasÄ±: {result['model']}")
            logger.info(f"ğŸ“ Vocabulary dosyasÄ±: {result['vocab']}")
            logger.info(f"â±ï¸  Toplam eÄŸitim sÃ¼resi: {result['training_time']:.2f}s")
            logger.info("")
            logger.info("ğŸš€ Tokenizer'Ä±nÄ±z hazÄ±r! Test etmek iÃ§in:")
            logger.info("   python test_turkish_tokenizer.py")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ EÄŸitim baÅŸarÄ±sÄ±z: {e}")
            raise

def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(description="TÃ¼rkÃ§e SentencePiece Tokenizer EÄŸitici")
    parser.add_argument('--vocab-size', type=int, default=32000, 
                       help='Vocabulary boyutu (varsayÄ±lan: 32000)')
    parser.add_argument('--model-type', choices=['bpe', 'unigram', 'word', 'char'], 
                       default='bpe', help='Model tipi (varsayÄ±lan: bpe)')
    parser.add_argument('--coverage', type=float, default=0.9995,
                       help='Karakter kapsamÄ± (varsayÄ±lan: 0.9995)')
    parser.add_argument('--output-dir', type=str, default='turkish_tokenizer',
                       help='Ã‡Ä±ktÄ± dizini (varsayÄ±lan: turkish_tokenizer)')
    
    args = parser.parse_args()
    
    # Trainer'Ä± baÅŸlat
    trainer = TurkishTokenizerTrainer(
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.coverage,
        output_dir=args.output_dir
    )
    
    # EÄŸitimi Ã§alÄ±ÅŸtÄ±r
    trainer.run_training()

if __name__ == "__main__":
    main() 