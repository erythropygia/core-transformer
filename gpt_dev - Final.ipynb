{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749637467548,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "BRd6stqMLBbb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('kinyas_kayra_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078394,
     "status": "ok",
     "timestamp": 1749638561545,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "vX1dxQw8Or2f",
    "outputId": "d2c923c5-48c3-4690-8511-32d7f906ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('196', '177') (frequency: 45258)\n",
      "Merge 501: ('196177', '110') (frequency: 9525)\n",
      "Merge 1000: ('196177', '110') (frequency: 9525)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "class ByteLevelBPE:\n",
    "    def __init__(self, merges: Optional[List[Tuple[str, str]]] = None, \n",
    "                 token_to_id: Optional[Dict[str, int]] = None, \n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        \"\"\"Byte-level BPE tokenizer for Turkish text with UTF-8 support.\n",
    "        \n",
    "        Args:\n",
    "            merges: List of merge rules as tuples\n",
    "            token_to_id: Vocabulary mapping tokens to ids\n",
    "            special_tokens: Special tokens to add to vocabulary\n",
    "        \"\"\"\n",
    "        self.merges = merges or []\n",
    "        self.merges_set = set(self.merges)\n",
    "        self.token_to_id = token_to_id or {}\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.vocab = None\n",
    "\n",
    "        # Special tokens with default ones\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._initialize_special_tokens()\n",
    "        \n",
    "    def _initialize_special_tokens(self) -> None:\n",
    "        \"\"\"Initialize special tokens in vocabulary.\"\"\"\n",
    "        for tok in self.special_tokens:\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = len(self.token_to_id)\n",
    "        self.id_to_token.update({v: k for k, v in self.token_to_id.items()})\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.special_tokens_set = set(self.special_tokens)\n",
    "\n",
    "    def train(self, text: str, num_merges: int = 5000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train tokenizer on given text.\n",
    "        \n",
    "        Args:\n",
    "            text: Training text corpus\n",
    "            num_merges: Number of merge operations to perform\n",
    "            verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        self.vocab = self._get_vocab(text)\n",
    "        self.merges = []\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self._get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self._merge_vocab(best, self.vocab)\n",
    "            self.merges.append(best)\n",
    "            \n",
    "            if verbose and (i % 500 == 0 or i == num_merges - 1):\n",
    "                print(f\"Merge {i + 1}: {best} (frequency: {pairs[best]})\")\n",
    "                \n",
    "        self.merges_set = set(self.merges)\n",
    "        self._build_token_vocab()\n",
    "\n",
    "    def _get_vocab(self, text: str) -> Counter:\n",
    "        \"\"\"Initialize vocabulary from text by splitting into UTF-8 bytes.\"\"\"\n",
    "        vocab = Counter()\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        \n",
    "        for word in words:\n",
    "            word_bytes = list(word.encode('utf-8'))\n",
    "            word_bytes_str = [f\"{b:03d}\" for b in word_bytes]\n",
    "            tokenized = ' '.join(word_bytes_str + ['</w>'])\n",
    "            vocab[tokenized] += 1\n",
    "            \n",
    "        return vocab\n",
    "\n",
    "    def _get_stats(self, vocab: Counter) -> Counter:\n",
    "        \"\"\"Get frequency statistics for possible merges.\"\"\"\n",
    "        pairs = Counter()\n",
    "        \n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                if symbols[i + 1] == '</w>':\n",
    "                    continue\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "                \n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab_in: Counter) -> Counter:\n",
    "        \"\"\"Merge the given pair in the vocabulary.\"\"\"\n",
    "        vocab_out = Counter()\n",
    "        replacement = pair[0] + pair[1]\n",
    "        \n",
    "        for word, freq in vocab_in.items():\n",
    "            symbols = word.split()\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(symbols):\n",
    "                if (i < len(symbols) - 1 and \n",
    "                    (symbols[i], symbols[i + 1]) == pair and\n",
    "                    len(symbols[i]) == 3 and  # Ensure proper byte tokens\n",
    "                    len(symbols[i + 1]) == 3):\n",
    "                    new_symbols.append(replacement)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "                    \n",
    "            new_word = ' '.join(new_symbols)\n",
    "            vocab_out[new_word] = freq\n",
    "            \n",
    "        return vocab_out\n",
    "\n",
    "    def _build_token_vocab(self) -> None:\n",
    "        \"\"\"Build the final vocabulary from base bytes and merges.\"\"\"\n",
    "        # Reset with special tokens\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(self.special_tokens)}\n",
    "        \n",
    "        # Add base byte tokens (000-255)\n",
    "        for i in range(256):\n",
    "            tok = f\"{i:03d}\"\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = len(self.token_to_id)\n",
    "        \n",
    "        # Add merged tokens from vocabulary\n",
    "        for word in self.vocab.keys():\n",
    "            for token in word.split():\n",
    "                if token != '</w>' and token not in self.token_to_id:\n",
    "                    # Validate token format (multiples of 3 digits)\n",
    "                    if len(token) % 3 == 0 and all(c.isdigit() for c in token):\n",
    "                        self.token_to_id[token] = len(self.token_to_id)\n",
    "        \n",
    "        # Add all possible merges\n",
    "        for a, b in self.merges:\n",
    "            merged = a + b\n",
    "            if merged not in self.token_to_id:\n",
    "                self.token_to_id[merged] = len(self.token_to_id)\n",
    "        \n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "\n",
    "    def encode(self, text: str, dropout: float = 0.0, \n",
    "               add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Encode text into token IDs.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to encode\n",
    "            dropout: Merge operation dropout probability\n",
    "            add_special_tokens: Whether to add special tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        encoded = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded.append(self.special_token_ids['<sos>'])\n",
    "        \n",
    "        for word in words:\n",
    "            word_bytes = [f\"{b:03d}\" for b in word.encode('utf-8')] + ['</w>']\n",
    "            \n",
    "            # Apply BPE merges\n",
    "            while len(word_bytes) > 1:\n",
    "                pairs = self._get_word_pairs(word_bytes)\n",
    "                mergeable = [p for p in pairs if p in self.merges_set]\n",
    "                \n",
    "                if not mergeable:\n",
    "                    break\n",
    "                    \n",
    "                best = self._select_best_merge(pairs, dropout)\n",
    "                if not best:\n",
    "                    break\n",
    "                    \n",
    "                word_bytes = self._apply_merge(word_bytes, best)\n",
    "            \n",
    "            # Add to encoded tokens\n",
    "            for token in word_bytes:\n",
    "                if token == '</w>':\n",
    "                    continue\n",
    "                encoded.append(self.token_to_id.get(token, self.special_token_ids['<unk>']))\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded.append(self.special_token_ids['<eos>'])\n",
    "            \n",
    "        return encoded\n",
    "\n",
    "    def _get_word_pairs(self, word_bytes: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get all possible adjacent pairs in word.\"\"\"\n",
    "        return [(word_bytes[i], word_bytes[i + 1]) for i in range(len(word_bytes) - 1)]\n",
    "\n",
    "    def _select_best_merge(self, pairs: List[Tuple[str, str]], \n",
    "                          dropout: float) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"Select the best merge according to merge list with optional dropout.\"\"\"\n",
    "        for merge in self.merges:\n",
    "            if merge in pairs:\n",
    "                if dropout > 0 and random.random() < dropout:\n",
    "                    continue  # Skip this merge due to dropout\n",
    "                return merge\n",
    "        return None\n",
    "\n",
    "    def _apply_merge(self, word_bytes: List[str], \n",
    "                    best: Tuple[str, str]) -> List[str]:\n",
    "        \"\"\"Apply the merge operation to the word bytes.\"\"\"\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word_bytes):\n",
    "            if (i < len(word_bytes) - 1 and \n",
    "                (word_bytes[i], word_bytes[i + 1]) == best):\n",
    "                new_word.append(word_bytes[i] + word_bytes[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word_bytes[i])\n",
    "                i += 1\n",
    "        return new_word\n",
    "\n",
    "    def decode(self, token_ids: List[int], \n",
    "               skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Decode token IDs back to text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs to decode\n",
    "            skip_special_tokens: Whether to skip special tokens\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        byte_seq = []\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token.get(token_id)\n",
    "            if token is None or (skip_special_tokens and token in self.special_tokens_set):\n",
    "                continue\n",
    "                \n",
    "            # Split token into 3-digit byte parts\n",
    "            parts = []\n",
    "            i = 0\n",
    "            while i < len(token):\n",
    "                part = token[i:i+3]\n",
    "                if len(part) == 3 and part.isdigit():\n",
    "                    parts.append(part)\n",
    "                    i += 3\n",
    "                else:\n",
    "                    # Handle incomplete parts (shouldn't happen with proper tokens)\n",
    "                    i += 1\n",
    "            \n",
    "            # Convert to bytes\n",
    "            for part in parts:\n",
    "                try:\n",
    "                    byte_val = int(part)\n",
    "                    if 0 <= byte_val <= 255:\n",
    "                        byte_seq.append(byte_val)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        # Decode with error handling\n",
    "        try:\n",
    "            return bytes(byte_seq).decode('utf-8', errors='strict')\n",
    "        except UnicodeDecodeError:\n",
    "            # Fallback to replace invalid sequences\n",
    "            return bytes(byte_seq).decode('utf-8', errors='replace')\n",
    "\n",
    "    def save_vocab(self, file_prefix: str) -> None:\n",
    "        \"\"\"Save vocabulary and merges to files.\n",
    "        \n",
    "        Args:\n",
    "            file_prefix: Prefix for vocab and merges files\n",
    "        \"\"\"\n",
    "        with open(f\"{file_prefix}_merges.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.merges, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        with open(f\"{file_prefix}_vocab.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.token_to_id, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab(self, file_prefix: str) -> None:\n",
    "        \"\"\"Load vocabulary and merges from files.\n",
    "        \n",
    "        Args:\n",
    "            file_prefix: Prefix for vocab and merges files\n",
    "        \"\"\"\n",
    "        with open(f\"{file_prefix}_merges.json\", 'r', encoding='utf-8') as f:\n",
    "            self.merges = [tuple(merge) for merge in json.load(f)]\n",
    "            self.merges_set = set(self.merges)\n",
    "            \n",
    "        with open(f\"{file_prefix}_vocab.json\", 'r', encoding='utf-8') as f:\n",
    "            self.token_to_id = json.load(f)\n",
    "            \n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self._initialize_special_tokens()\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Get the size of the vocabulary.\"\"\"\n",
    "        return len(self.token_to_id)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into subword tokens (for inspection).\"\"\"\n",
    "        token_ids = self.encode(text, add_special_tokens=False)\n",
    "        return [self.id_to_token[token_id] for token_id in token_ids]\n",
    "\n",
    "    def inspect_tokenization(self, text: str) -> None:\n",
    "        \"\"\"Print detailed tokenization information for debugging.\"\"\"\n",
    "        print(f\"Original text: {repr(text)}\")\n",
    "        tokens = self.tokenize(text)\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Token IDs:\", self.encode(text, add_special_tokens=False))\n",
    "        decoded = self.decode(self.encode(text))\n",
    "        print(\"Decoded:\", repr(decoded))\n",
    "        print(\"Byte sequence:\", [ord(c) for c in decoded])\n",
    "\n",
    "tokenizer = ByteLevelBPE()\n",
    "tokenizer.train(text, num_merges=1000)\n",
    "tokenizer.save_vocab('turkish_bpe')\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "#chars = sorted(list(set(text)))\n",
    "#vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "#stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "#itos = { i:ch for i,ch in enumerate(chars) }\n",
    "#encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "#decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 'şçöğü'\n",
      "Tokens: ['197159', '195167', '195', '182', '196159', '195188']\n",
      "Token IDs: [269, 268, 199, 186, 266, 261]\n",
      "Decoded: 'şçöğü'\n",
      "Byte sequence: [351, 231, 246, 287, 252]\n",
      "merhaba dünya\n"
     ]
    }
   ],
   "source": [
    "tokenizer.inspect_tokenization(\"şçöğü\")\n",
    "\n",
    "# Metni encode/decode etme\n",
    "encoded = tokenizer.encode(\"merhaba dünya\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)  # \"merhaba dünya\"\n",
    "\n",
    "# Vocab kaydetme/yükleme\n",
    "#tokenizer.save_vocab(\"turkish_bpe\")\n",
    "tokenizer.load_vocab(\"bpe_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: İstanbul'da şehir içi ulaşım çok karmaşık\n",
      "Original text: \"İstanbul'da şehir içi ulaşım çok karmaşık\"\n",
      "Tokens: ['196', '176', '115', '116', '097110', '098', '117', '108', '039', '100', '097', '032', '197159', '101', '104', '105', '114', '032', '105', '195167', '105', '032', '117', '108', '097', '197159', '196177', '109', '032', '195167', '111', '107', '032', '107', '097114', '109', '097', '197159', '196177', '107']\n",
      "Token IDs: [200, 180, 119, 120, 263, 102, 121, 112, 43, 104, 101, 36, 269, 105, 108, 109, 118, 36, 109, 268, 109, 36, 121, 112, 101, 269, 265, 113, 36, 268, 115, 111, 36, 111, 260, 113, 101, 269, 265, 111]\n",
      "Decoded: \"İstanbul'da şehir içi ulaşım çok karmaşık\"\n",
      "Byte sequence: [304, 115, 116, 97, 110, 98, 117, 108, 39, 100, 97, 32, 351, 101, 104, 105, 114, 32, 105, 231, 105, 32, 117, 108, 97, 351, 305, 109, 32, 231, 111, 107, 32, 107, 97, 114, 109, 97, 351, 305, 107]\n",
      "Success!\n",
      "\n",
      "Testing: Pijamalı hasta yağız şoföre çabucak güvendi\n",
      "Original text: 'Pijamalı hasta yağız şoföre çabucak güvendi'\n",
      "Tokens: ['080', '105', '106', '097', '109', '097', '108', '196177', '032', '104', '097', '115', '116', '097', '032', '121', '097', '196159', '196177', '122', '032', '197159', '111', '102', '195', '182', '114', '101', '032', '195167', '097', '098', '117', '099', '097', '107', '032', '103', '195188', '118', '101110', '100', '105']\n",
      "Token IDs: [84, 109, 110, 101, 113, 101, 112, 265, 36, 108, 101, 119, 120, 101, 36, 125, 101, 266, 265, 126, 36, 269, 115, 106, 199, 186, 118, 105, 36, 268, 101, 102, 121, 103, 101, 111, 36, 107, 261, 122, 271, 104, 109]\n",
      "Decoded: 'Pijamalı hasta yağız şoföre çabucak güvendi'\n",
      "Byte sequence: [80, 105, 106, 97, 109, 97, 108, 305, 32, 104, 97, 115, 116, 97, 32, 121, 97, 287, 305, 122, 32, 351, 111, 102, 246, 114, 101, 32, 231, 97, 98, 117, 99, 97, 107, 32, 103, 252, 118, 101, 110, 100, 105]\n",
      "Success!\n",
      "\n",
      "Testing: Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde\n",
      "Original text: 'Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde'\n",
      "Tokens: ['070', '097', '104', '105', '197159', '032', '102', '105', '121', '097', '116', '108', '097114', '108', '097', '032', '109', '195188', '099', '097', '100', '101', '108', '101', '032', '101', '100', '101114', '107', '101110', '032', '103', '195188', '196159', '195188', '109', '115', '195188', '032', '114', '101110', '107', '108', '101114', '032', '105', '195167', '105110', '100', '101']\n",
      "Token IDs: [74, 101, 108, 109, 269, 36, 106, 109, 125, 101, 120, 112, 260, 112, 101, 36, 113, 261, 103, 101, 104, 105, 112, 105, 36, 105, 104, 270, 111, 271, 36, 107, 261, 266, 261, 113, 119, 261, 36, 118, 271, 111, 112, 270, 36, 109, 268, 262, 104, 105]\n",
      "Decoded: 'Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde'\n",
      "Byte sequence: [70, 97, 104, 105, 351, 32, 102, 105, 121, 97, 116, 108, 97, 114, 108, 97, 32, 109, 252, 99, 97, 100, 101, 108, 101, 32, 101, 100, 101, 114, 107, 101, 110, 32, 103, 252, 287, 252, 109, 115, 252, 32, 114, 101, 110, 107, 108, 101, 114, 32, 105, 231, 105, 110, 100, 101]\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    \"İstanbul'da şehir içi ulaşım çok karmaşık\",\n",
    "    \"Pijamalı hasta yağız şoföre çabucak güvendi\",\n",
    "    \"Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde\"\n",
    "]\n",
    "\n",
    "for _text in test_cases:\n",
    "    print(f\"\\nTesting: {_text}\")\n",
    "    tokenizer.inspect_tokenization(_text)\n",
    "    encoded = tokenizer.encode(_text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(\"Success!\" if decoded == _text else \"Failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1255381,
     "status": "ok",
     "timestamp": 1749641238803,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "R0Les052W0UR",
    "outputId": "f57ca121-89da-4e1e-f1aa-0c3671831796"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding with BPE: 100%|██████████| 286013/286013 [00:03<00:00, 93714.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token: 1001484\n",
      "Train data size: 901335\n",
      "Val data size: 100149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def encode_text_with_bpe_ids(bpe_obj, text):\n",
    "    tokens = []\n",
    "    words = re.findall(r'\\S+|\\s+', text)\n",
    "    for word in tqdm(words, desc=\"Encoding with BPE\"):\n",
    "        tokens.extend(bpe_obj.encode(word, add_special_tokens=False))\n",
    "    return tokens\n",
    "\n",
    "tokens = encode_text_with_bpe_ids(tokenizer, text)\n",
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Total token: {len(data)}\")\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "# Train and test splits\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "#train_data = data[:n]\n",
    "#val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1749642394519,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "TLhTmjxnLEco"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749642397145,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "xsw2pAgALFgS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642398317,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "j9c7A4D3LHLF"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642399521,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "nrGYKFB8LI3p"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642400568,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "gbflotPaLKLs"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749642401840,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "6EoQ-B-pLLbI"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "grad_clip = 1.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 3\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.5\n",
    "# ------------\n",
    "\n",
    "vocab_size = len(tokenizer.token_to_id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1749642403088,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "_V5Li4w_LNhx"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            logits = logits / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_v = v[:, -1].unsqueeze(1)\n",
    "                logits = torch.where(logits < min_v, torch.full_like(logits, -float('Inf')), logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1749642407293,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "hoelkOrFY8bN",
    "outputId": "4a347330-c096-4e6a-b09b-90b6e3b2fddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.243793 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTvQXsPFh05H",
    "outputId": "aca584a3-8d01-4e45-89a4-3177fcea87f2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/bpe_transformer\")\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-3, total_iters=5000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Learning rate scheduler\n",
    "    lr = get_lr(iter)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    # Değerlendirme ve log\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "\n",
    "        print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, iter)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, iter)\n",
    "        writer.add_scalar(\"Learning Rate\", lr, iter)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"The new model is better than the old model. The best model has been updated.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Eğitim adımı\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "#for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "#    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#        losses = estimate_loss()\n",
    "#        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "#    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "#    logits, loss = model(xb, yb)\n",
    "#    optimizer.zero_grad(set_to_none=True)\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'032'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkinyas kayra\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mbpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# split yapma\u001b[39;00m\n\u001b[1;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(prompt_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(context, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m, in \u001b[0;36mByteLevelBPE.encode\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</w>\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     encoded_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_ids\n",
      "\u001b[0;31mKeyError\u001b[0m: '032'"
     ]
    }
   ],
   "source": [
    "prompt = \"kinyas kayra\"\n",
    "prompt_tokens = tokenizer.encode(prompt)  # split yapma\n",
    "context = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(context, max_new_tokens=100, temperature=0.7, top_k=50)[0].tolist()\n",
    "print(\"Generated text:\")\n",
    "print(tokenizer.decode(generated_ids))\n",
    "\n",
    "\n",
    "# generate from the model\n",
    "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('032' in bpe.token_to_id) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
