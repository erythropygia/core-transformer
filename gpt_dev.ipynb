{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('kinyas_kayra_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of dataset in characters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtext\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#1115394\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "#1115394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepsi yaralar, sonuncusu Ã¶ldÃ¼rÃ¼r! Birinci Kitap Kinyas, Kayra ve Hayat AsansÃ¶r dÃ¶rdÃ¼ncÃ¼ katta durdu.\n",
      "\n",
      "KapÄ±sÄ±nda 17 yazan daireye girdik.\n",
      "\n",
      "Tahmin ettiÄŸim gibi evde Ã§ok az mobilya vardÄ±.\n",
      "\n",
      "Salonun duvarlarÄ± fotoÄŸraflar ve afiÅŸlerle kaplanmÄ±ÅŸtÄ±.\n",
      "\n",
      "Ortada, eskiciden alÄ±nmÄ±ÅŸ izlenimi veren ceviz yemek masasÄ±, ucuz barlarda Ã§Ä±kmasÄ± muhtemel kavgalarda hasarÄ± Ã¶nlemek amacÄ±yla yere Ã§akÄ±lmÄ±ÅŸÃ§asÄ±na duruyordu.\n",
      "\n",
      "Ve dÃ¶rt adet Ã§elik sandalye tarafÄ±ndan kuÅŸatÄ±lmÄ±ÅŸtÄ±.\n",
      "\n",
      "Yerlerde yÃ¼zlerce iÃ§ki ÅŸiÅŸesi parkeyi bir halÄ± gibi kaplÄ±yordu.\n",
      "\n",
      "KapalÄ± perdelerden, pencerelerin Ã§ok uzun zamandÄ±r aÃ§Ä±lmadÄ±ÄŸÄ± anlaÅŸÄ±lÄ±yordu.\n",
      "\n",
      "Zaten havaya hkim olan keskin alkol ve tÃ¼tÃ¼n kokusu da bunu gÃ¶steriyordu.\n",
      "\n",
      "MasanÄ±n Ã¼stÃ¼ndeki boÅŸ ve daÄŸÄ±nÄ±k kÄŸÄ±tlar, cesetler gibi, birileri tarafÄ±ndan toplanmayÄ± bekliyordu.\n",
      "\n",
      "Ve salondaki en deÄŸerli eÅŸya kÄŸÄ±tlarÄ±n yanÄ±nda duran, Ã¼Ã§ ayrÄ± kÃ¶ÅŸedeki abajurun Ä±ÅŸÄ±ÄŸÄ±yla hayat bulan, olduÄŸu yere kendini hiÃ§ de ait hissetmeyen ve benim Ã§ok eskilerden hatÄ±rladÄ±ÄŸÄ±m altÄ±n kaplamalÄ± dolmakalemdi.\n",
      "\n",
      "Hareketsiz, \n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP9dmoJDK0gT"
   },
   "source": [
    "### ğŸ­ **Vocabulary Analizi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Metindeki tÃ¼m unique karakterleri bulup vocabulary oluÅŸturuyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`set(text)`:** String'i set'e Ã§evirerek duplicate karakterleri kaldÄ±rÄ±r\n",
    "- **`sorted(list(...))`:** Karakterleri alfabetik sÄ±raya koyar (tutarlÄ±lÄ±k iÃ§in)\n",
    "- **Vocabulary size: 65 karakter**\n",
    "  - Harfler: a-z, A-Z (52 adet)\n",
    "  - Rakamlar: 0-9 (sadece 3 adet gÃ¶rÃ¼nÃ¼yor)\n",
    "  - Noktalama: !$&',-.3:;? ve boÅŸluk\n",
    "- **Character encoding implications:**\n",
    "  - Her karakter bir index alacak (0-64)\n",
    "  - Embedding table 65x(embedding_dim) olacak\n",
    "- **Comparison with word-level:**\n",
    "  - Word vocabulary: 10K-50K+ words\n",
    "  - Character vocabulary: ~65 chars\n",
    "  - Ã‡ok daha kompakt representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !+,-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÃ‡Ã–ÃœÃ§Ã¶Ã¼ÄŸÄ°Ä±ÅÅŸ\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4l6WgZqK0gU"
   },
   "source": [
    "### ğŸ”¢ **Tokenization: String â†” Integer DÃ¶nÃ¼ÅŸÃ¼mÃ¼ - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Karakterleri integer'lara ve integer'larÄ± karakterlere dÃ¶nÃ¼ÅŸtÃ¼ren encoder/decoder fonksiyonlarÄ± oluÅŸturuyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`stoi` (string to integer):** Dictionary mapping karakterden sayÄ±ya\n",
    "- **`itos` (integer to string):** Dictionary mapping sayÄ±dan karaktere\n",
    "- **`encode` lambda fonksiyonu:**\n",
    "  - Input: String (\"hii there\")\n",
    "  - Output: List of integers ([46, 47, 47, 1, 58, 46, 43, 56, 43])\n",
    "  - Her karakter vocabulary'deki index'ine Ã§evriliyor\n",
    "- **`decode` lambda fonksiyonu:**\n",
    "  - Input: List of integers\n",
    "  - Output: Original string\n",
    "  - Reverse operation of encode\n",
    "- **Neural network requirement:**\n",
    "  - NN'ler sayÄ±larla Ã§alÄ±ÅŸÄ±r, metinle deÄŸil\n",
    "  - Bu mapping bidirectional ve lossless olmalÄ±\n",
    "- **Lambda functions:** Concise function definition syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 55, 60, 71, 47, 65, 1, 68, 51, 1, 31, 47, 71, 64, 47]\n",
      "Kinyas ve Kayra\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Kinyas ve Kayra\"))\n",
    "print(decode(encode(\"Kinyas ve Kayra\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmArfg33K0gV"
   },
   "source": [
    "### ğŸ§  **PyTorch Tensor'e DÃ¶nÃ¼ÅŸÃ¼m - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** TÃ¼m metni encode edip PyTorch tensor'Ä±na Ã§eviriyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`torch.tensor()`:** Python list'ini PyTorch tensor'Ä±na Ã§evirir\n",
    "- **`dtype=torch.long`:** 64-bit integer type\n",
    "  - Token index'leri iÃ§in yeterli (0-64 arasÄ±)\n",
    "  - GPU operasyonlarÄ± iÃ§in optimize\n",
    "  - Embedding layer input olarak gerekli\n",
    "- **Tensor shape:** `[1115394]` - 1D tensor\n",
    "- **Memory efficiency:**\n",
    "  - Original text: ~1.1MB (UTF-8 strings)\n",
    "  - Tensor: ~4.4MB (int64 * 1,115,394)\n",
    "  - Trade-off: memory vs. computation speed\n",
    "- **GPU readiness:** Tensor format GPU'ya transfer edilebilir\n",
    "- **Vectorization:** Batch operations iÃ§in hazÄ±r format\n",
    "- **Data type importance:** Wrong dtype â†’ runtime errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1097793]) torch.int64\n",
      "tensor([28, 51, 62, 65, 55,  1, 71, 47, 64, 47, 58, 47, 64,  4,  1, 65, 61, 60,\n",
      "        67, 60, 49, 67, 65, 67,  1, 77, 58, 50, 78, 64, 78, 64,  2,  1, 22, 55,\n",
      "        64, 55, 60, 49, 55,  1, 31, 55, 66, 47, 62,  1, 31, 55, 60, 71, 47, 65,\n",
      "         4,  1, 31, 47, 71, 64, 47,  1, 68, 51,  1, 28, 47, 71, 47, 66,  1, 21,\n",
      "        65, 47, 60, 65, 77, 64,  1, 50, 77, 64, 50, 78, 60, 49, 78,  1, 57, 47,\n",
      "        66, 66, 47,  1, 50, 67, 64, 50, 67,  6,  0,  0, 31, 47, 62, 81, 65, 81,\n",
      "        60, 50, 47,  1,  8, 14,  1, 71, 47, 72, 47, 60,  1, 50, 47, 55, 64, 51,\n",
      "        71, 51,  1, 53, 55, 64, 50, 55, 57,  6,  0,  0, 40, 47, 54, 59, 55, 60,\n",
      "         1, 51, 66, 66, 55, 79, 55, 59,  1, 53, 55, 48, 55,  1, 51, 68, 50, 51,\n",
      "         1, 76, 61, 57,  1, 47, 72,  1, 59, 61, 48, 55, 58, 71, 47,  1, 68, 47,\n",
      "        64, 50, 81,  6,  0,  0, 39, 47, 58, 61, 60, 67, 60,  1, 50, 67, 68, 47,\n",
      "        64, 58, 47, 64, 81,  1, 52, 61, 66, 61, 79, 64, 47, 52, 58, 47, 64,  1,\n",
      "        68, 51,  1, 47, 52, 55, 83, 58, 51, 64, 58, 51,  1, 57, 47, 62, 58, 47,\n",
      "        60, 59, 81, 83, 66, 81,  6,  0,  0, 35, 64, 66, 47, 50, 47,  4,  1, 51,\n",
      "        65, 57, 55, 49, 55, 50, 51, 60,  1, 47, 58, 81, 60, 59, 81, 83,  1, 55,\n",
      "        72, 58, 51, 60, 55, 59, 55,  1, 68, 51, 64, 51, 60,  1, 49, 51, 68, 55,\n",
      "        72,  1, 71, 51, 59, 51, 57,  1, 59, 47, 65, 47, 65, 81,  4,  1, 67, 49,\n",
      "        67, 72,  1, 48, 47, 64, 58, 47, 64, 50, 47,  1, 76, 81, 57, 59, 47, 65,\n",
      "        81,  1, 59, 67, 54, 66, 51, 59, 51, 58,  1, 57, 47, 68, 53, 47, 58, 47,\n",
      "        64, 50, 47,  1, 54, 47, 65, 47, 64, 81,  1, 77, 60, 58, 51, 59, 51, 57,\n",
      "         1, 47, 59, 47, 49, 81, 71, 58, 47,  1, 71, 51, 64, 51,  1, 76, 47, 57,\n",
      "        81, 58, 59, 81, 83, 76, 47, 65, 81, 60, 47,  1, 50, 67, 64, 67, 71, 61,\n",
      "        64, 50, 67,  6,  0,  0, 42, 51,  1, 50, 77, 64, 66,  1, 47, 50, 51, 66,\n",
      "         1, 76, 51, 58, 55, 57,  1, 65, 47, 60, 50, 47, 58, 71, 51,  1, 66, 47,\n",
      "        64, 47, 52, 81, 60, 50, 47, 60,  1, 57, 67, 83, 47, 66, 81, 58, 59, 81,\n",
      "        83, 66, 81,  6,  0,  0, 45, 51, 64, 58, 51, 64, 50, 51,  1, 71, 78, 72,\n",
      "        58, 51, 64, 49, 51,  1, 55, 76, 57, 55,  1, 83, 55, 83, 51, 65, 55,  1,\n",
      "        62, 47, 64, 57, 51, 71, 55,  1, 48, 55, 64,  1, 54, 47, 58, 81,  1, 53,\n",
      "        55, 48, 55,  1, 57, 47, 62, 58, 81, 71, 61, 64, 50, 67,  6,  0,  0, 31,\n",
      "        47, 62, 47, 58, 81,  1, 62, 51, 64, 50, 51, 58, 51, 64, 50, 51, 60,  4,\n",
      "         1, 62, 51, 60, 49, 51, 64, 51, 58, 51, 64, 55, 60,  1, 76, 61, 57,  1,\n",
      "        67, 72, 67, 60,  1, 72, 47, 59, 47, 60, 50, 81, 64,  1, 47, 76, 81, 58,\n",
      "        59, 47, 50, 81, 79, 81,  1, 47, 60, 58, 47, 83, 81, 58, 81, 71, 61, 64,\n",
      "        50, 67,  6,  0,  0, 46, 47, 66, 51, 60,  1, 54, 47, 68, 47, 71, 47,  1,\n",
      "        54, 57, 55, 59,  1, 61, 58, 47, 60,  1, 57, 51, 65, 57, 55, 60,  1, 47,\n",
      "        58, 57, 61, 58,  1, 68, 51,  1, 66, 78, 66, 78, 60,  1, 57, 61, 57, 67,\n",
      "        65, 67,  1, 50, 47,  1, 48, 67, 60, 67,  1, 53, 77, 65, 66, 51, 64, 55,\n",
      "        71, 61, 64, 50, 67,  6,  0,  0, 33, 47, 65, 47, 60, 81, 60,  1, 78, 65,\n",
      "        66, 78, 60, 50, 51, 57, 55,  1, 48, 61, 83,  1, 68, 51,  1, 50, 47, 79,\n",
      "        81, 60, 81, 57,  1, 57, 79, 81, 66, 58, 47, 64,  4,  1, 49, 51, 65, 51,\n",
      "        66, 58, 51, 64,  1, 53, 55, 48, 55,  4,  1, 48, 55, 64, 55, 58, 51, 64,\n",
      "        55,  1, 66, 47, 64, 47, 52, 81, 60, 50, 47, 60,  1, 66, 61, 62, 58, 47,\n",
      "        60, 59, 47, 71, 81,  1, 48, 51, 57, 58, 55, 71, 61, 64, 50, 67,  6,  0,\n",
      "         0, 42, 51,  1, 65, 47, 58, 61, 60, 50, 47, 57, 55,  1, 51, 60,  1, 50,\n",
      "        51, 79, 51, 64, 58, 55,  1, 51, 83, 71, 47,  1, 57, 79, 81, 66, 58, 47,\n",
      "        64, 81, 60,  1, 71, 47, 60, 81, 60, 50, 47,  1, 50, 67, 64, 47, 60,  4,\n",
      "         1, 78, 76,  1, 47, 71, 64, 81,  1, 57, 77, 83, 51, 50, 51, 57, 55,  1,\n",
      "        47, 48, 47, 56, 67, 64, 67, 60,  1, 81, 83, 81, 79, 81, 71, 58, 47,  1,\n",
      "        54, 47, 71, 47, 66,  1, 48, 67, 58, 47, 60,  4,  1, 61, 58, 50, 67, 79,\n",
      "        67,  1, 71, 51, 64, 51,  1, 57, 51, 60, 50, 55, 60, 55,  1, 54, 55, 76,\n",
      "         1, 50, 51,  1, 47, 55, 66,  1, 54, 55, 65, 65, 51, 66, 59, 51, 71, 51,\n",
      "        60,  1, 68, 51,  1, 48, 51, 60, 55, 59,  1, 76, 61, 57,  1, 51, 65, 57,\n",
      "        55, 58, 51, 64, 50, 51, 60,  1, 54, 47, 66, 81, 64, 58, 47, 50, 81, 79,\n",
      "        81, 59,  1, 47, 58, 66, 81, 60,  1, 57, 47, 62, 58, 47, 59, 47, 58, 81,\n",
      "         1, 50, 61, 58, 59, 47, 57, 47, 58, 51, 59, 50, 55,  6,  0,  0, 28, 47,\n",
      "        64, 51, 57, 51, 66, 65, 55, 72,  4,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V--zyB6fK0gW"
   },
   "source": [
    "### ğŸ¯ **Block Size KavramÄ± - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Context window boyutunu 8 karakter olarak belirler ve Ã¶rnek gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Block size = Context length = Sequence length:** AynÄ± kavram\n",
    "- **8 karakterlik window:** Model aynÄ± anda 8 karaktere kadar bakabilir\n",
    "- **tensor([18, 47, 56, 57, 58, 1, 15, 47, 58]):** 9 element\n",
    "  - Ä°lk 8: input context\n",
    "  - Son 8: target predictions (1 shift)\n",
    "- **Sliding window approach:** Her pozisyon iÃ§in prediction\n",
    "- **Transformer limitation:**\n",
    "  - Fixed maximum context length\n",
    "  - Real GPT models: 2048, 4096, 100K+ tokens\n",
    "- **Memory complexity:** O(nÂ²) attention computation\n",
    "- **Training efficiency:** KÃ¼Ã§Ã¼k block size = daha hÄ±zlÄ± training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 51, 62, 65, 55,  1, 71, 47, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D98cEHyrK0gX"
   },
   "source": [
    "### ğŸ¯ **Autoregressive Training YaklaÅŸÄ±mÄ± - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Her pozisyon iÃ§in context-target Ã§iftlerini gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Autoregressive modeling:** Her token, Ã¶nceki tÃ¼m token'lara bakarak tahmin edilir\n",
    "- **8 farklÄ± training example:** Tek sequence'dan 8 Ã¶ÄŸrenme Ã¶rneÄŸi\n",
    "  - Context [18] â†’ Target: 47\n",
    "  - Context [18,47] â†’ Target: 56\n",
    "  - ... vb.\n",
    "- **Teacher forcing:** Training sÄ±rasÄ±nda gerÃ§ek token'larÄ± kullan\n",
    "- **Progressive context:** Giderek daha fazla bilgi veriliyor\n",
    "- **Efficiency:** Tek forward pass'te 8 prediction\n",
    "- **Causal masking:** Gelecek token'larÄ± gÃ¶rme yasaÄŸÄ±\n",
    "- **Maximum likelihood training:** Next token probability maximize edilir\n",
    "- **Sequence modeling temel prensibi:** P(wâ‚,wâ‚‚,...,wâ‚™) = âˆP(wáµ¢|wâ‚,...,wáµ¢â‚‹â‚)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([28]) the target: 51\n",
      "when input is tensor([28, 51]) the target: 62\n",
      "when input is tensor([28, 51, 62]) the target: 65\n",
      "when input is tensor([28, 51, 62, 65]) the target: 55\n",
      "when input is tensor([28, 51, 62, 65, 55]) the target: 1\n",
      "when input is tensor([28, 51, 62, 65, 55,  1]) the target: 71\n",
      "when input is tensor([28, 51, 62, 65, 55,  1, 71]) the target: 47\n",
      "when input is tensor([28, 51, 62, 65, 55,  1, 71, 47]) the target: 64\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUvO98lHK0gX"
   },
   "source": [
    "### ğŸš€ **Batch Processing Sistemi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Mini-batch'ler oluÅŸturup paralel training iÃ§in hazÄ±rlar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Batch size = 4:** 4 farklÄ± sequence paralel iÅŸlenir\n",
    "- **Random sampling:** `torch.randint()` ile rastgele baÅŸlangÄ±Ã§ pozisyonlarÄ±\n",
    "- **Tensor shapes:**\n",
    "  - Input `xb`: [4, 8] - 4 sequence, her biri 8 token\n",
    "  - Target `yb`: [4, 8] - shifted targets\n",
    "- **get_batch() fonksiyonu:**\n",
    "  - Split parametresi: 'train' veya 'val'\n",
    "  - Dynamic data loading\n",
    "  - GPU-ready tensors\n",
    "- **Parallelization benefits:**\n",
    "  - GPU cores'u etkili kullanÄ±m\n",
    "  - Batch normalization iÃ§in gerekli\n",
    "  - Gradient estimation iyileÅŸir\n",
    "- **32 training example:** 4 sequence Ã— 8 position = 32 simultaneous prediction\n",
    "- **Memory vs. Speed trade-off:** BÃ¼yÃ¼k batch = daha fazla memory, daha stabil gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[79, 81,  1, 55, 76, 55, 60,  1],\n",
      "        [71, 58, 51, 59, 51, 57,  1, 55],\n",
      "        [47,  1, 78, 76,  1, 53, 78, 60],\n",
      "        [48, 47, 50, 81, 64,  6,  0,  0]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[81,  1, 55, 76, 55, 60,  1, 71],\n",
      "        [58, 51, 59, 51, 57,  1, 55, 65],\n",
      "        [ 1, 78, 76,  1, 53, 78, 60, 50],\n",
      "        [47, 50, 81, 64,  6,  0,  0, 31]])\n",
      "----\n",
      "when input is [79] the target: 81\n",
      "when input is [79, 81] the target: 1\n",
      "when input is [79, 81, 1] the target: 55\n",
      "when input is [79, 81, 1, 55] the target: 76\n",
      "when input is [79, 81, 1, 55, 76] the target: 55\n",
      "when input is [79, 81, 1, 55, 76, 55] the target: 60\n",
      "when input is [79, 81, 1, 55, 76, 55, 60] the target: 1\n",
      "when input is [79, 81, 1, 55, 76, 55, 60, 1] the target: 71\n",
      "when input is [71] the target: 58\n",
      "when input is [71, 58] the target: 51\n",
      "when input is [71, 58, 51] the target: 59\n",
      "when input is [71, 58, 51, 59] the target: 51\n",
      "when input is [71, 58, 51, 59, 51] the target: 57\n",
      "when input is [71, 58, 51, 59, 51, 57] the target: 1\n",
      "when input is [71, 58, 51, 59, 51, 57, 1] the target: 55\n",
      "when input is [71, 58, 51, 59, 51, 57, 1, 55] the target: 65\n",
      "when input is [47] the target: 1\n",
      "when input is [47, 1] the target: 78\n",
      "when input is [47, 1, 78] the target: 76\n",
      "when input is [47, 1, 78, 76] the target: 1\n",
      "when input is [47, 1, 78, 76, 1] the target: 53\n",
      "when input is [47, 1, 78, 76, 1, 53] the target: 78\n",
      "when input is [47, 1, 78, 76, 1, 53, 78] the target: 60\n",
      "when input is [47, 1, 78, 76, 1, 53, 78, 60] the target: 50\n",
      "when input is [48] the target: 47\n",
      "when input is [48, 47] the target: 50\n",
      "when input is [48, 47, 50] the target: 81\n",
      "when input is [48, 47, 50, 81] the target: 64\n",
      "when input is [48, 47, 50, 81, 64] the target: 6\n",
      "when input is [48, 47, 50, 81, 64, 6] the target: 0\n",
      "when input is [48, 47, 50, 81, 64, 6, 0] the target: 0\n",
      "when input is [48, 47, 50, 81, 64, 6, 0, 0] the target: 31\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymfTScccK0gY"
   },
   "source": [
    "### ğŸ‘ï¸ **Input Tensor Ä°ncelemesi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Transformer'a gidecek input tensor'Ä±nÄ± gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Tensor iÃ§eriÄŸi:** 4Ã—8 matrix, her element bir token ID (0-64 arasÄ±)\n",
    "- **Batch dimension (dim=0):** 4 farklÄ± sequence\n",
    "- **Sequence dimension (dim=1):** Her sequence'ta 8 token\n",
    "- **Token meanings:**\n",
    "  - 24 â†’ 'L', 43 â†’ 'e', 58 â†’ 't', vb.\n",
    "  - GerÃ§ek Shakespeare karakterleri\n",
    "- **No embeddings yet:** Ham token ID'leri, henÃ¼z vector representation'a Ã§evrilmedi\n",
    "- **Transformer input format:** Standard [Batch, Sequence, ...] convention\n",
    "- **Memory layout:** Contiguous tensor, GPU transfer iÃ§in optimize\n",
    "- **Next step:** Bu integer'lar embedding table'dan vector'lara Ã§evrilecek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck3rUNKmK0gY"
   },
   "source": [
    "### ğŸ¤– **Bigram Language Model - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** En basit language model'i implement eder: sadece Ã¶nceki karaktere bakar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Bigram Model:** P(next_char | previous_char) - sadece 1 token geriye bakar\n",
    "- **Architecture:**\n",
    "  - `token_embedding_table`: [vocab_size, vocab_size] = [65, 65]\n",
    "  - Her token ID â†’ probability distribution over next tokens\n",
    "- **Forward pass:**\n",
    "  - Input: token indices [B, T]\n",
    "  - Embedding lookup â†’ logits [B, T, C]\n",
    "  - Cross-entropy loss hesaplanÄ±r\n",
    "- **Loss = 4.8786:** Random baseline ~4.17 (log(65)), biraz daha iyi\n",
    "- **Generate method:**\n",
    "  - Autoregressive sampling\n",
    "  - Multinomial sampling from softmax probabilities\n",
    "  - No temperature control (raw probabilities)\n",
    "- **Limitations:** Ã‡ok kÄ±sa memory, complex patterns Ã¶ÄŸrenemez\n",
    "- **Baseline model:** Daha complex modeller iÃ§in karÅŸÄ±laÅŸtÄ±rma noktasÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 84])\n",
      "tensor(4.8234, grad_fn=<NllLossBackward0>)\n",
      "\n",
      ",F.,-HÃ–;BmNJ,l\n",
      "VMMPmgÃ§RRÃ§O\n",
      "J:SFe6WGSPeÃ–5IZ0e+sÄ±VhJWXÃ¶xmoI!tUÃ¶f8tOnBO-oJRÃ§eXN7!9;WZgPsÄ±ve9ii?Ta;aFJjZ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z863nBcjK0gZ"
   },
   "source": [
    "### âš™ï¸ **Optimizer Kurulumu - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** AdamW optimizer'Ä± learning rate 1e-3 ile kurar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **AdamW (Adam with Weight Decay):**\n",
    "  - Adaptive moment estimation\n",
    "  - Weight decay regularization\n",
    "  - Transformer'lar iÃ§in best practice\n",
    "- **Learning rate 1e-3 = 0.001:**\n",
    "  - Conservative baÅŸlangÄ±Ã§\n",
    "  - Too high â†’ unstable training\n",
    "  - Too low â†’ very slow convergence\n",
    "- **m.parameters():** Model'deki tÃ¼m trainable weights\n",
    "  - Embedding table: 65Ã—65 = 4,225 params\n",
    "  - Bias yok (bias=False)\n",
    "- **Optimizer state:**\n",
    "  - Momentum (first moment)\n",
    "  - Variance (second moment)\n",
    "  - Memory usage ~2x model parameters\n",
    "- **AdamW vs. Adam:** Better generalization with proper weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YUD1DJqmK0gZ",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸƒâ€â™‚ï¸ **Training Loop - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** 100 step basit training loop Ã§alÄ±ÅŸtÄ±rÄ±r.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Training steps:** 100 iteration (Ã§ok az, demo amaÃ§lÄ±)\n",
    "- **Batch size artÄ±ÅŸÄ±:** 32'ye Ã§Ä±karÄ±ldÄ± (daha stabil gradients)\n",
    "- **Training cycle:**\n",
    "  1. `get_batch('train')` â†’ fresh data\n",
    "  2. `model(xb, yb)` â†’ forward pass\n",
    "  3. `optimizer.zero_grad()` â†’ clear old gradients\n",
    "  4. `loss.backward()` â†’ backpropagation\n",
    "  5. `optimizer.step()` â†’ parameter update\n",
    "- **`set_to_none=True`:** Memory efficiency for gradient clearing\n",
    "- **Loss = 4.656:** Slight improvement from 4.878\n",
    "- **Underfitting:** 100 steps Ã§ok az, model capacity'si var ama time yok\n",
    "- **Learning curve:** Monotonic decrease expected with more steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.713395118713379\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "huzlJC8zK0gZ",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ­ **Text Generation Ã–rneÄŸi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Training sonrasÄ± modelden 500 karakter text generate eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Generation process:**\n",
    "  - Start token: `torch.zeros((1,1))` = '\\n' (newline)\n",
    "  - Autoregressive sampling 500 step\n",
    "  - Her step'te en probable next character seÃ§ilir\n",
    "- **Quality assessment:**\n",
    "  - Random gibberish'e benziyor: \"oTo.JUZ!!zqe!\"\n",
    "  - Ã‡ok kÄ±sa training + basit model = poor results\n",
    "  - BazÄ± word-like patterns: \"SKI\", \"AcE\", \"wyn\"\n",
    "- **Bigram limitations gÃ¶rÃ¼lÃ¼yor:**\n",
    "  - No long-term coherence\n",
    "  - No word-level understanding\n",
    "  - Pure statistical character transitions\n",
    "- **Improvement needed:** Daha complex architecture (Transformer!)\n",
    "- **Sampling strategy:** Multinomial (random) vs. greedy vs. top-k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9?PEk:8-JO8 +\n",
      "hXCMnx7Ä±TkjHaV,gÃ§4 o:Ä±ÄŸ4+LI:?imBde2Ä°Z,9aVHÃ§bbO\n",
      ":Ed;nrZ\n",
      "ÅDÄ±8Ã¶Ã¶oFDÃ–ZT6wFJvchi3qbvÃ¼xjC9qb8:RIraO+yMOÃ¼dOApI-U6Mmw+6WvWW0Ã¶jvSsNWuk!;+0zÃ¼\n",
      "dnVG\n",
      "AÅVÃ§1M2qTÃ‡ÄŸtbhlAp1ÅNbÅÄŸÃ¼hoS.2ÄŸ7Ã¶Ã–Ã‡ÄŸp00?ZhoeaEU.5Ya+yyNVgQqÃ–Ã¶8,; J\n",
      "9D\n",
      "0I5zÄŸ29G+XÃ‡ÄŸNKCcUQCRaRvKQdÄŸ4=QNI:O7fÅ\n",
      "CuEe:QdY?ZaxHÄ°Vt,:vivÅÄ°IMÅŸTcRWS15Ou0Ã¶+xAkÃ¼+68 EJqK=;Ã§:A5kWcvÅŸq-Er+AtbÃ¼xJe6V!FNbH72qIJq2iiW iGjHeYYCthsDwH2Ã¼spRDPlSP9?VzLÄ±RtYpZM6xt2ÄŸÃ‡.,Ã¼iSQOT3PRk.H=ÄŸFEÃ‡35:\n",
      "iA9Ã–G!:a\n",
      "pÃ¼PoqdÃœXj,X5zÄŸOT;Ã¶mJÃ¼Ã‡lÄ°KxÃ¶:2ÄŸ3KqXFZy!N19CUÃ–8k6 xÅ9sJÃœ7Ä°8OPSwÄ±SPBÅŸEdTyÃ‡mpwa,Ãœ\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "g1NYoLUbK0ga",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ§® **Weighted Aggregation MatematiÄŸi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Matrix multiplication ile weighted average hesaplamayÄ± gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Lower triangular matrix:**\n",
    "  - `torch.tril()`: Alt Ã¼Ã§gen matrix oluÅŸturur\n",
    "  - Causal masking iÃ§in temel yapÄ±\n",
    "  - Future tokens'a eriÅŸimi engeller\n",
    "- **Normalization:** Her satÄ±r toplamÄ± 1 olacak ÅŸekilde bÃ¶ler\n",
    "  - Row-wise normalization\n",
    "  - Probability distribution creates\n",
    "- **Matrix multiplication magic:**\n",
    "  - `a @ b`: Weighted combination of previous rows\n",
    "  - Efficient vectorized computation\n",
    "  - Broadcasting semantics\n",
    "- **Result interpretation:**\n",
    "  - Row 0: Sadece ilk element (1.0 weight)\n",
    "  - Row 1: Ä°lk iki elementin ortalamasÄ± (0.5, 0.5)\n",
    "  - Row 2: Ä°lk Ã¼Ã§ elementin ortalamasÄ± (0.33, 0.33, 0.33)\n",
    "- **Attention'Ä±n temel matematiksel prensibi!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6Qk--V_PK0ga",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ² **Bag-of-Words Baseline (Version 1) - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Naive approach ile cumulative average hesaplar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Tensor shapes:** [4, 8, 2] = [Batch, Time, Channels]\n",
    "- **Bag-of-words approach:**\n",
    "  - Her pozisyon, Ã¶nceki tÃ¼m pozisyonlarÄ±n ortalamasÄ±\n",
    "  - No positional information preserved\n",
    "  - Order-agnostic representation\n",
    "- **Nested loops:** Inefficient O(BÃ—TÂ²) complexity\n",
    "- **`torch.mean(xprev, 0)`:** Dimension 0 boyunca average\n",
    "- **Communication mechanism:**\n",
    "  - Past tokens \"talk\" to current token\n",
    "  - All past information equally weighted\n",
    "  - No learned attention weights\n",
    "- **Problems:**\n",
    "  - Recent vs. distant tokens treated equally\n",
    "  - No query-key-value mechanism\n",
    "  - Fixed aggregation pattern\n",
    "- **Next: Matrix multiplication optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "jv08tNb3K0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### âš¡ **Matrix Multiplication Optimization (Version 2) - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Loops yerine efficient matrix multiplication kullanÄ±r.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Vectorization:** O(BÃ—TÂ²) loops â†’ O(1) matrix operations\n",
    "- **Broadcasting magic:**\n",
    "  - `wei`: [T, T] matrix\n",
    "  - `x`: [B, T, C] tensor\n",
    "  - Result: [B, T, C] (same as version 1)\n",
    "- **Lower triangular matrix:** Future masking iÃ§in\n",
    "- **Row normalization:** `wei.sum(1, keepdim=True)` â†’ probability weights\n",
    "- **Matrix multiplication semantics:**\n",
    "  - `wei @ x`: [T,T] @ [B,T,C] â†’ [B,T,C]\n",
    "  - Each row of `wei` defines aggregation weights\n",
    "- **GPU optimization:** Vectorized ops Ã§ok daha hÄ±zlÄ±\n",
    "- **Memory efficiency:** Intermediate computation'lar minimize\n",
    "- **torch.allclose():** Numerical precision check\n",
    "- **Same result, much faster!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "noowOPecK0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ¯ **Softmax ile Smooth Attention (Version 3) - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Hard masking yerine soft attention mechanism kullanÄ±r.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Softmax transformation:**\n",
    "  - `float('-inf')` â†’ 0 probability after softmax\n",
    "  - Numeric stability iÃ§in important\n",
    "- **`masked_fill()` function:**\n",
    "  - Condition'a gÃ¶re tensor elements'i replace eder\n",
    "  - `tril == 0` â†’ Future positions\n",
    "  - `-inf` masking â†’ no information leakage\n",
    "- **Softmax properties:**\n",
    "  - Probabilities sum to 1 per row\n",
    "  - Exponential â†’ non-negative values\n",
    "  - Smooth, differentiable function\n",
    "- **Causal attention:** Sadece past ve current tokens visible\n",
    "- **`F.softmax(wei, dim=-1)`:**\n",
    "  - Row-wise normalization\n",
    "  - Last dimension over columns\n",
    "- **Attention weights interpretation:** DÃ¼zgÃ¼n probability distribution\n",
    "- **Gradient flow:** Smooth function â†’ better training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "nGJRqy5BK0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ§  **Self-Attention Mechanism (Version 4) - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** GerÃ§ek self-attention'Ä± Query-Key-Value paradigmasÄ±yla implement eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Q-K-V Architecture:**\n",
    "  - `key = nn.Linear(C, head_size)`: Content-based addressing\n",
    "  - `query = nn.Linear(C, head_size)`: What am I looking for?\n",
    "  - `value = nn.Linear(C, head_size)`: What information to pass?\n",
    "- **Attention score calculation:**\n",
    "  - `wei = q @ k.transpose(-2, -1)`: Query-Key similarities\n",
    "  - Shape: [B, T, head_size] @ [B, head_size, T] â†’ [B, T, T]\n",
    "- **Data-dependent weights:** Ä°Ã§eriÄŸe gÃ¶re attention patterns\n",
    "- **Head size = 16:** Dimension of attention space\n",
    "- **Value transformation:** Sadece raw input deÄŸil, transformed features\n",
    "- **Causal masking:** Future hiding still preserved\n",
    "- **Learnable parameters:** Q, K, V projections trainable\n",
    "- **Information bottleneck:** 32â†’16 dimension reduction\n",
    "- **Output shape:** [B, T, head_size] = [4, 8, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "xqZ8pZQMK0gc",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ“Š **Attention Weights Visualization - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Ã–ÄŸrenilen attention pattern'larÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Attention matrix:** [8Ã—8] causal pattern\n",
    "- **Row interpretation:** Her satÄ±r bir token'Ä±n attention distribution'Ä±\n",
    "- **Column interpretation:** Her sÃ¼tun bir token'Ä±n ne kadar attend edildiÄŸi\n",
    "- **Key observations:**\n",
    "  - Row 0: [1.0, 0, 0, ...] â†’ Sadece kendine bakar\n",
    "  - Row 1: [0.157, 0.843, 0, ...] â†’ Mostly current, some previous\n",
    "  - Row 7: Mixed attention across all previous tokens\n",
    "- **Data-dependent patterns:** Fixed uniform weights deÄŸil, learned!\n",
    "- **Causal structure preserved:** Ãœst Ã¼Ã§gen sÄ±fÄ±r\n",
    "- **Probability distributions:** Her satÄ±r toplamÄ± 1.0\n",
    "- **Information flow:** Past â†’ present, never future\n",
    "- **Dynamic attention:** Content'e gÃ¶re deÄŸiÅŸen weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "GVp_PDehK0gl",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### âš–ï¸ **Scaled Attention & Variance Control - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Attention score'larÄ±nÄ± scale ederek softmax saturation'Ä± Ã¶nler.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`head_size**-0.5`:** Square root scaling (âˆšd_k'nin tersi)\n",
    "- **Random tensor generation:** Unit variance Gaussian initialization\n",
    "- **Variance analysis:**\n",
    "  - `k.var() â‰ˆ 1.0`: Key vectors unit variance\n",
    "  - `q.var() â‰ˆ 1.0`: Query vectors unit variance\n",
    "  - `wei.var() â‰ˆ 1.0`: Scaled attention scores unit variance\n",
    "- **Mathematical reasoning:**\n",
    "  - QÂ·K without scaling â†’ variance = d_k Ã— 1 = 16\n",
    "  - QÂ·K with scaling â†’ variance = d_k Ã— 1 / d_k = 1\n",
    "- **Softmax behavior:**\n",
    "  - Large values â†’ sharp distributions (one-hot like)\n",
    "  - Unit variance â†’ diffuse distributions (better gradients)\n",
    "- **Attention collapse prevention:** Sharp attention â†’ information bottleneck\n",
    "- **Standard practice:** Scaled Dot-Product Attention (Vaswani et al.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rNR-c8YcK0gm",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ“ˆ **Softmax Saturation Demonstration - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Scaling'in softmax behavior Ã¼zerindeki etkisini gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Baseline softmax:** [0.193, 0.143, 0.235, 0.143, 0.287]\n",
    "  - Reasonable distribution\n",
    "  - All values have meaningful contribution\n",
    "- **Scaled by 8:** [0.033, 0.003, 0.162, 0.003, 0.800]\n",
    "  - Sharp, almost one-hot distribution\n",
    "  - Dominant value: 0.8, others minimal\n",
    "- **Softmax temperature effect:**\n",
    "  - Low temperature (high scaling) â†’ sharp distribution\n",
    "  - High temperature (low scaling) â†’ uniform distribution\n",
    "- **Gradient flow implications:**\n",
    "  - Sharp distributions â†’ vanishing gradients for low-probability items\n",
    "  - Diffuse distributions â†’ better gradient propagation\n",
    "- **Information bottleneck:**\n",
    "  - Sharp attention â†’ loses information diversity\n",
    "  - Model becomes overconfident, less robust\n",
    "- **Practical importance:** Scaling prevents attention collapse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RcsUzh4KK0gm",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ”§ **Custom Layer Normalization - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Layer normalization'Ä± sÄ±fÄ±rdan implement eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **LayerNorm vs BatchNorm:**\n",
    "  - BatchNorm: Batch dimension boyunca normalize\n",
    "  - LayerNorm: Feature dimension boyunca normalize (per sample)\n",
    "- **Per-sample normalization:** Her sample kendi istatistikleriyle normalize\n",
    "- **Parameters:**\n",
    "  - `gamma`: Learnable scale parameter (initialized to 1)\n",
    "  - `beta`: Learnable shift parameter (initialized to 0)\n",
    "- **Normalization formula:** `(x - mean) / sqrt(var + eps)`\n",
    "- **`eps=1e-5`:** Numerical stability (division by zero prevention)\n",
    "- **`keepdim=True`:** Dimension preservation for broadcasting\n",
    "- **Output shape:** [32, 100] â†’ same as input\n",
    "- **Transformer importance:** Essential for training stability\n",
    "- **Gradient flow:** Prevents internal covariate shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FiQs0s6DK0gn",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ“Š **Layer Normalization Statistics - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Layer normalization'Ä±n etkisini istatistiksel olarak doÄŸrular.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Feature-wise statistics:** `x[:,0].mean(), x[:,0].std()`\n",
    "  - Ä°lk feature'Ä±n batch boyunca istatistikleri\n",
    "  - Mean: 0.147, Std: 0.880 (normalized deÄŸil)\n",
    "- **Sample-wise statistics:** `x[0,:].mean(), x[0,:].std()`\n",
    "  - Ä°lk sample'Ä±n feature boyunca istatistikleri\n",
    "  - Mean: ~0, Std: 1.0 (perfect normalization!)\n",
    "- **Layer normalization property:**\n",
    "  - Her sample iÃ§in mean=0, std=1 garantisi\n",
    "  - Batch size'dan baÄŸÄ±msÄ±z\n",
    "  - Per-sample consistency\n",
    "- **Numerical precision:** -9.5e-09 â‰ˆ 0 (floating point precision)\n",
    "- **Training stability:** Consistent input distributions to next layer\n",
    "- **Batch independence:** Unlike BatchNorm, no batch dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rÃ©seaux de neurones sont gÃ©niaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bfOm3ZgdK0gn",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸŒ **Encoder-Decoder Architecture Ã–rneÄŸi - DetaylÄ± AÃ§Ä±klama**\n",
    "\n",
    "**Ne yapÄ±yor:** Machine translation iÃ§in tipik transformer kullanÄ±mÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Encoder kÄ±smÄ±:** \"les rÃ©seaux de neurones sont gÃ©niaux!\"\n",
    "  - Kaynak dili (FransÄ±zca) iÅŸler\n",
    "  - Bidirectional attention (her token birbirine bakabilir)\n",
    "  - Sequence'i latent representation'a Ã§evirir\n",
    "- **Decoder kÄ±smÄ±:** \"\\<START\\> neural networks are awesome!\\<END\\>\"\n",
    "  - Hedef dili (Ä°ngilizce) generate eder\n",
    "  - Causal attention (gelecek gÃ¶rÃ¼lmez)\n",
    "  - Encoder'dan bilgi alÄ±r (cross-attention)\n",
    "- **Special tokens:**\n",
    "  - \\<START\\>: Generation baÅŸlangÄ±cÄ±\n",
    "  - \\<END\\>: Sequence bitiÅŸi marker\n",
    "- **Training paradigm:** Teacher forcing ile target'Ä± verip loss hesapla\n",
    "- **Inference:** Autoregressive generation step by step\n",
    "- **Cross-attention:** Decoder queries, encoder keys/values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "gSXJF_eIK0go",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17340,
     "status": "ok",
     "timestamp": 1749632516249,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "2vTiys2DLhQg",
    "outputId": "3661bfcc-8d22-409c-ca54-52b3b7a5d193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1749639824310,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "O-wyzPgsK-40"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "grad_clip = 1.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749637467548,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "BRd6stqMLBbb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('/content/drive/MyDrive/ML-MODELS/GPT/GPT - Base/kinyas_kayra_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078394,
     "status": "ok",
     "timestamp": 1749638561545,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "vX1dxQw8Or2f",
    "outputId": "d2c923c5-48c3-4690-8511-32d7f906ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('196', '177')\n",
      "Merge 101: ('197159', '101')\n",
      "Merge 201: ('105', '104')\n",
      "Merge 301: ('075', '105110121097')\n",
      "Merge 401: ('097114', '107')\n",
      "Merge 501: ('116', '097114097')\n",
      "Merge 601: ('108097', '121097')\n",
      "Merge 701: ('111108109097', '108196177')\n",
      "Merge 801: ('196176', '107105')\n",
      "Merge 901: ('105110', '099105')\n",
      "Merge 1001: ('097110', '110101')\n",
      "Merge 1101: ('107097114', '196177')\n",
      "Merge 1201: ('103', '195188108')\n",
      "Merge 1301: ('117196159', '114097')\n",
      "Merge 1401: ('107097', '102')\n",
      "Merge 1501: ('100097110', '046')\n",
      "Merge 1601: ('112', '105122')\n",
      "Merge 1701: ('098097', '122196177')\n",
      "Merge 1801: ('076', '111')\n",
      "Merge 1901: ('197159', '097110')\n",
      "Merge 2001: ('097', '105116')\n",
      "Merge 2101: ('098101110122101', '121101110')\n",
      "Merge 2201: ('101116', '116105109')\n",
      "Merge 2301: ('116105', '116114101')\n",
      "Merge 2401: ('100195188110121097', '121097')\n",
      "Merge 2501: ('111108117114', '100117')\n",
      "Merge 2601: ('107097114197159196177', '108196177196159196177110100097')\n",
      "Merge 2701: ('107097108196177', '114')\n",
      "Merge 2801: ('100117114117109', '100097')\n",
      "Merge 2901: ('115097', '116196177')\n",
      "Merge 3001: ('100117', '109044')\n",
      "Merge 3101: ('103101114101107105114', '100105046')\n",
      "Merge 3201: ('107097', '115')\n",
      "Merge 3301: ('118', '111108')\n",
      "Merge 3401: ('118101114115105', '116101')\n",
      "Merge 3501: ('083097', '195167')\n",
      "Merge 3601: ('109097', '116105107')\n",
      "Merge 3701: ('098101107', '108101114')\n",
      "Merge 3801: ('080', '111108')\n",
      "Merge 3901: ('098101107108101', '109101107')\n",
      "Merge 4001: ('105195167', '101114')\n",
      "Merge 4101: ('104097102196177', '122097109')\n",
      "Merge 4201: ('066', '111197159')\n",
      "Merge 4301: ('100117', '197159')\n",
      "Merge 4401: ('117110117', '116097')\n",
      "Merge 4501: ('109097107121097', '106')\n",
      "Merge 4601: ('068111107', '116111114')\n",
      "Merge 4701: ('109097', '109196177197159116196177046')\n",
      "Merge 4801: ('121101114', '100101044')\n",
      "Merge 4901: ('084195188114', '107105')\n",
      "Merge 5001: ('108101114', '105109100101')\n",
      "Merge 5101: ('107097112196177', '100097110')\n",
      "Merge 5201: ('108097114', '107101110')\n",
      "Merge 5301: ('111108109097', '115196177110196177110')\n",
      "Merge 5401: ('195135196177', '107')\n",
      "Merge 5501: ('097115', '121111110')\n",
      "Merge 5601: ('072', '097107108196177')\n",
      "Merge 5701: ('101116', '109101110')\n",
      "Merge 5801: ('069108', '108101114105110105')\n",
      "Merge 5901: ('085109', '097114196177109')\n",
      "Merge 6001: ('116195188107', '046')\n",
      "Merge 6101: ('100111115', '116117')\n",
      "Merge 6201: ('107097121110097107', '108097110196177121111114100117046')\n",
      "Merge 6301: ('116097', '115196177')\n",
      "Merge 6401: ('065121', '110')\n",
      "Merge 6501: ('109097115196177', '121108097')\n",
      "Merge 6601: ('098097197159', '097114097')\n",
      "Merge 6701: ('104097121097116', '108097114196177')\n",
      "Merge 6801: ('085121107117', '115117122')\n",
      "Merge 6901: ('115', '105110121097')\n",
      "Merge 7001: ('121111108', '108097109097107')\n",
      "Merge 7101: ('111121110', '097114')\n",
      "Merge 7201: ('100117118097114', '196177110097')\n",
      "Merge 7301: ('109195188122105196159', '105')\n",
      "Merge 7401: ('107097108', '109097121097')\n",
      "Merge 7501: ('098105108109101', '109101107')\n",
      "Merge 7601: ('089097114', '100196177109')\n",
      "Merge 7701: ('100101196159105197159', '109105197159116105046')\n",
      "Merge 7801: ('086101114', '100105196159105')\n",
      "Merge 7901: ('103195182196159115', '195188109101')\n",
      "Merge 8000: ('103105114', '109101115105')\n",
      "Encoded: [635]\n",
      "Decoded: Kinyas\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class ByteLevelBPE:\n",
    "    def __init__(self, text, num_merges=500):\n",
    "        self.text = text\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = None\n",
    "        self.merges = []\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self._learn_bpe()\n",
    "        self._build_token_vocab()\n",
    "\n",
    "    def _get_vocab(self):\n",
    "        vocab = Counter()\n",
    "        words = self.text.strip().split()\n",
    "\n",
    "        for word in words:\n",
    "            word_bytes = list(word.encode('utf-8'))\n",
    "            word_bytes_str = [f\"{b:03d}\" for b in word_bytes]\n",
    "            tokenized = ' '.join(word_bytes_str + ['</w>'])\n",
    "            vocab[tokenized] += 1\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                if symbols[i+1] == '</w>':\n",
    "                    continue\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, vocab_in):\n",
    "        vocab_out = {}\n",
    "        replacement = pair[0] + pair[1]\n",
    "\n",
    "        for word, freq in vocab_in.items():\n",
    "            symbols = word.split()\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == pair:\n",
    "                    new_symbols.append(replacement)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "\n",
    "            new_word = ' '.join(new_symbols)\n",
    "            vocab_out[new_word] = freq\n",
    "\n",
    "        return vocab_out\n",
    "\n",
    "    def _learn_bpe(self):\n",
    "        self.vocab = self._get_vocab()\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self._get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self._merge_vocab(best, self.vocab)\n",
    "            self.merges.append(best)\n",
    "            if i % 100 == 0 or i == self.num_merges - 1:\n",
    "                print(f\"Merge {i+1}: {best}\")\n",
    "\n",
    "        self.merges_set = set(self.merges)\n",
    "\n",
    "    def _build_token_vocab(self):\n",
    "        # BPE tokenlarÄ±: baÅŸlangÄ±Ã§ta tÃ¼m byte kodlarÄ± + merged tokenlar\n",
    "        tokens = set()\n",
    "        # TÃ¼m kelimeler\n",
    "        for word in self.vocab.keys():\n",
    "            for token in word.split():\n",
    "                tokens.add(token)\n",
    "        # AyrÄ±ca merge'lerle oluÅŸan tokenlar\n",
    "        for a,b in self.merges:\n",
    "            tokens.add(a+b)\n",
    "        tokens.discard('</w>')  # </w> genelde tokenize edilmez veya farklÄ± iÅŸlenir\n",
    "        tokens = sorted(list(tokens))\n",
    "        self.token_to_id = {tok: idx for idx, tok in enumerate(tokens)}\n",
    "        self.id_to_token = {idx: tok for tok, idx in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_bytes = [f\"{b:03d}\" for b in word.encode('utf-8')] + ['</w>']\n",
    "\n",
    "        while True:\n",
    "            pairs = [(word_bytes[i], word_bytes[i+1]) for i in range(len(word_bytes)-1)]\n",
    "            mergeable = [p for p in pairs if p in self.merges]\n",
    "\n",
    "            if not mergeable:\n",
    "                break\n",
    "\n",
    "            best = None\n",
    "            for merge in self.merges:\n",
    "                if merge in pairs:\n",
    "                    best = merge\n",
    "                    break\n",
    "\n",
    "            if best is None:\n",
    "                break\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word_bytes):\n",
    "                if i < len(word_bytes) - 1 and (word_bytes[i], word_bytes[i+1]) == best:\n",
    "                    new_word.append(word_bytes[i] + word_bytes[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word_bytes[i])\n",
    "                    i += 1\n",
    "\n",
    "            word_bytes = new_word\n",
    "\n",
    "        encoded_ids = []\n",
    "        for token in word_bytes:\n",
    "            if token == '</w>':\n",
    "                continue\n",
    "            encoded_ids.append(self.token_to_id[token])\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.id_to_token[id_] for id_ in token_ids]\n",
    "        byte_sequence = []\n",
    "        for token in tokens:\n",
    "            for i in range(0, len(token), 3):\n",
    "                byte_sequence.append(int(token[i:i+3]))\n",
    "        return bytes(byte_sequence).decode('utf-8', errors='replace')\n",
    "\n",
    "bpe = ByteLevelBPE(text, num_merges=8000)\n",
    "\n",
    "word = \"Kinyas\"\n",
    "encoded = bpe.encode(word)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "decoded = bpe.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "#chars = sorted(list(set(text)))\n",
    "#vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "#stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "#itos = { i:ch for i,ch in enumerate(chars) }\n",
    "#encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "#decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1255381,
     "status": "ok",
     "timestamp": 1749641238803,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "R0Les052W0UR",
    "outputId": "f57ca121-89da-4e1e-f1aa-0c3671831796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token: 213475\n",
      "Train data size: 192127\n",
      "Val data size: 21348\n"
     ]
    }
   ],
   "source": [
    "def encode_text_with_bpe_ids(bpe_obj, text):\n",
    "    tokens = []\n",
    "    for word in text.strip().split():\n",
    "        tokens.extend(bpe_obj.encode(word))\n",
    "    return tokens\n",
    "\n",
    "tokens = encode_text_with_bpe_ids(bpe, text)\n",
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Total token: {len(data)}\")\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "# Train and test splits\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "#train_data = data[:n]\n",
    "#val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1749642394519,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "TLhTmjxnLEco"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749642397145,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "xsw2pAgALFgS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642398317,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "j9c7A4D3LHLF"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642399521,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "nrGYKFB8LI3p"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642400568,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "gbflotPaLKLs"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749642401840,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "6EoQ-B-pLLbI"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1749642403088,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "_V5Li4w_LNhx"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            logits = logits / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_v = v[:, -1].unsqueeze(1)\n",
    "                logits = torch.where(logits < min_v, torch.full_like(logits, -float('Inf')), logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749642405237,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "03xzxp-QZ3Nm",
    "outputId": "c746a857-74b5-4234-9ab8-6d2363f41d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8071"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(bpe.token_to_id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1749642407293,
     "user": {
      "displayName": "Mehmet YÄ±ldÄ±rÄ±m",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "hoelkOrFY8bN",
    "outputId": "4a347330-c096-4e6a-b09b-90b6e3b2fddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.945543 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTvQXsPFh05H",
    "outputId": "aca584a3-8d01-4e45-89a4-3177fcea87f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train 9.1643, Val 9.1696, LR 0.000000\n",
      "âœ¨ Best model saved.\n",
      "Step 500: Train 4.3856, Val 7.1656, LR 0.001000\n",
      "âœ¨ Best model saved.\n",
      "Step 1000: Train 1.3357, Val 9.2076, LR 0.000970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/bpe_transformer\")\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-3, total_iters=5000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Learning rate scheduler\n",
    "    lr = get_lr(iter)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    # DeÄŸerlendirme ve log\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "\n",
    "        print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, iter)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, iter)\n",
    "        writer.add_scalar(\"Learning Rate\", lr, iter)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"âœ¨ Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"ğŸ›‘ Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # EÄŸitim adÄ±mÄ±\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "#for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "#    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#        losses = estimate_loss()\n",
    "#        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "#    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "#    logits, loss = model(xb, yb)\n",
    "#    optimizer.zero_grad(set_to_none=True)\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": [
    "prompt = \"Kinyas ve\"\n",
    "prompt_tokens = []\n",
    "for w in prompt.strip().split():\n",
    "    prompt_tokens.extend(bpe.encode(w))\n",
    "context = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(context, max_new_tokens=50, temperature=0.7, top_k=50)[0].tolist()\n",
    "print(\"Generated text:\")\n",
    "print(bpe.decode(generated_ids))\n",
    "\n",
    "# generate from the model\n",
    "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
