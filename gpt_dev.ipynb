{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('kinyas_kayra_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of dataset in characters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtext\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#1115394\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))\n",
    "#1115394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepsi yaralar, sonuncusu öldürür! Birinci Kitap Kinyas, Kayra ve Hayat Asansör dördüncü katta durdu.\n",
      "\n",
      "Kapısında 17 yazan daireye girdik.\n",
      "\n",
      "Tahmin ettiğim gibi evde çok az mobilya vardı.\n",
      "\n",
      "Salonun duvarları fotoğraflar ve afişlerle kaplanmıştı.\n",
      "\n",
      "Ortada, eskiciden alınmış izlenimi veren ceviz yemek masası, ucuz barlarda çıkması muhtemel kavgalarda hasarı önlemek amacıyla yere çakılmışçasına duruyordu.\n",
      "\n",
      "Ve dört adet çelik sandalye tarafından kuşatılmıştı.\n",
      "\n",
      "Yerlerde yüzlerce içki şişesi parkeyi bir halı gibi kaplıyordu.\n",
      "\n",
      "Kapalı perdelerden, pencerelerin çok uzun zamandır açılmadığı anlaşılıyordu.\n",
      "\n",
      "Zaten havaya hkim olan keskin alkol ve tütün kokusu da bunu gösteriyordu.\n",
      "\n",
      "Masanın üstündeki boş ve dağınık kğıtlar, cesetler gibi, birileri tarafından toplanmayı bekliyordu.\n",
      "\n",
      "Ve salondaki en değerli eşya kğıtların yanında duran, üç ayrı köşedeki abajurun ışığıyla hayat bulan, olduğu yere kendini hiç de ait hissetmeyen ve benim çok eskilerden hatırladığım altın kaplamalı dolmakalemdi.\n",
      "\n",
      "Hareketsiz, \n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP9dmoJDK0gT"
   },
   "source": [
    "### 🎭 **Vocabulary Analizi - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Metindeki tüm unique karakterleri bulup vocabulary oluşturuyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`set(text)`:** String'i set'e çevirerek duplicate karakterleri kaldırır\n",
    "- **`sorted(list(...))`:** Karakterleri alfabetik sıraya koyar (tutarlılık için)\n",
    "- **Vocabulary size: 65 karakter**\n",
    "  - Harfler: a-z, A-Z (52 adet)\n",
    "  - Rakamlar: 0-9 (sadece 3 adet görünüyor)\n",
    "  - Noktalama: !$&',-.3:;? ve boşluk\n",
    "- **Character encoding implications:**\n",
    "  - Her karakter bir index alacak (0-64)\n",
    "  - Embedding table 65x(embedding_dim) olacak\n",
    "- **Comparison with word-level:**\n",
    "  - Word vocabulary: 10K-50K+ words\n",
    "  - Character vocabulary: ~65 chars\n",
    "  - Çok daha kompakt representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !+,-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÇÖÜçöüğİıŞş\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4l6WgZqK0gU"
   },
   "source": [
    "### 🔢 **Tokenization: String ↔ Integer Dönüşümü - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Karakterleri integer'lara ve integer'ları karakterlere dönüştüren encoder/decoder fonksiyonları oluşturuyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`stoi` (string to integer):** Dictionary mapping karakterden sayıya\n",
    "- **`itos` (integer to string):** Dictionary mapping sayıdan karaktere\n",
    "- **`encode` lambda fonksiyonu:**\n",
    "  - Input: String (\"hii there\")\n",
    "  - Output: List of integers ([46, 47, 47, 1, 58, 46, 43, 56, 43])\n",
    "  - Her karakter vocabulary'deki index'ine çevriliyor\n",
    "- **`decode` lambda fonksiyonu:**\n",
    "  - Input: List of integers\n",
    "  - Output: Original string\n",
    "  - Reverse operation of encode\n",
    "- **Neural network requirement:**\n",
    "  - NN'ler sayılarla çalışır, metinle değil\n",
    "  - Bu mapping bidirectional ve lossless olmalı\n",
    "- **Lambda functions:** Concise function definition syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 55, 60, 71, 47, 65, 1, 68, 51, 1, 31, 47, 71, 64, 47]\n",
      "Kinyas ve Kayra\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Kinyas ve Kayra\"))\n",
    "print(decode(encode(\"Kinyas ve Kayra\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmArfg33K0gV"
   },
   "source": [
    "### 🧠 **PyTorch Tensor'e Dönüşüm - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Tüm metni encode edip PyTorch tensor'ına çeviriyor.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`torch.tensor()`:** Python list'ini PyTorch tensor'ına çevirir\n",
    "- **`dtype=torch.long`:** 64-bit integer type\n",
    "  - Token index'leri için yeterli (0-64 arası)\n",
    "  - GPU operasyonları için optimize\n",
    "  - Embedding layer input olarak gerekli\n",
    "- **Tensor shape:** `[1115394]` - 1D tensor\n",
    "- **Memory efficiency:**\n",
    "  - Original text: ~1.1MB (UTF-8 strings)\n",
    "  - Tensor: ~4.4MB (int64 * 1,115,394)\n",
    "  - Trade-off: memory vs. computation speed\n",
    "- **GPU readiness:** Tensor format GPU'ya transfer edilebilir\n",
    "- **Vectorization:** Batch operations için hazır format\n",
    "- **Data type importance:** Wrong dtype → runtime errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1097793]) torch.int64\n",
      "tensor([28, 51, 62, 65, 55,  1, 71, 47, 64, 47, 58, 47, 64,  4,  1, 65, 61, 60,\n",
      "        67, 60, 49, 67, 65, 67,  1, 77, 58, 50, 78, 64, 78, 64,  2,  1, 22, 55,\n",
      "        64, 55, 60, 49, 55,  1, 31, 55, 66, 47, 62,  1, 31, 55, 60, 71, 47, 65,\n",
      "         4,  1, 31, 47, 71, 64, 47,  1, 68, 51,  1, 28, 47, 71, 47, 66,  1, 21,\n",
      "        65, 47, 60, 65, 77, 64,  1, 50, 77, 64, 50, 78, 60, 49, 78,  1, 57, 47,\n",
      "        66, 66, 47,  1, 50, 67, 64, 50, 67,  6,  0,  0, 31, 47, 62, 81, 65, 81,\n",
      "        60, 50, 47,  1,  8, 14,  1, 71, 47, 72, 47, 60,  1, 50, 47, 55, 64, 51,\n",
      "        71, 51,  1, 53, 55, 64, 50, 55, 57,  6,  0,  0, 40, 47, 54, 59, 55, 60,\n",
      "         1, 51, 66, 66, 55, 79, 55, 59,  1, 53, 55, 48, 55,  1, 51, 68, 50, 51,\n",
      "         1, 76, 61, 57,  1, 47, 72,  1, 59, 61, 48, 55, 58, 71, 47,  1, 68, 47,\n",
      "        64, 50, 81,  6,  0,  0, 39, 47, 58, 61, 60, 67, 60,  1, 50, 67, 68, 47,\n",
      "        64, 58, 47, 64, 81,  1, 52, 61, 66, 61, 79, 64, 47, 52, 58, 47, 64,  1,\n",
      "        68, 51,  1, 47, 52, 55, 83, 58, 51, 64, 58, 51,  1, 57, 47, 62, 58, 47,\n",
      "        60, 59, 81, 83, 66, 81,  6,  0,  0, 35, 64, 66, 47, 50, 47,  4,  1, 51,\n",
      "        65, 57, 55, 49, 55, 50, 51, 60,  1, 47, 58, 81, 60, 59, 81, 83,  1, 55,\n",
      "        72, 58, 51, 60, 55, 59, 55,  1, 68, 51, 64, 51, 60,  1, 49, 51, 68, 55,\n",
      "        72,  1, 71, 51, 59, 51, 57,  1, 59, 47, 65, 47, 65, 81,  4,  1, 67, 49,\n",
      "        67, 72,  1, 48, 47, 64, 58, 47, 64, 50, 47,  1, 76, 81, 57, 59, 47, 65,\n",
      "        81,  1, 59, 67, 54, 66, 51, 59, 51, 58,  1, 57, 47, 68, 53, 47, 58, 47,\n",
      "        64, 50, 47,  1, 54, 47, 65, 47, 64, 81,  1, 77, 60, 58, 51, 59, 51, 57,\n",
      "         1, 47, 59, 47, 49, 81, 71, 58, 47,  1, 71, 51, 64, 51,  1, 76, 47, 57,\n",
      "        81, 58, 59, 81, 83, 76, 47, 65, 81, 60, 47,  1, 50, 67, 64, 67, 71, 61,\n",
      "        64, 50, 67,  6,  0,  0, 42, 51,  1, 50, 77, 64, 66,  1, 47, 50, 51, 66,\n",
      "         1, 76, 51, 58, 55, 57,  1, 65, 47, 60, 50, 47, 58, 71, 51,  1, 66, 47,\n",
      "        64, 47, 52, 81, 60, 50, 47, 60,  1, 57, 67, 83, 47, 66, 81, 58, 59, 81,\n",
      "        83, 66, 81,  6,  0,  0, 45, 51, 64, 58, 51, 64, 50, 51,  1, 71, 78, 72,\n",
      "        58, 51, 64, 49, 51,  1, 55, 76, 57, 55,  1, 83, 55, 83, 51, 65, 55,  1,\n",
      "        62, 47, 64, 57, 51, 71, 55,  1, 48, 55, 64,  1, 54, 47, 58, 81,  1, 53,\n",
      "        55, 48, 55,  1, 57, 47, 62, 58, 81, 71, 61, 64, 50, 67,  6,  0,  0, 31,\n",
      "        47, 62, 47, 58, 81,  1, 62, 51, 64, 50, 51, 58, 51, 64, 50, 51, 60,  4,\n",
      "         1, 62, 51, 60, 49, 51, 64, 51, 58, 51, 64, 55, 60,  1, 76, 61, 57,  1,\n",
      "        67, 72, 67, 60,  1, 72, 47, 59, 47, 60, 50, 81, 64,  1, 47, 76, 81, 58,\n",
      "        59, 47, 50, 81, 79, 81,  1, 47, 60, 58, 47, 83, 81, 58, 81, 71, 61, 64,\n",
      "        50, 67,  6,  0,  0, 46, 47, 66, 51, 60,  1, 54, 47, 68, 47, 71, 47,  1,\n",
      "        54, 57, 55, 59,  1, 61, 58, 47, 60,  1, 57, 51, 65, 57, 55, 60,  1, 47,\n",
      "        58, 57, 61, 58,  1, 68, 51,  1, 66, 78, 66, 78, 60,  1, 57, 61, 57, 67,\n",
      "        65, 67,  1, 50, 47,  1, 48, 67, 60, 67,  1, 53, 77, 65, 66, 51, 64, 55,\n",
      "        71, 61, 64, 50, 67,  6,  0,  0, 33, 47, 65, 47, 60, 81, 60,  1, 78, 65,\n",
      "        66, 78, 60, 50, 51, 57, 55,  1, 48, 61, 83,  1, 68, 51,  1, 50, 47, 79,\n",
      "        81, 60, 81, 57,  1, 57, 79, 81, 66, 58, 47, 64,  4,  1, 49, 51, 65, 51,\n",
      "        66, 58, 51, 64,  1, 53, 55, 48, 55,  4,  1, 48, 55, 64, 55, 58, 51, 64,\n",
      "        55,  1, 66, 47, 64, 47, 52, 81, 60, 50, 47, 60,  1, 66, 61, 62, 58, 47,\n",
      "        60, 59, 47, 71, 81,  1, 48, 51, 57, 58, 55, 71, 61, 64, 50, 67,  6,  0,\n",
      "         0, 42, 51,  1, 65, 47, 58, 61, 60, 50, 47, 57, 55,  1, 51, 60,  1, 50,\n",
      "        51, 79, 51, 64, 58, 55,  1, 51, 83, 71, 47,  1, 57, 79, 81, 66, 58, 47,\n",
      "        64, 81, 60,  1, 71, 47, 60, 81, 60, 50, 47,  1, 50, 67, 64, 47, 60,  4,\n",
      "         1, 78, 76,  1, 47, 71, 64, 81,  1, 57, 77, 83, 51, 50, 51, 57, 55,  1,\n",
      "        47, 48, 47, 56, 67, 64, 67, 60,  1, 81, 83, 81, 79, 81, 71, 58, 47,  1,\n",
      "        54, 47, 71, 47, 66,  1, 48, 67, 58, 47, 60,  4,  1, 61, 58, 50, 67, 79,\n",
      "        67,  1, 71, 51, 64, 51,  1, 57, 51, 60, 50, 55, 60, 55,  1, 54, 55, 76,\n",
      "         1, 50, 51,  1, 47, 55, 66,  1, 54, 55, 65, 65, 51, 66, 59, 51, 71, 51,\n",
      "        60,  1, 68, 51,  1, 48, 51, 60, 55, 59,  1, 76, 61, 57,  1, 51, 65, 57,\n",
      "        55, 58, 51, 64, 50, 51, 60,  1, 54, 47, 66, 81, 64, 58, 47, 50, 81, 79,\n",
      "        81, 59,  1, 47, 58, 66, 81, 60,  1, 57, 47, 62, 58, 47, 59, 47, 58, 81,\n",
      "         1, 50, 61, 58, 59, 47, 57, 47, 58, 51, 59, 50, 55,  6,  0,  0, 28, 47,\n",
      "        64, 51, 57, 51, 66, 65, 55, 72,  4,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V--zyB6fK0gW"
   },
   "source": [
    "### 🎯 **Block Size Kavramı - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Context window boyutunu 8 karakter olarak belirler ve örnek gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Block size = Context length = Sequence length:** Aynı kavram\n",
    "- **8 karakterlik window:** Model aynı anda 8 karaktere kadar bakabilir\n",
    "- **tensor([18, 47, 56, 57, 58, 1, 15, 47, 58]):** 9 element\n",
    "  - İlk 8: input context\n",
    "  - Son 8: target predictions (1 shift)\n",
    "- **Sliding window approach:** Her pozisyon için prediction\n",
    "- **Transformer limitation:**\n",
    "  - Fixed maximum context length\n",
    "  - Real GPT models: 2048, 4096, 100K+ tokens\n",
    "- **Memory complexity:** O(n²) attention computation\n",
    "- **Training efficiency:** Küçük block size = daha hızlı training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28, 51, 62, 65, 55,  1, 71, 47, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D98cEHyrK0gX"
   },
   "source": [
    "### 🎯 **Autoregressive Training Yaklaşımı - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Her pozisyon için context-target çiftlerini gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Autoregressive modeling:** Her token, önceki tüm token'lara bakarak tahmin edilir\n",
    "- **8 farklı training example:** Tek sequence'dan 8 öğrenme örneği\n",
    "  - Context [18] → Target: 47\n",
    "  - Context [18,47] → Target: 56\n",
    "  - ... vb.\n",
    "- **Teacher forcing:** Training sırasında gerçek token'ları kullan\n",
    "- **Progressive context:** Giderek daha fazla bilgi veriliyor\n",
    "- **Efficiency:** Tek forward pass'te 8 prediction\n",
    "- **Causal masking:** Gelecek token'ları görme yasağı\n",
    "- **Maximum likelihood training:** Next token probability maximize edilir\n",
    "- **Sequence modeling temel prensibi:** P(w₁,w₂,...,wₙ) = ∏P(wᵢ|w₁,...,wᵢ₋₁)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([28]) the target: 51\n",
      "when input is tensor([28, 51]) the target: 62\n",
      "when input is tensor([28, 51, 62]) the target: 65\n",
      "when input is tensor([28, 51, 62, 65]) the target: 55\n",
      "when input is tensor([28, 51, 62, 65, 55]) the target: 1\n",
      "when input is tensor([28, 51, 62, 65, 55,  1]) the target: 71\n",
      "when input is tensor([28, 51, 62, 65, 55,  1, 71]) the target: 47\n",
      "when input is tensor([28, 51, 62, 65, 55,  1, 71, 47]) the target: 64\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUvO98lHK0gX"
   },
   "source": [
    "### 🚀 **Batch Processing Sistemi - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Mini-batch'ler oluşturup paralel training için hazırlar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Batch size = 4:** 4 farklı sequence paralel işlenir\n",
    "- **Random sampling:** `torch.randint()` ile rastgele başlangıç pozisyonları\n",
    "- **Tensor shapes:**\n",
    "  - Input `xb`: [4, 8] - 4 sequence, her biri 8 token\n",
    "  - Target `yb`: [4, 8] - shifted targets\n",
    "- **get_batch() fonksiyonu:**\n",
    "  - Split parametresi: 'train' veya 'val'\n",
    "  - Dynamic data loading\n",
    "  - GPU-ready tensors\n",
    "- **Parallelization benefits:**\n",
    "  - GPU cores'u etkili kullanım\n",
    "  - Batch normalization için gerekli\n",
    "  - Gradient estimation iyileşir\n",
    "- **32 training example:** 4 sequence × 8 position = 32 simultaneous prediction\n",
    "- **Memory vs. Speed trade-off:** Büyük batch = daha fazla memory, daha stabil gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[79, 81,  1, 55, 76, 55, 60,  1],\n",
      "        [71, 58, 51, 59, 51, 57,  1, 55],\n",
      "        [47,  1, 78, 76,  1, 53, 78, 60],\n",
      "        [48, 47, 50, 81, 64,  6,  0,  0]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[81,  1, 55, 76, 55, 60,  1, 71],\n",
      "        [58, 51, 59, 51, 57,  1, 55, 65],\n",
      "        [ 1, 78, 76,  1, 53, 78, 60, 50],\n",
      "        [47, 50, 81, 64,  6,  0,  0, 31]])\n",
      "----\n",
      "when input is [79] the target: 81\n",
      "when input is [79, 81] the target: 1\n",
      "when input is [79, 81, 1] the target: 55\n",
      "when input is [79, 81, 1, 55] the target: 76\n",
      "when input is [79, 81, 1, 55, 76] the target: 55\n",
      "when input is [79, 81, 1, 55, 76, 55] the target: 60\n",
      "when input is [79, 81, 1, 55, 76, 55, 60] the target: 1\n",
      "when input is [79, 81, 1, 55, 76, 55, 60, 1] the target: 71\n",
      "when input is [71] the target: 58\n",
      "when input is [71, 58] the target: 51\n",
      "when input is [71, 58, 51] the target: 59\n",
      "when input is [71, 58, 51, 59] the target: 51\n",
      "when input is [71, 58, 51, 59, 51] the target: 57\n",
      "when input is [71, 58, 51, 59, 51, 57] the target: 1\n",
      "when input is [71, 58, 51, 59, 51, 57, 1] the target: 55\n",
      "when input is [71, 58, 51, 59, 51, 57, 1, 55] the target: 65\n",
      "when input is [47] the target: 1\n",
      "when input is [47, 1] the target: 78\n",
      "when input is [47, 1, 78] the target: 76\n",
      "when input is [47, 1, 78, 76] the target: 1\n",
      "when input is [47, 1, 78, 76, 1] the target: 53\n",
      "when input is [47, 1, 78, 76, 1, 53] the target: 78\n",
      "when input is [47, 1, 78, 76, 1, 53, 78] the target: 60\n",
      "when input is [47, 1, 78, 76, 1, 53, 78, 60] the target: 50\n",
      "when input is [48] the target: 47\n",
      "when input is [48, 47] the target: 50\n",
      "when input is [48, 47, 50] the target: 81\n",
      "when input is [48, 47, 50, 81] the target: 64\n",
      "when input is [48, 47, 50, 81, 64] the target: 6\n",
      "when input is [48, 47, 50, 81, 64, 6] the target: 0\n",
      "when input is [48, 47, 50, 81, 64, 6, 0] the target: 0\n",
      "when input is [48, 47, 50, 81, 64, 6, 0, 0] the target: 31\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymfTScccK0gY"
   },
   "source": [
    "### 👁️ **Input Tensor İncelemesi - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Transformer'a gidecek input tensor'ını gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Tensor içeriği:** 4×8 matrix, her element bir token ID (0-64 arası)\n",
    "- **Batch dimension (dim=0):** 4 farklı sequence\n",
    "- **Sequence dimension (dim=1):** Her sequence'ta 8 token\n",
    "- **Token meanings:**\n",
    "  - 24 → 'L', 43 → 'e', 58 → 't', vb.\n",
    "  - Gerçek Shakespeare karakterleri\n",
    "- **No embeddings yet:** Ham token ID'leri, henüz vector representation'a çevrilmedi\n",
    "- **Transformer input format:** Standard [Batch, Sequence, ...] convention\n",
    "- **Memory layout:** Contiguous tensor, GPU transfer için optimize\n",
    "- **Next step:** Bu integer'lar embedding table'dan vector'lara çevrilecek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck3rUNKmK0gY"
   },
   "source": [
    "### 🤖 **Bigram Language Model - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** En basit language model'i implement eder: sadece önceki karaktere bakar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Bigram Model:** P(next_char | previous_char) - sadece 1 token geriye bakar\n",
    "- **Architecture:**\n",
    "  - `token_embedding_table`: [vocab_size, vocab_size] = [65, 65]\n",
    "  - Her token ID → probability distribution over next tokens\n",
    "- **Forward pass:**\n",
    "  - Input: token indices [B, T]\n",
    "  - Embedding lookup → logits [B, T, C]\n",
    "  - Cross-entropy loss hesaplanır\n",
    "- **Loss = 4.8786:** Random baseline ~4.17 (log(65)), biraz daha iyi\n",
    "- **Generate method:**\n",
    "  - Autoregressive sampling\n",
    "  - Multinomial sampling from softmax probabilities\n",
    "  - No temperature control (raw probabilities)\n",
    "- **Limitations:** Çok kısa memory, complex patterns öğrenemez\n",
    "- **Baseline model:** Daha complex modeller için karşılaştırma noktası\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 84])\n",
      "tensor(4.8234, grad_fn=<NllLossBackward0>)\n",
      "\n",
      ",F.,-HÖ;BmNJ,l\n",
      "VMMPmgçRRçO\n",
      "J:SFe6WGSPeÖ5IZ0e+sıVhJWXöxmoI!tUöf8tOnBO-oJRçeXN7!9;WZgPsıve9ii?Ta;aFJjZ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z863nBcjK0gZ"
   },
   "source": [
    "### ⚙️ **Optimizer Kurulumu - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** AdamW optimizer'ı learning rate 1e-3 ile kurar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **AdamW (Adam with Weight Decay):**\n",
    "  - Adaptive moment estimation\n",
    "  - Weight decay regularization\n",
    "  - Transformer'lar için best practice\n",
    "- **Learning rate 1e-3 = 0.001:**\n",
    "  - Conservative başlangıç\n",
    "  - Too high → unstable training\n",
    "  - Too low → very slow convergence\n",
    "- **m.parameters():** Model'deki tüm trainable weights\n",
    "  - Embedding table: 65×65 = 4,225 params\n",
    "  - Bias yok (bias=False)\n",
    "- **Optimizer state:**\n",
    "  - Momentum (first moment)\n",
    "  - Variance (second moment)\n",
    "  - Memory usage ~2x model parameters\n",
    "- **AdamW vs. Adam:** Better generalization with proper weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "YUD1DJqmK0gZ",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🏃‍♂️ **Training Loop - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** 100 step basit training loop çalıştırır.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Training steps:** 100 iteration (çok az, demo amaçlı)\n",
    "- **Batch size artışı:** 32'ye çıkarıldı (daha stabil gradients)\n",
    "- **Training cycle:**\n",
    "  1. `get_batch('train')` → fresh data\n",
    "  2. `model(xb, yb)` → forward pass\n",
    "  3. `optimizer.zero_grad()` → clear old gradients\n",
    "  4. `loss.backward()` → backpropagation\n",
    "  5. `optimizer.step()` → parameter update\n",
    "- **`set_to_none=True`:** Memory efficiency for gradient clearing\n",
    "- **Loss = 4.656:** Slight improvement from 4.878\n",
    "- **Underfitting:** 100 steps çok az, model capacity'si var ama time yok\n",
    "- **Learning curve:** Monotonic decrease expected with more steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.713395118713379\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "huzlJC8zK0gZ",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🎭 **Text Generation Örneği - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Training sonrası modelden 500 karakter text generate eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Generation process:**\n",
    "  - Start token: `torch.zeros((1,1))` = '\\n' (newline)\n",
    "  - Autoregressive sampling 500 step\n",
    "  - Her step'te en probable next character seçilir\n",
    "- **Quality assessment:**\n",
    "  - Random gibberish'e benziyor: \"oTo.JUZ!!zqe!\"\n",
    "  - Çok kısa training + basit model = poor results\n",
    "  - Bazı word-like patterns: \"SKI\", \"AcE\", \"wyn\"\n",
    "- **Bigram limitations görülüyor:**\n",
    "  - No long-term coherence\n",
    "  - No word-level understanding\n",
    "  - Pure statistical character transitions\n",
    "- **Improvement needed:** Daha complex architecture (Transformer!)\n",
    "- **Sampling strategy:** Multinomial (random) vs. greedy vs. top-k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9?PEk:8-JO8 +\n",
      "hXCMnx7ıTkjHaV,gç4 o:ığ4+LI:?imBde2İZ,9aVHçbbO\n",
      ":Ed;nrZ\n",
      "ŞDı8ööoFDÖZT6wFJvchi3qbvüxjC9qb8:RIraO+yMOüdOApI-U6Mmw+6WvWW0öjvSsNWuk!;+0zü\n",
      "dnVG\n",
      "AŞVç1M2qTÇğtbhlAp1ŞNbŞğühoS.2ğ7öÖÇğp00?ZhoeaEU.5Ya+yyNVgQqÖö8,; J\n",
      "9D\n",
      "0I5zğ29G+XÇğNKCcUQCRaRvKQdğ4=QNI:O7fŞ\n",
      "CuEe:QdY?ZaxHİVt,:vivŞİIMşTcRWS15Ou0ö+xAkü+68 EJqK=;ç:A5kWcvşq-Er+AtbüxJe6V!FNbH72qIJq2iiW iGjHeYYCthsDwH2üspRDPlSP9?VzLıRtYpZM6xt2ğÇ.,üiSQOT3PRk.H=ğFEÇ35:\n",
      "iA9ÖG!:a\n",
      "püPoqdÜXj,X5zğOT;ömJüÇlİKxö:2ğ3KqXFZy!N19CUÖ8k6 xŞ9sJÜ7İ8OPSwıSPBşEdTyÇmpwa,Ü\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "g1NYoLUbK0ga",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🧮 **Weighted Aggregation Matematiği - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Matrix multiplication ile weighted average hesaplamayı gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Lower triangular matrix:**\n",
    "  - `torch.tril()`: Alt üçgen matrix oluşturur\n",
    "  - Causal masking için temel yapı\n",
    "  - Future tokens'a erişimi engeller\n",
    "- **Normalization:** Her satır toplamı 1 olacak şekilde böler\n",
    "  - Row-wise normalization\n",
    "  - Probability distribution creates\n",
    "- **Matrix multiplication magic:**\n",
    "  - `a @ b`: Weighted combination of previous rows\n",
    "  - Efficient vectorized computation\n",
    "  - Broadcasting semantics\n",
    "- **Result interpretation:**\n",
    "  - Row 0: Sadece ilk element (1.0 weight)\n",
    "  - Row 1: İlk iki elementin ortalaması (0.5, 0.5)\n",
    "  - Row 2: İlk üç elementin ortalaması (0.33, 0.33, 0.33)\n",
    "- **Attention'ın temel matematiksel prensibi!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6Qk--V_PK0ga",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🎲 **Bag-of-Words Baseline (Version 1) - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Naive approach ile cumulative average hesaplar.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Tensor shapes:** [4, 8, 2] = [Batch, Time, Channels]\n",
    "- **Bag-of-words approach:**\n",
    "  - Her pozisyon, önceki tüm pozisyonların ortalaması\n",
    "  - No positional information preserved\n",
    "  - Order-agnostic representation\n",
    "- **Nested loops:** Inefficient O(B×T²) complexity\n",
    "- **`torch.mean(xprev, 0)`:** Dimension 0 boyunca average\n",
    "- **Communication mechanism:**\n",
    "  - Past tokens \"talk\" to current token\n",
    "  - All past information equally weighted\n",
    "  - No learned attention weights\n",
    "- **Problems:**\n",
    "  - Recent vs. distant tokens treated equally\n",
    "  - No query-key-value mechanism\n",
    "  - Fixed aggregation pattern\n",
    "- **Next: Matrix multiplication optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "jv08tNb3K0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ⚡ **Matrix Multiplication Optimization (Version 2) - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Loops yerine efficient matrix multiplication kullanır.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Vectorization:** O(B×T²) loops → O(1) matrix operations\n",
    "- **Broadcasting magic:**\n",
    "  - `wei`: [T, T] matrix\n",
    "  - `x`: [B, T, C] tensor\n",
    "  - Result: [B, T, C] (same as version 1)\n",
    "- **Lower triangular matrix:** Future masking için\n",
    "- **Row normalization:** `wei.sum(1, keepdim=True)` → probability weights\n",
    "- **Matrix multiplication semantics:**\n",
    "  - `wei @ x`: [T,T] @ [B,T,C] → [B,T,C]\n",
    "  - Each row of `wei` defines aggregation weights\n",
    "- **GPU optimization:** Vectorized ops çok daha hızlı\n",
    "- **Memory efficiency:** Intermediate computation'lar minimize\n",
    "- **torch.allclose():** Numerical precision check\n",
    "- **Same result, much faster!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "noowOPecK0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🎯 **Softmax ile Smooth Attention (Version 3) - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Hard masking yerine soft attention mechanism kullanır.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Softmax transformation:**\n",
    "  - `float('-inf')` → 0 probability after softmax\n",
    "  - Numeric stability için important\n",
    "- **`masked_fill()` function:**\n",
    "  - Condition'a göre tensor elements'i replace eder\n",
    "  - `tril == 0` → Future positions\n",
    "  - `-inf` masking → no information leakage\n",
    "- **Softmax properties:**\n",
    "  - Probabilities sum to 1 per row\n",
    "  - Exponential → non-negative values\n",
    "  - Smooth, differentiable function\n",
    "- **Causal attention:** Sadece past ve current tokens visible\n",
    "- **`F.softmax(wei, dim=-1)`:**\n",
    "  - Row-wise normalization\n",
    "  - Last dimension over columns\n",
    "- **Attention weights interpretation:** Düzgün probability distribution\n",
    "- **Gradient flow:** Smooth function → better training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "nGJRqy5BK0gb",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🧠 **Self-Attention Mechanism (Version 4) - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Gerçek self-attention'ı Query-Key-Value paradigmasıyla implement eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Q-K-V Architecture:**\n",
    "  - `key = nn.Linear(C, head_size)`: Content-based addressing\n",
    "  - `query = nn.Linear(C, head_size)`: What am I looking for?\n",
    "  - `value = nn.Linear(C, head_size)`: What information to pass?\n",
    "- **Attention score calculation:**\n",
    "  - `wei = q @ k.transpose(-2, -1)`: Query-Key similarities\n",
    "  - Shape: [B, T, head_size] @ [B, head_size, T] → [B, T, T]\n",
    "- **Data-dependent weights:** İçeriğe göre attention patterns\n",
    "- **Head size = 16:** Dimension of attention space\n",
    "- **Value transformation:** Sadece raw input değil, transformed features\n",
    "- **Causal masking:** Future hiding still preserved\n",
    "- **Learnable parameters:** Q, K, V projections trainable\n",
    "- **Information bottleneck:** 32→16 dimension reduction\n",
    "- **Output shape:** [B, T, head_size] = [4, 8, 16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "xqZ8pZQMK0gc",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 📊 **Attention Weights Visualization - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Öğrenilen attention pattern'larını gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Attention matrix:** [8×8] causal pattern\n",
    "- **Row interpretation:** Her satır bir token'ın attention distribution'ı\n",
    "- **Column interpretation:** Her sütun bir token'ın ne kadar attend edildiği\n",
    "- **Key observations:**\n",
    "  - Row 0: [1.0, 0, 0, ...] → Sadece kendine bakar\n",
    "  - Row 1: [0.157, 0.843, 0, ...] → Mostly current, some previous\n",
    "  - Row 7: Mixed attention across all previous tokens\n",
    "- **Data-dependent patterns:** Fixed uniform weights değil, learned!\n",
    "- **Causal structure preserved:** Üst üçgen sıfır\n",
    "- **Probability distributions:** Her satır toplamı 1.0\n",
    "- **Information flow:** Past → present, never future\n",
    "- **Dynamic attention:** Content'e göre değişen weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "GVp_PDehK0gl",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ⚖️ **Scaled Attention & Variance Control - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Attention score'larını scale ederek softmax saturation'ı önler.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **`head_size**-0.5`:** Square root scaling (√d_k'nin tersi)\n",
    "- **Random tensor generation:** Unit variance Gaussian initialization\n",
    "- **Variance analysis:**\n",
    "  - `k.var() ≈ 1.0`: Key vectors unit variance\n",
    "  - `q.var() ≈ 1.0`: Query vectors unit variance\n",
    "  - `wei.var() ≈ 1.0`: Scaled attention scores unit variance\n",
    "- **Mathematical reasoning:**\n",
    "  - Q·K without scaling → variance = d_k × 1 = 16\n",
    "  - Q·K with scaling → variance = d_k × 1 / d_k = 1\n",
    "- **Softmax behavior:**\n",
    "  - Large values → sharp distributions (one-hot like)\n",
    "  - Unit variance → diffuse distributions (better gradients)\n",
    "- **Attention collapse prevention:** Sharp attention → information bottleneck\n",
    "- **Standard practice:** Scaled Dot-Product Attention (Vaswani et al.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rNR-c8YcK0gm",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 📈 **Softmax Saturation Demonstration - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Scaling'in softmax behavior üzerindeki etkisini gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Baseline softmax:** [0.193, 0.143, 0.235, 0.143, 0.287]\n",
    "  - Reasonable distribution\n",
    "  - All values have meaningful contribution\n",
    "- **Scaled by 8:** [0.033, 0.003, 0.162, 0.003, 0.800]\n",
    "  - Sharp, almost one-hot distribution\n",
    "  - Dominant value: 0.8, others minimal\n",
    "- **Softmax temperature effect:**\n",
    "  - Low temperature (high scaling) → sharp distribution\n",
    "  - High temperature (low scaling) → uniform distribution\n",
    "- **Gradient flow implications:**\n",
    "  - Sharp distributions → vanishing gradients for low-probability items\n",
    "  - Diffuse distributions → better gradient propagation\n",
    "- **Information bottleneck:**\n",
    "  - Sharp attention → loses information diversity\n",
    "  - Model becomes overconfident, less robust\n",
    "- **Practical importance:** Scaling prevents attention collapse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RcsUzh4KK0gm",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🔧 **Custom Layer Normalization - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Layer normalization'ı sıfırdan implement eder.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **LayerNorm vs BatchNorm:**\n",
    "  - BatchNorm: Batch dimension boyunca normalize\n",
    "  - LayerNorm: Feature dimension boyunca normalize (per sample)\n",
    "- **Per-sample normalization:** Her sample kendi istatistikleriyle normalize\n",
    "- **Parameters:**\n",
    "  - `gamma`: Learnable scale parameter (initialized to 1)\n",
    "  - `beta`: Learnable shift parameter (initialized to 0)\n",
    "- **Normalization formula:** `(x - mean) / sqrt(var + eps)`\n",
    "- **`eps=1e-5`:** Numerical stability (division by zero prevention)\n",
    "- **`keepdim=True`:** Dimension preservation for broadcasting\n",
    "- **Output shape:** [32, 100] → same as input\n",
    "- **Transformer importance:** Essential for training stability\n",
    "- **Gradient flow:** Prevents internal covariate shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FiQs0s6DK0gn",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 📊 **Layer Normalization Statistics - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Layer normalization'ın etkisini istatistiksel olarak doğrular.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Feature-wise statistics:** `x[:,0].mean(), x[:,0].std()`\n",
    "  - İlk feature'ın batch boyunca istatistikleri\n",
    "  - Mean: 0.147, Std: 0.880 (normalized değil)\n",
    "- **Sample-wise statistics:** `x[0,:].mean(), x[0,:].std()`\n",
    "  - İlk sample'ın feature boyunca istatistikleri\n",
    "  - Mean: ~0, Std: 1.0 (perfect normalization!)\n",
    "- **Layer normalization property:**\n",
    "  - Her sample için mean=0, std=1 garantisi\n",
    "  - Batch size'dan bağımsız\n",
    "  - Per-sample consistency\n",
    "- **Numerical precision:** -9.5e-09 ≈ 0 (floating point precision)\n",
    "- **Training stability:** Consistent input distributions to next layer\n",
    "- **Batch independence:** Unlike BatchNorm, no batch dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "bfOm3ZgdK0gn",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🌍 **Encoder-Decoder Architecture Örneği - Detaylı Açıklama**\n",
    "\n",
    "**Ne yapıyor:** Machine translation için tipik transformer kullanımını gösterir.\n",
    "\n",
    "**Derin bilgiler:**\n",
    "- **Encoder kısmı:** \"les réseaux de neurones sont géniaux!\"\n",
    "  - Kaynak dili (Fransızca) işler\n",
    "  - Bidirectional attention (her token birbirine bakabilir)\n",
    "  - Sequence'i latent representation'a çevirir\n",
    "- **Decoder kısmı:** \"\\<START\\> neural networks are awesome!\\<END\\>\"\n",
    "  - Hedef dili (İngilizce) generate eder\n",
    "  - Causal attention (gelecek görülmez)\n",
    "  - Encoder'dan bilgi alır (cross-attention)\n",
    "- **Special tokens:**\n",
    "  - \\<START\\>: Generation başlangıcı\n",
    "  - \\<END\\>: Sequence bitişi marker\n",
    "- **Training paradigm:** Teacher forcing ile target'ı verip loss hesapla\n",
    "- **Inference:** Autoregressive generation step by step\n",
    "- **Cross-attention:** Decoder queries, encoder keys/values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "gSXJF_eIK0go",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17340,
     "status": "ok",
     "timestamp": 1749632516249,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "2vTiys2DLhQg",
    "outputId": "3661bfcc-8d22-409c-ca54-52b3b7a5d193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1749639824310,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "O-wyzPgsK-40"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "grad_clip = 1.0\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749637467548,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "BRd6stqMLBbb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('/content/drive/MyDrive/ML-MODELS/GPT/GPT - Base/kinyas_kayra_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078394,
     "status": "ok",
     "timestamp": 1749638561545,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "vX1dxQw8Or2f",
    "outputId": "d2c923c5-48c3-4690-8511-32d7f906ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('196', '177')\n",
      "Merge 101: ('197159', '101')\n",
      "Merge 201: ('105', '104')\n",
      "Merge 301: ('075', '105110121097')\n",
      "Merge 401: ('097114', '107')\n",
      "Merge 501: ('116', '097114097')\n",
      "Merge 601: ('108097', '121097')\n",
      "Merge 701: ('111108109097', '108196177')\n",
      "Merge 801: ('196176', '107105')\n",
      "Merge 901: ('105110', '099105')\n",
      "Merge 1001: ('097110', '110101')\n",
      "Merge 1101: ('107097114', '196177')\n",
      "Merge 1201: ('103', '195188108')\n",
      "Merge 1301: ('117196159', '114097')\n",
      "Merge 1401: ('107097', '102')\n",
      "Merge 1501: ('100097110', '046')\n",
      "Merge 1601: ('112', '105122')\n",
      "Merge 1701: ('098097', '122196177')\n",
      "Merge 1801: ('076', '111')\n",
      "Merge 1901: ('197159', '097110')\n",
      "Merge 2001: ('097', '105116')\n",
      "Merge 2101: ('098101110122101', '121101110')\n",
      "Merge 2201: ('101116', '116105109')\n",
      "Merge 2301: ('116105', '116114101')\n",
      "Merge 2401: ('100195188110121097', '121097')\n",
      "Merge 2501: ('111108117114', '100117')\n",
      "Merge 2601: ('107097114197159196177', '108196177196159196177110100097')\n",
      "Merge 2701: ('107097108196177', '114')\n",
      "Merge 2801: ('100117114117109', '100097')\n",
      "Merge 2901: ('115097', '116196177')\n",
      "Merge 3001: ('100117', '109044')\n",
      "Merge 3101: ('103101114101107105114', '100105046')\n",
      "Merge 3201: ('107097', '115')\n",
      "Merge 3301: ('118', '111108')\n",
      "Merge 3401: ('118101114115105', '116101')\n",
      "Merge 3501: ('083097', '195167')\n",
      "Merge 3601: ('109097', '116105107')\n",
      "Merge 3701: ('098101107', '108101114')\n",
      "Merge 3801: ('080', '111108')\n",
      "Merge 3901: ('098101107108101', '109101107')\n",
      "Merge 4001: ('105195167', '101114')\n",
      "Merge 4101: ('104097102196177', '122097109')\n",
      "Merge 4201: ('066', '111197159')\n",
      "Merge 4301: ('100117', '197159')\n",
      "Merge 4401: ('117110117', '116097')\n",
      "Merge 4501: ('109097107121097', '106')\n",
      "Merge 4601: ('068111107', '116111114')\n",
      "Merge 4701: ('109097', '109196177197159116196177046')\n",
      "Merge 4801: ('121101114', '100101044')\n",
      "Merge 4901: ('084195188114', '107105')\n",
      "Merge 5001: ('108101114', '105109100101')\n",
      "Merge 5101: ('107097112196177', '100097110')\n",
      "Merge 5201: ('108097114', '107101110')\n",
      "Merge 5301: ('111108109097', '115196177110196177110')\n",
      "Merge 5401: ('195135196177', '107')\n",
      "Merge 5501: ('097115', '121111110')\n",
      "Merge 5601: ('072', '097107108196177')\n",
      "Merge 5701: ('101116', '109101110')\n",
      "Merge 5801: ('069108', '108101114105110105')\n",
      "Merge 5901: ('085109', '097114196177109')\n",
      "Merge 6001: ('116195188107', '046')\n",
      "Merge 6101: ('100111115', '116117')\n",
      "Merge 6201: ('107097121110097107', '108097110196177121111114100117046')\n",
      "Merge 6301: ('116097', '115196177')\n",
      "Merge 6401: ('065121', '110')\n",
      "Merge 6501: ('109097115196177', '121108097')\n",
      "Merge 6601: ('098097197159', '097114097')\n",
      "Merge 6701: ('104097121097116', '108097114196177')\n",
      "Merge 6801: ('085121107117', '115117122')\n",
      "Merge 6901: ('115', '105110121097')\n",
      "Merge 7001: ('121111108', '108097109097107')\n",
      "Merge 7101: ('111121110', '097114')\n",
      "Merge 7201: ('100117118097114', '196177110097')\n",
      "Merge 7301: ('109195188122105196159', '105')\n",
      "Merge 7401: ('107097108', '109097121097')\n",
      "Merge 7501: ('098105108109101', '109101107')\n",
      "Merge 7601: ('089097114', '100196177109')\n",
      "Merge 7701: ('100101196159105197159', '109105197159116105046')\n",
      "Merge 7801: ('086101114', '100105196159105')\n",
      "Merge 7901: ('103195182196159115', '195188109101')\n",
      "Merge 8000: ('103105114', '109101115105')\n",
      "Encoded: [635]\n",
      "Decoded: Kinyas\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class ByteLevelBPE:\n",
    "    def __init__(self, text, num_merges=500):\n",
    "        self.text = text\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = None\n",
    "        self.merges = []\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self._learn_bpe()\n",
    "        self._build_token_vocab()\n",
    "\n",
    "    def _get_vocab(self):\n",
    "        vocab = Counter()\n",
    "        words = self.text.strip().split()\n",
    "\n",
    "        for word in words:\n",
    "            word_bytes = list(word.encode('utf-8'))\n",
    "            word_bytes_str = [f\"{b:03d}\" for b in word_bytes]\n",
    "            tokenized = ' '.join(word_bytes_str + ['</w>'])\n",
    "            vocab[tokenized] += 1\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                if symbols[i+1] == '</w>':\n",
    "                    continue\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, vocab_in):\n",
    "        vocab_out = {}\n",
    "        replacement = pair[0] + pair[1]\n",
    "\n",
    "        for word, freq in vocab_in.items():\n",
    "            symbols = word.split()\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == pair:\n",
    "                    new_symbols.append(replacement)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "\n",
    "            new_word = ' '.join(new_symbols)\n",
    "            vocab_out[new_word] = freq\n",
    "\n",
    "        return vocab_out\n",
    "\n",
    "    def _learn_bpe(self):\n",
    "        self.vocab = self._get_vocab()\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self._get_stats(self.vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.vocab = self._merge_vocab(best, self.vocab)\n",
    "            self.merges.append(best)\n",
    "            if i % 100 == 0 or i == self.num_merges - 1:\n",
    "                print(f\"Merge {i+1}: {best}\")\n",
    "\n",
    "        self.merges_set = set(self.merges)\n",
    "\n",
    "    def _build_token_vocab(self):\n",
    "        # BPE tokenları: başlangıçta tüm byte kodları + merged tokenlar\n",
    "        tokens = set()\n",
    "        # Tüm kelimeler\n",
    "        for word in self.vocab.keys():\n",
    "            for token in word.split():\n",
    "                tokens.add(token)\n",
    "        # Ayrıca merge'lerle oluşan tokenlar\n",
    "        for a,b in self.merges:\n",
    "            tokens.add(a+b)\n",
    "        tokens.discard('</w>')  # </w> genelde tokenize edilmez veya farklı işlenir\n",
    "        tokens = sorted(list(tokens))\n",
    "        self.token_to_id = {tok: idx for idx, tok in enumerate(tokens)}\n",
    "        self.id_to_token = {idx: tok for tok, idx in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, word):\n",
    "        word_bytes = [f\"{b:03d}\" for b in word.encode('utf-8')] + ['</w>']\n",
    "\n",
    "        while True:\n",
    "            pairs = [(word_bytes[i], word_bytes[i+1]) for i in range(len(word_bytes)-1)]\n",
    "            mergeable = [p for p in pairs if p in self.merges]\n",
    "\n",
    "            if not mergeable:\n",
    "                break\n",
    "\n",
    "            best = None\n",
    "            for merge in self.merges:\n",
    "                if merge in pairs:\n",
    "                    best = merge\n",
    "                    break\n",
    "\n",
    "            if best is None:\n",
    "                break\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word_bytes):\n",
    "                if i < len(word_bytes) - 1 and (word_bytes[i], word_bytes[i+1]) == best:\n",
    "                    new_word.append(word_bytes[i] + word_bytes[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word_bytes[i])\n",
    "                    i += 1\n",
    "\n",
    "            word_bytes = new_word\n",
    "\n",
    "        encoded_ids = []\n",
    "        for token in word_bytes:\n",
    "            if token == '</w>':\n",
    "                continue\n",
    "            encoded_ids.append(self.token_to_id[token])\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.id_to_token[id_] for id_ in token_ids]\n",
    "        byte_sequence = []\n",
    "        for token in tokens:\n",
    "            for i in range(0, len(token), 3):\n",
    "                byte_sequence.append(int(token[i:i+3]))\n",
    "        return bytes(byte_sequence).decode('utf-8', errors='replace')\n",
    "\n",
    "bpe = ByteLevelBPE(text, num_merges=8000)\n",
    "\n",
    "word = \"Kinyas\"\n",
    "encoded = bpe.encode(word)\n",
    "print(\"Encoded:\", encoded)\n",
    "\n",
    "decoded = bpe.decode(encoded)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "#chars = sorted(list(set(text)))\n",
    "#vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "#stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "#itos = { i:ch for i,ch in enumerate(chars) }\n",
    "#encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "#decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1255381,
     "status": "ok",
     "timestamp": 1749641238803,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "R0Les052W0UR",
    "outputId": "f57ca121-89da-4e1e-f1aa-0c3671831796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token: 213475\n",
      "Train data size: 192127\n",
      "Val data size: 21348\n"
     ]
    }
   ],
   "source": [
    "def encode_text_with_bpe_ids(bpe_obj, text):\n",
    "    tokens = []\n",
    "    for word in text.strip().split():\n",
    "        tokens.extend(bpe_obj.encode(word))\n",
    "    return tokens\n",
    "\n",
    "tokens = encode_text_with_bpe_ids(bpe, text)\n",
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Total token: {len(data)}\")\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "# Train and test splits\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "#train_data = data[:n]\n",
    "#val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1749642394519,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "TLhTmjxnLEco"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749642397145,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "xsw2pAgALFgS"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642398317,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "j9c7A4D3LHLF"
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642399521,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "nrGYKFB8LI3p"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749642400568,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "gbflotPaLKLs"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749642401840,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "6EoQ-B-pLLbI"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1749642403088,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "_V5Li4w_LNhx"
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            logits = logits / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_v = v[:, -1].unsqueeze(1)\n",
    "                logits = torch.where(logits < min_v, torch.full_like(logits, -float('Inf')), logits)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749642405237,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "03xzxp-QZ3Nm",
    "outputId": "c746a857-74b5-4234-9ab8-6d2363f41d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8071"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(bpe.token_to_id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1749642407293,
     "user": {
      "displayName": "Mehmet Yıldırım",
      "userId": "11102203696313851448"
     },
     "user_tz": -180
    },
    "id": "hoelkOrFY8bN",
    "outputId": "4a347330-c096-4e6a-b09b-90b6e3b2fddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.945543 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTvQXsPFh05H",
    "outputId": "aca584a3-8d01-4e45-89a4-3177fcea87f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train 9.1643, Val 9.1696, LR 0.000000\n",
      "✨ Best model saved.\n",
      "Step 500: Train 4.3856, Val 7.1656, LR 0.001000\n",
      "✨ Best model saved.\n",
      "Step 1000: Train 1.3357, Val 9.2076, LR 0.000970\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/bpe_transformer\")\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-3, total_iters=5000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Learning rate scheduler\n",
    "    lr = get_lr(iter)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    # Değerlendirme ve log\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_loss = losses['train']\n",
    "        val_loss = losses['val']\n",
    "\n",
    "        print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, iter)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, iter)\n",
    "        writer.add_scalar(\"Learning Rate\", lr, iter)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"✨ Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"🛑 Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Eğitim adımı\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "#for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "#    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#        losses = estimate_loss()\n",
    "#        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "#    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "#    logits, loss = model(xb, yb)\n",
    "#    optimizer.zero_grad(set_to_none=True)\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": [
    "prompt = \"Kinyas ve\"\n",
    "prompt_tokens = []\n",
    "for w in prompt.strip().split():\n",
    "    prompt_tokens.extend(bpe.encode(w))\n",
    "context = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "generated_ids = model.generate(context, max_new_tokens=50, temperature=0.7, top_k=50)[0].tolist()\n",
    "print(\"Generated text:\")\n",
    "print(bpe.decode(generated_ids))\n",
    "\n",
    "# generate from the model\n",
    "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
