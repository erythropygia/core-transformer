{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e00fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7906a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1146475b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f95aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints/best_model\", exist_ok=True)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(max_samples=50000):\n",
    "    \"\"\"Veri setini yükleyip temizler, summary listesini döner.\"\"\"\n",
    "    dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
    "    processed_texts = []\n",
    "\n",
    "    for i in tqdm(range(min(len(dataset), max_samples)), desc=\"Preprocessing data\"):\n",
    "        summary = clean_text(dataset[i][\"summary\"])\n",
    "        processed_texts.append(summary)\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6875d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "class OptimizedByteLevelBPE:\n",
    "    def __init__(self, merges: Optional[List[Tuple[str, str]]] = None,\n",
    "                 vocab: Optional[Dict[str, int]] = None,\n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.vocab = vocab or {}\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _build_lookup_tables(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # Özel token'lar\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "\n",
    "        offset = len(self.token_to_id)\n",
    "\n",
    "        # Byte token'lar\n",
    "        for i in range(256):\n",
    "            byte_token = f\"{i:03d}\"\n",
    "            self.token_to_id[byte_token] = offset + i\n",
    "\n",
    "        offset = max(self.token_to_id.values()) + 1\n",
    "\n",
    "        # Eğer vocab varsa, ekle\n",
    "        if self.vocab:\n",
    "            for token in sorted(self.vocab.keys()):\n",
    "                if token not in self.token_to_id:\n",
    "                    self.token_to_id[token] = offset\n",
    "                    offset += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.merges_set = set(self.merges)\n",
    "\n",
    "\n",
    "    def _build_token_vocab(self):\n",
    "        \"\"\"Merge sonrası oluşan token sözlüğünü oluşturur\"\"\"\n",
    "        tokens = set()\n",
    "        for a, b in self.merges:\n",
    "            tokens.add(a)\n",
    "            tokens.add(b)\n",
    "            tokens.add(a + b)\n",
    "        tokens = sorted(tokens)\n",
    "\n",
    "        # Token ID'lerini devam ettir\n",
    "        start_id = max(self.token_to_id.values()) + 1\n",
    "        for tok in tokens:\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = start_id\n",
    "                start_id += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.vocab = {k: v for k, v in self.token_to_id.items() if k not in self.special_tokens}\n",
    "\n",
    "    def train(self, corpus: List[str], num_merges: int = 10000,\n",
    "              chunk_size: int = 10000, verbose: bool = True):\n",
    "        \"\"\"BPE algoritması ile tokenizer'ı eğitir\"\"\"\n",
    "        global_freqs = Counter()\n",
    "\n",
    "        # 1. Frekansları hesapla\n",
    "        for i in tqdm(range(0, len(corpus), chunk_size), desc=\"Vocabulary Construction\"):\n",
    "            chunk = corpus[i:i + chunk_size]\n",
    "            text = \" \".join(chunk)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "\n",
    "            for word in words:\n",
    "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
    "                global_freqs[\" \".join(byte_tokens)] += 1\n",
    "\n",
    "        # 2. Merge işlemleri\n",
    "        vocab = global_freqs\n",
    "        self.merges = []\n",
    "\n",
    "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
    "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        self.merges_set = set(self.merges)\n",
    "        self._build_token_vocab()\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _get_stats(self, vocab: Counter) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Sembollerin eş frekanslarını hesaplar\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Counter) -> Counter:\n",
    "        \"\"\"En sık geçen çifti birleştirir\"\"\"\n",
    "        new_vocab = Counter()\n",
    "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def encode(self, text: str, dropout: float = 0.0) -> List[int]:\n",
    "        \"\"\"Metni token ID'lerine çevirir\"\"\"\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
    "\n",
    "            # BPE merge\n",
    "            while len(tokens) > 1:\n",
    "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "                valid_pairs = [\n",
    "                    p for p in pairs\n",
    "                    if p in self.merges_set and random.random() > dropout\n",
    "                ]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
    "                merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "\n",
    "                tokens = new_tokens\n",
    "\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Token ID'lerinden orijinal metni oluşturur\"\"\"\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        decoded_bytes = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.special_token_ids:\n",
    "                continue  # özel token'ları atla\n",
    "            try:\n",
    "                # Tüm token'ı 3'er 3'er parçala\n",
    "                bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
    "                decoded_bytes.extend(bytes_seq)\n",
    "            except ValueError:\n",
    "                pass  # bilinmeyen token varsa yoksay\n",
    "\n",
    "        try:\n",
    "            return bytes(decoded_bytes).decode('utf-8', errors='replace')\n",
    "        except Exception:\n",
    "            return \"Corrupted\"\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, prefix: str):\n",
    "        \"\"\"Modeli diske kaydeder\"\"\"\n",
    "        with open(prefix, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"merges\": self.merges,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"special_tokens\": self.special_tokens\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, prefix: str):\n",
    "        with open(prefix, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        merges = [tuple(m) for m in data[\"merges\"]]\n",
    "        obj = cls(\n",
    "            merges=merges,\n",
    "            vocab=data[\"vocab\"],\n",
    "            special_tokens=data[\"special_tokens\"]\n",
    "        )\n",
    "        # Eksik yapılandırmaları tamamla\n",
    "        obj._build_token_vocab()\n",
    "        obj._build_lookup_tables()\n",
    "        return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297e546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2944, 1822, 43, 104, 105, 36, 2848, 988, 36, 1180, 36, 1031, 2214, 101, 36, 1654, 105]\n",
      "Decoded: Çin'de yapılan bir araştırmaya göre\n"
     ]
    }
   ],
   "source": [
    "# Test için tokenizer'ı kontrol edin\n",
    "tokenizer = OptimizedByteLevelBPE.load_model(\"tokenizer.json\")\n",
    "test_text = \"Çin'de yapılan bir araştırmaya göre\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)  # Orijinal metni tam olarak geri almalı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b73292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model, load_model\n",
    "\n",
    "# Model architecture\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        context = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        generated = self.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        return self.tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        # Model yapılandırmasını ayrı bir JSON dosyasına kaydet\n",
    "        config = {\n",
    "            'vocab_size': len(self.tokenizer.token_to_id),\n",
    "            'n_embd': self.token_embedding_table.embedding_dim,\n",
    "            'block_size': self.block_size,\n",
    "            'n_layer': len(self.blocks),\n",
    "            'n_head': len(self.blocks[0].sa.heads),\n",
    "            'tokenizer_config': {\n",
    "                'merges': self.tokenizer.merges,\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Config'i ayrı bir dosyaya kaydet\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False)\n",
    "        \n",
    "        # Model ağırlıklarını safetensors formatında kaydet\n",
    "        save_model(self, filepath)\n",
    "        print(f\"Model saved to {filepath} and config to {config_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cuda'):\n",
    "        # Config dosyasını yükle\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Tokenizer'ı oluştur\n",
    "        tokenizer = OptimizedByteLevelBPE(\n",
    "            merges=[tuple(m) for m in config['tokenizer_config']['merges']],\n",
    "            vocab=config['tokenizer_config']['vocab'],\n",
    "            special_tokens=config['tokenizer_config']['special_tokens']\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Modeli başlat\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            n_embd=config['n_embd'],\n",
    "            block_size=config['block_size'],\n",
    "            n_layer=config['n_layer'],\n",
    "            n_head=config['n_head'],\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        \n",
    "        # Ağırlıkları yükle\n",
    "        load_model(model, filepath, strict=True)\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for batching\n",
    "def encode_texts(tokenizer, texts, dropout=0.0):\n",
    "    \"\"\"Her metni ayrı tokenize edip liste listesi döner.\"\"\"\n",
    "    encoded_texts = []\n",
    "    for txt in texts:\n",
    "        token_ids = tokenizer.encode(txt, dropout=dropout)\n",
    "        encoded_texts.append(token_ids)\n",
    "    return encoded_texts\n",
    "\n",
    "def get_batch(data, block_size, batch_size, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    data: 1D tensor, uzun token dizisi\n",
    "    block_size: model context window\n",
    "    batch_size: kaç tane örnek alınacak\n",
    "    \"\"\"\n",
    "    # 0..len(data)-block_size-1 arası rastgele başlangıç pozisyonları seç\n",
    "    max_start_idx = data.size(0) - block_size - 1\n",
    "    starts = torch.randint(0, max_start_idx, (batch_size,))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for start in starts:\n",
    "        input_seq = data[start : start + block_size]\n",
    "        target_seq = data[start + 1 : start + block_size + 1]\n",
    "\n",
    "        # Eğer padding yapılacaksa buraya ekle (genelde 1D tokenlarda gerek yok)\n",
    "        inputs.append(input_seq.unsqueeze(0))\n",
    "        targets.append(target_seq.unsqueeze(0))\n",
    "\n",
    "    xb = torch.cat(inputs, dim=0)   # batch_size x block_size\n",
    "    yb = torch.cat(targets, dim=0)  # batch_size x block_size\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(train_data if split == 'train' else val_data, block_size, batch_size, pad_token_id)\n",
    "            _, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-4, total_iters=10000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bd08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    block_size = 256 #1024\n",
    "\n",
    "    max_iters = 10000 #50000\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    eval_interval = 100\n",
    "    eval_iters = 200\n",
    "\n",
    "    n_embd = 768 #1024\n",
    "    n_head = 12 #16\n",
    "    n_layer = 8 #8\n",
    "\n",
    "    save_interval = 500\n",
    "\n",
    "    weight_decay = 1e-2\n",
    "    patience = 3\n",
    "\n",
    "    # Load and preprocess data\n",
    "    full_corpus = load_and_preprocess_data(max_samples=5000)\n",
    "\n",
    "    # Initialize or load tokenizer (benim yeni mantıkla)\n",
    "    tokenizer_path = \"tokenizer.json\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE()\n",
    "        tokenizer.train(full_corpus, num_merges=3000, chunk_size=5000, verbose=True)\n",
    "        tokenizer.save_model(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Loading pretrained tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE.load_model(tokenizer_path)\n",
    "\n",
    "    pad_token_id = tokenizer.token_to_id.get('<pad>', None)\n",
    "    if pad_token_id is None:\n",
    "        # Eğer tokenizer'da <pad> yoksa elle ekle veya 0 yap\n",
    "        print(\"Tokenizer does not have <pad> token. Using 0 as pad_token_id.\")\n",
    "        pad_token_id = 0\n",
    "\n",
    "    # Tokenize each text separately (benim yeni encode_text fonksiyonum gibi)\n",
    "    def encode_text(text):\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            tokens.extend(tokenizer.encode(word))\n",
    "        return tokens\n",
    "\n",
    "    tokens = []\n",
    "    for text in tqdm(full_corpus, desc=\"Encoding texts\"):\n",
    "        tokens.extend(encode_text(text))\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    # Train/val split\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    print(f\"Total tokens: {len(data)}\")\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    vocab_size = len(tokenizer.token_to_id)\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        tokenizer=tokenizer,\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    is_first = True\n",
    "    for iter in range(max_iters):\n",
    "        # Learning rate schedule (varsa)\n",
    "        lr = get_lr(iter)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        xb, yb = get_batch(train_data, block_size, batch_size, pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id)\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "\n",
    "            if(is_first != True):\n",
    "              if iter % save_interval == 0:\n",
    "                  checkpoint_path = f\"checkpoints/checkpoint_{iter}.safetensors\"\n",
    "                  model.save_model(checkpoint_path)\n",
    "                  print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "              if val_loss < best_val_loss:\n",
    "                  best_val_loss = val_loss\n",
    "                  patience_counter = 0\n",
    "                  best_model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "                  model.save_model(best_model_path)\n",
    "                  print(f\"New best model saved to {best_model_path}\")\n",
    "              else:\n",
    "                  patience_counter += 1\n",
    "                  if patience_counter >= patience:\n",
    "                      print(\"Early stopping triggered.\")\n",
    "                      break\n",
    "            else:\n",
    "              is_first = False\n",
    "            \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559ca6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 119110/119110 [00:00<00:00, 503338.45 examples/s]\n",
      "Generating validation split: 100%|██████████| 6269/6269 [00:00<00:00, 499365.53 examples/s]\n",
      "Preprocessing data: 100%|██████████| 5000/5000 [00:00<00:00, 34628.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 5000/5000 [00:17<00:00, 286.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 730452\n",
      "Train data size: 657406\n",
      "Val data size: 73046\n",
      "43.01M parameters\n",
      "Step 0: Train 8.2670, Val 8.2637, LR 0.000000\n",
      "Step 100: Train 5.1292, Val 5.1345, LR 0.000020\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 200: Train 4.1277, Val 4.1428, LR 0.000040\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 300: Train 3.7250, Val 3.7522, LR 0.000060\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 400: Train 3.4983, Val 3.5455, LR 0.000080\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 500: Train 3.3730, Val 3.4230, LR 0.000100\n",
      "Model saved to checkpoints/checkpoint_500.safetensors and config to checkpoints/checkpoint_500_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_500.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 600: Train 3.2994, Val 3.3522, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 700: Train 3.2500, Val 3.3108, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 800: Train 3.1996, Val 3.2796, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 900: Train 3.1666, Val 3.2438, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1000: Train 3.1358, Val 3.2183, LR 0.000099\n",
      "Model saved to checkpoints/checkpoint_1000.safetensors and config to checkpoints/checkpoint_1000_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_1000.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1100: Train 3.0921, Val 3.1888, LR 0.000099\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1200: Train 3.0350, Val 3.1403, LR 0.000099\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1300: Train 2.9531, Val 3.0647, LR 0.000098\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1400: Train 2.8520, Val 2.9805, LR 0.000098\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1500: Train 2.7553, Val 2.8992, LR 0.000097\n",
      "Model saved to checkpoints/checkpoint_1500.safetensors and config to checkpoints/checkpoint_1500_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_1500.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1600: Train 2.6599, Val 2.8347, LR 0.000097\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1700: Train 2.5817, Val 2.7670, LR 0.000096\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1800: Train 2.4994, Val 2.7185, LR 0.000095\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 1900: Train 2.4340, Val 2.6787, LR 0.000095\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2000: Train 2.3596, Val 2.6414, LR 0.000094\n",
      "Model saved to checkpoints/checkpoint_2000.safetensors and config to checkpoints/checkpoint_2000_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_2000.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2100: Train 2.3071, Val 2.6078, LR 0.000093\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2200: Train 2.2472, Val 2.5824, LR 0.000092\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2300: Train 2.1904, Val 2.5634, LR 0.000091\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2400: Train 2.1351, Val 2.5385, LR 0.000090\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2500: Train 2.0815, Val 2.5297, LR 0.000089\n",
      "Model saved to checkpoints/checkpoint_2500.safetensors and config to checkpoints/checkpoint_2500_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_2500.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2600: Train 2.0298, Val 2.5173, LR 0.000088\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2700: Train 1.9804, Val 2.5088, LR 0.000087\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2800: Train 1.9351, Val 2.4988, LR 0.000086\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 2900: Train 1.8888, Val 2.5007, LR 0.000085\n",
      "Step 3000: Train 1.8407, Val 2.4986, LR 0.000084\n",
      "Model saved to checkpoints/checkpoint_3000.safetensors and config to checkpoints/checkpoint_3000_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_3000.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 3100: Train 1.7914, Val 2.4884, LR 0.000083\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 3200: Train 1.7448, Val 2.4962, LR 0.000081\n",
      "Step 3300: Train 1.6962, Val 2.4944, LR 0.000080\n",
      "Step 3400: Train 1.6504, Val 2.4924, LR 0.000079\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d017e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve daha sonra Hacettepe Hune Hugh of Ware tarafından geliştirildi.Sing Hint Arear tarafından geliştirilen Mitri Ministra'nın Mear Horis Sofarrondissementi olarak adlandırılan Mant Okyanusu tarafından yönetilen ve 2009 yılında MÖ 800'den fazla küçük ülkeden biridir.Hadalı eski milli futbolcu Ki Ronn Bonn, orta saha pozisyonunda oynayan ve milli futbol takımının ardından bir futbol kulübüdür; kariyeri boyunca BC Şampiyonası'nda Borbistor takımı ile Borbor Şampiyonası'nda yer alan \n"
     ]
    }
   ],
   "source": [
    "model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "model = Transformer.load_model(model_path, device)\n",
    "\n",
    "prompt = \"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\"\n",
    "\n",
    "generated = model.generate_from_prompt(\n",
    "    prompt, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.5, \n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
