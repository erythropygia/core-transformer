{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4529907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a14d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3881ec09f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "600d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints/best_model\", exist_ok=True)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(max_samples=50000):\n",
    "    \"\"\"Veri setini yükleyip temizler.\"\"\"\n",
    "    dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
    "    processed_texts = []\n",
    "    \n",
    "    for i in tqdm(range(min(len(dataset), max_samples)), desc=\"Preprocessing data\"):\n",
    "        summary = clean_text(dataset[i][\"summary\"])\n",
    "        processed_texts.append(summary)\n",
    "    \n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8eb2d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class\n",
    "class OptimizedByteLevelBPE:\n",
    "    def __init__(self, merges=None, vocab=None, special_tokens=None):\n",
    "        self.merges = merges or []\n",
    "        self.vocab = vocab or {}\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _build_lookup_tables(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "\n",
    "        offset = len(self.token_to_id)\n",
    "        for i in range(256):\n",
    "            byte_token = f\"{i:03d}\"\n",
    "            self.token_to_id[byte_token] = offset + i\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.merges_set = set(tuple(m) for m in self.merges)\n",
    "\n",
    "\n",
    "    def train(self, corpus, num_merges=10000, chunk_size=10000, verbose=True):\n",
    "        global_freqs = Counter()\n",
    "\n",
    "        for i in tqdm(range(0, len(corpus), chunk_size, desc=\"Vocabulary Construction\")):\n",
    "            chunk = corpus[i:i + chunk_size]\n",
    "            text = \" \".join(chunk)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "\n",
    "            for word in words:\n",
    "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
    "                global_freqs[\" \".join(byte_tokens)] += 1\n",
    "\n",
    "        vocab = global_freqs\n",
    "        self.merges = []\n",
    "\n",
    "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
    "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        self.merges_set = set(tuple(m) for m in self.merges)\n",
    "        self._build_token_vocab()\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, vocab):\n",
    "        new_vocab = Counter()\n",
    "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def encode(self, text, dropout=0.0):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
    "\n",
    "            while len(tokens) > 1:\n",
    "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "                valid_pairs = [p for p in pairs if p in self.merges_set and random.random() > dropout]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
    "                merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "\n",
    "                tokens = new_tokens\n",
    "\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        decoded_bytes = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.special_token_ids:\n",
    "                continue\n",
    "            try:\n",
    "                if len(token) == 6:\n",
    "                    bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
    "                else:\n",
    "                    bytes_seq = [int(token)]\n",
    "                decoded_bytes.extend(bytes_seq)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            return bytes(decoded_bytes).decode('utf-8', errors='ignore')\n",
    "        except Exception:\n",
    "            return \"Corrupted\"\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"merges\": self.merges,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"special_tokens\": self.special_tokens\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        merges = [tuple(m) for m in data[\"merges\"]]\n",
    "        return cls(merges=merges, vocab=data[\"vocab\"], special_tokens=data[\"special_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f41d1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model, load_model\n",
    "\n",
    "# Model architecture\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        context = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        generated = self.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        return self.tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        # Model yapılandırmasını ayrı bir JSON dosyasına kaydet\n",
    "        config = {\n",
    "            'vocab_size': len(self.tokenizer.token_to_id),\n",
    "            'n_embd': self.token_embedding_table.embedding_dim,\n",
    "            'block_size': self.block_size,\n",
    "            'n_layer': len(self.blocks),\n",
    "            'n_head': len(self.blocks[0].sa.heads),\n",
    "            'tokenizer_config': {\n",
    "                'merges': self.tokenizer.merges,\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Config'i ayrı bir dosyaya kaydet\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False)\n",
    "        \n",
    "        # Model ağırlıklarını safetensors formatında kaydet\n",
    "        save_model(self, filepath)\n",
    "        print(f\"Model saved to {filepath} and config to {config_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cuda'):\n",
    "        # Config dosyasını yükle\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Tokenizer'ı oluştur\n",
    "        tokenizer = OptimizedByteLevelBPE(\n",
    "            merges=[tuple(m) for m in config['tokenizer_config']['merges']],\n",
    "            vocab=config['tokenizer_config']['vocab'],\n",
    "            special_tokens=config['tokenizer_config']['special_tokens']\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Modeli başlat\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            n_embd=config['n_embd'],\n",
    "            block_size=config['block_size'],\n",
    "            n_layer=config['n_layer'],\n",
    "            n_head=config['n_head'],\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        \n",
    "        # Ağırlıkları yükle\n",
    "        load_model(model, filepath, strict=True)\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def get_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(train_data if split == 'train' else val_data, block_size, batch_size)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-4, total_iters=10000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    block_size = 1024\n",
    "    max_iters = 50000\n",
    "    eval_interval = 500\n",
    "    learning_rate = 3e-4\n",
    "    eval_iters = 200\n",
    "    n_embd = 1024\n",
    "    n_head = 16\n",
    "    n_layer = 12\n",
    "    dropout = 0.2\n",
    "    save_interval = 100 \n",
    "    \n",
    "    # Load and preprocess data\n",
    "    full_corpus = load_and_preprocess_data(max_samples=50000)\n",
    "    text = \" \".join(full_corpus)\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = OptimizedByteLevelBPE()\n",
    "    tokenizer_path = \"turkish_bpe_model.json\"\n",
    "    \n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training tokenizer...\")\n",
    "        tokenizer.train(full_corpus, num_merges=3000, chunk_size=5000, verbose=True)\n",
    "        tokenizer.save_model(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Loading pretrained tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE.load_model(tokenizer_path)\n",
    "    \n",
    "    # Tokenize text\n",
    "    def encode_text(text):\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        tokens = []\n",
    "        for word in tqdm(words, desc=\"Tokenizing\"):\n",
    "            tokens.extend(tokenizer.encode(word))\n",
    "        return tokens\n",
    "    \n",
    "    tokens = encode_text(text)\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    # Train/val split\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    \n",
    "    print(f\"Total tokens: {len(data)}\")\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Val data size: {len(val_data)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    vocab_size = len(tokenizer.token_to_id)\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        tokenizer=tokenizer\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 3\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        # LR scheduling\n",
    "        lr = get_lr(iter)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Get batch\n",
    "        xb, yb = get_batch(train_data, block_size, batch_size)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, loss = model(xb, yb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation and logging\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters)\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            \n",
    "            print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if iter % save_interval == 0:\n",
    "                checkpoint_path = f\"checkpoints/checkpoint_{iter}.safetensors\"\n",
    "                model.save_model(checkpoint_path)\n",
    "                print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "                model.save_model(best_model_path)\n",
    "                print(f\"New best model saved to {best_model_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6fa91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 500/500 [00:00<00:00, 7536.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 35009/35009 [00:02<00:00, 13568.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 72611\n",
      "Train data size: 65349\n",
      "Val data size: 7262\n",
      "0.38M parameters\n",
      "Step 0: Train 6.1170, Val 6.1168, LR 0.000000\n",
      "Model saved to checkpoints/checkpoint_0.safetensors and config to checkpoints/checkpoint_0_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_0.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 500: Train 1.5764, Val 1.5263, LR 0.000300\n",
      "Model saved to checkpoints/checkpoint_500.safetensors and config to checkpoints/checkpoint_500_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_500.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 999: Train 1.4914, Val 1.4513, LR 0.000298\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoints/checkpoint_0.safetensors\"\n",
    "model = Transformer.load_model(model_path, device)\n",
    "\n",
    "prompt = \"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\"\n",
    "generated = model.generate_from_prompt(\n",
    "    prompt, \n",
    "    max_new_tokens=200, \n",
    "    temperature=0.5, \n",
    "    top_k=50\n",
    ")\n",
    "    \n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
