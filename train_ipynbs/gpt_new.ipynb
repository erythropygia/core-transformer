{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e00fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8d6bf049f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f95aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints/best_model\", exist_ok=True)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(max_samples=50000):\n",
    "    \"\"\"Veri setini yükleyip temizler, summary listesini döner.\"\"\"\n",
    "    dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
    "    processed_texts = []\n",
    "\n",
    "    for i in tqdm(range(min(len(dataset), max_samples)), desc=\"Preprocessing data\"):\n",
    "        summary = clean_text(dataset[i][\"summary\"])\n",
    "        processed_texts.append(summary)\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6875d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "class OptimizedByteLevelBPE:\n",
    "    def __init__(self, merges: Optional[List[Tuple[str, str]]] = None,\n",
    "                 vocab: Optional[Dict[str, int]] = None,\n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.vocab = vocab or {}\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _build_lookup_tables(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # Özel token'lar\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "\n",
    "        offset = len(self.token_to_id)\n",
    "\n",
    "        # Byte token'lar\n",
    "        for i in range(256):\n",
    "            byte_token = f\"{i:03d}\"\n",
    "            self.token_to_id[byte_token] = offset + i\n",
    "\n",
    "        offset = max(self.token_to_id.values()) + 1\n",
    "\n",
    "        # Eğer vocab varsa, ekle\n",
    "        if self.vocab:\n",
    "            for token in sorted(self.vocab.keys()):\n",
    "                if token not in self.token_to_id:\n",
    "                    self.token_to_id[token] = offset\n",
    "                    offset += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.merges_set = set(self.merges)\n",
    "\n",
    "\n",
    "    def _build_token_vocab(self):\n",
    "        \"\"\"Merge sonrası oluşan token sözlüğünü oluşturur\"\"\"\n",
    "        tokens = set()\n",
    "        for a, b in self.merges:\n",
    "            tokens.add(a)\n",
    "            tokens.add(b)\n",
    "            tokens.add(a + b)\n",
    "        tokens = sorted(tokens)\n",
    "\n",
    "        # Token ID'lerini devam ettir\n",
    "        start_id = max(self.token_to_id.values()) + 1\n",
    "        for tok in tokens:\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = start_id\n",
    "                start_id += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.vocab = {k: v for k, v in self.token_to_id.items() if k not in self.special_tokens}\n",
    "\n",
    "    def train(self, corpus: List[str], num_merges: int = 10000,\n",
    "              chunk_size: int = 10000, verbose: bool = True):\n",
    "        \"\"\"BPE algoritması ile tokenizer'ı eğitir\"\"\"\n",
    "        global_freqs = Counter()\n",
    "\n",
    "        # 1. Frekansları hesapla\n",
    "        for i in tqdm(range(0, len(corpus), chunk_size), desc=\"Vocabulary Construction\"):\n",
    "            chunk = corpus[i:i + chunk_size]\n",
    "            text = \" \".join(chunk)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "\n",
    "            for word in words:\n",
    "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
    "                global_freqs[\" \".join(byte_tokens)] += 1\n",
    "\n",
    "        # 2. Merge işlemleri\n",
    "        vocab = global_freqs\n",
    "        self.merges = []\n",
    "\n",
    "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
    "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        self.merges_set = set(self.merges)\n",
    "        self._build_token_vocab()\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _get_stats(self, vocab: Counter) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Sembollerin eş frekanslarını hesaplar\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Counter) -> Counter:\n",
    "        \"\"\"En sık geçen çifti birleştirir\"\"\"\n",
    "        new_vocab = Counter()\n",
    "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def encode(self, text: str, dropout: float = 0.0) -> List[int]:\n",
    "        \"\"\"Metni token ID'lerine çevirir\"\"\"\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
    "\n",
    "            # BPE merge\n",
    "            while len(tokens) > 1:\n",
    "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "                valid_pairs = [\n",
    "                    p for p in pairs\n",
    "                    if p in self.merges_set and random.random() > dropout\n",
    "                ]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
    "                merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "\n",
    "                tokens = new_tokens\n",
    "\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Token ID'lerinden orijinal metni oluşturur\"\"\"\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        decoded_bytes = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.special_token_ids:\n",
    "                continue  # özel token'ları atla\n",
    "            try:\n",
    "                # Tüm token'ı 3'er 3'er parçala\n",
    "                bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
    "                decoded_bytes.extend(bytes_seq)\n",
    "            except ValueError:\n",
    "                pass  # bilinmeyen token varsa yoksay\n",
    "\n",
    "        try:\n",
    "            return bytes(decoded_bytes).decode('utf-8', errors='replace')\n",
    "        except Exception:\n",
    "            return \"Corrupted\"\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, prefix: str):\n",
    "        \"\"\"Modeli diske kaydeder\"\"\"\n",
    "        with open(prefix, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"merges\": self.merges,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"special_tokens\": self.special_tokens\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, prefix: str):\n",
    "        with open(prefix, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        merges = [tuple(m) for m in data[\"merges\"]]\n",
    "        obj = cls(\n",
    "            merges=merges,\n",
    "            vocab=data[\"vocab\"],\n",
    "            special_tokens=data[\"special_tokens\"]\n",
    "        )\n",
    "        # Eksik yapılandırmaları tamamla\n",
    "        obj._build_token_vocab()\n",
    "        obj._build_lookup_tables()\n",
    "        return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297e546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2944, 1822, 43, 104, 105, 36, 2848, 988, 36, 1180, 36, 1031, 2214, 101, 36, 1654, 105]\n",
      "Decoded: Çin'de yapılan bir araştırmaya göre\n"
     ]
    }
   ],
   "source": [
    "# Test için tokenizer'ı kontrol edin\n",
    "tokenizer = OptimizedByteLevelBPE.load_model(\"tokenizer.json\")\n",
    "test_text = \"Çin'de yapılan bir araştırmaya göre\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)  # Orijinal metni tam olarak geri almalı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b73292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from safetensors.torch import save_model, load_model\n",
    "import os\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        context = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        generated = self.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        return self.tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    def save_model(self, filepath, optimizer=None, iter_step=None, best_val_loss=None):\n",
    "        config = {\n",
    "            'vocab_size': len(self.tokenizer.token_to_id),\n",
    "            'n_embd': self.token_embedding_table.embedding_dim,\n",
    "            'block_size': self.block_size,\n",
    "            'n_layer': len(self.blocks),\n",
    "            'n_head': len(self.blocks[0].sa.heads),\n",
    "            'tokenizer_config': {\n",
    "                'merges': self.tokenizer.merges,\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens\n",
    "            }\n",
    "        }\n",
    "\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        trainstate_path = filepath.replace('.safetensors', '_trainstate.pt')\n",
    "\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False)\n",
    "\n",
    "        save_model(self, filepath)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            torch.save({\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'iter': iter_step,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, trainstate_path)\n",
    "            print(f\"Model weights and Training state saved to {trainstate_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cuda'):\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        trainstate_path = filepath.replace('.safetensors', '_trainstate.pt')\n",
    "\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        tokenizer = OptimizedByteLevelBPE(\n",
    "            merges=[tuple(m) for m in config['tokenizer_config']['merges']],\n",
    "            vocab=config['tokenizer_config']['vocab'],\n",
    "            special_tokens=config['tokenizer_config']['special_tokens']\n",
    "        )\n",
    "\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            n_embd=config['n_embd'],\n",
    "            block_size=config['block_size'],\n",
    "            n_layer=config['n_layer'],\n",
    "            n_head=config['n_head'],\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "        load_model(model, filepath, strict=True)\n",
    "\n",
    "        train_state = None\n",
    "        if os.path.exists(trainstate_path):\n",
    "            train_state = torch.load(trainstate_path, map_location=device)\n",
    "            print(f\"Training state loaded from {trainstate_path}\")\n",
    "\n",
    "        print(f\"Model loaded from {filepath} with config {config_path}\")\n",
    "        return model, tokenizer, train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for batching\n",
    "def encode_texts(tokenizer, texts, dropout=0.0):\n",
    "    \"\"\"Her metni ayrı tokenize edip liste listesi döner.\"\"\"\n",
    "    encoded_texts = []\n",
    "    for txt in texts:\n",
    "        token_ids = tokenizer.encode(txt, dropout=dropout)\n",
    "        encoded_texts.append(token_ids)\n",
    "    return encoded_texts\n",
    "\n",
    "def get_batch(data, block_size, batch_size, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    data: 1D tensor, uzun token dizisi\n",
    "    block_size: model context window\n",
    "    batch_size: kaç tane örnek alınacak\n",
    "    \"\"\"\n",
    "    # 0..len(data)-block_size-1 arası rastgele başlangıç pozisyonları seç\n",
    "    max_start_idx = data.size(0) - block_size - 1\n",
    "    starts = torch.randint(0, max_start_idx, (batch_size,))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for start in starts:\n",
    "        input_seq = data[start : start + block_size]\n",
    "        target_seq = data[start + 1 : start + block_size + 1]\n",
    "\n",
    "        # Eğer padding yapılacaksa buraya ekle (genelde 1D tokenlarda gerek yok)\n",
    "        inputs.append(input_seq.unsqueeze(0))\n",
    "        targets.append(target_seq.unsqueeze(0))\n",
    "\n",
    "    xb = torch.cat(inputs, dim=0)   # batch_size x block_size\n",
    "    yb = torch.cat(targets, dim=0)  # batch_size x block_size\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(train_data if split == 'train' else val_data, block_size, batch_size, pad_token_id)\n",
    "            _, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=3e-4, min_lr=1e-5, total_iters=10000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "        return min_lr + (max_lr - min_lr) * cosine_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(resume_training=False, checkpoint_path=None):\n",
    "    \n",
    "    batch_size = 64\n",
    "    block_size = 64\n",
    "    max_iters = 10000\n",
    "    learning_rate = 3e-4\n",
    "    eval_interval = 100\n",
    "    eval_iters = 200\n",
    "    n_embd = 64\n",
    "    n_head = 8\n",
    "    n_layer = 8\n",
    "    save_interval = 500\n",
    "    weight_decay = 1e-2\n",
    "    patience = 3\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    full_corpus = load_and_preprocess_data(max_samples=10000)\n",
    "\n",
    "    tokenizer_path = \"tokenizer.json\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE()\n",
    "        tokenizer.train(full_corpus, num_merges=3000, chunk_size=5000, verbose=True)\n",
    "        tokenizer.save_model(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Loading pretrained tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE.load_model(tokenizer_path)\n",
    "\n",
    "    pad_token_id = tokenizer.token_to_id.get('<pad>', 0)\n",
    "\n",
    "    def encode_text(text):\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            tokens.extend(tokenizer.encode(word))\n",
    "        return tokens\n",
    "\n",
    "    tokens = []\n",
    "    for text in tqdm(full_corpus, desc=\"Encoding texts\"):\n",
    "        tokens.extend(encode_text(text))\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    print(f\"Total tokens: {len(data)}\")\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "    iter_start = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if resume_training and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Resuming training from {checkpoint_path}...\")\n",
    "        model, tokenizer, train_state = Transformer.load_model(checkpoint_path, device=device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        optimizer.load_state_dict(train_state['optimizer'])\n",
    "        iter_start = train_state['iter']\n",
    "        best_val_loss = train_state['best_val_loss']\n",
    "    else:\n",
    "        vocab_size = len(tokenizer.token_to_id)\n",
    "        model = Transformer(\n",
    "            vocab_size=vocab_size,\n",
    "            n_embd=n_embd,\n",
    "            block_size=block_size,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "    model.train()\n",
    "    patience_counter = 0\n",
    "    is_first = True\n",
    "\n",
    "    scaler = GradScaler(device=\"cuda\")\n",
    "\n",
    "    for iter in range(iter_start, max_iters):\n",
    "        lr = get_lr(iter)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for acc_step in range(accumulation_steps):\n",
    "            xb, yb = get_batch(train_data, block_size, batch_size, pad_token_id)\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                logits, loss = model(xb, yb)\n",
    "                loss = loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id)\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "\n",
    "            if not is_first:\n",
    "                if iter % save_interval == 0:\n",
    "                    checkpoint_path = f\"checkpoints/checkpoint_{iter}.safetensors\"\n",
    "                    model.save_model(checkpoint_path, optimizer=optimizer, iter_step=iter, best_val_loss=best_val_loss)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "                    model.save_model(best_model_path, optimizer=optimizer, iter_step=iter, best_val_loss=best_val_loss)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                is_first = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ca6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(resume_training=True, checkpoint_path=\"checkpoints/best_model/best_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d017e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve daha sonra birçok ülkede kullanılmıştır.The Walter Walter of Fall tarafından geliştirilen ve Kuzey Amerika'da yayınlanan bir video oyunu olan bir video oyunudur ve birçok oyun ve ödül kazanmıştır.Kuzey Amerika'da doğan bir bilim insanıdır ve insan ve doğal olan ve doğal alanlarında uzmanlaşmıştır. Slovak ilinin Çağ ilçesine bağlı olan köy, tarihi Gürcü köyünün adı olan \"Kuzey Kuzey Kuzey Kıbrıs Türk Cumhuriyeti'nde yer alan ve Türk halk müziği olup, müzik ve tarihsel eserlerinin de önem\n"
     ]
    }
   ],
   "source": [
    "model_path = \"checkpoints/best_model.safetensors\"\n",
    "model = Transformer.load_model(model_path, device)\n",
    "\n",
    "prompt = \"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\"\n",
    "\n",
    "generated = model.generate_from_prompt(\n",
    "    prompt, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.3, \n",
    "    top_k=30\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve SS S S La tarafından verilen bir olarak kabul edilen bir ve p al olup, da da bu ve ca da bu p ve gibi da da da da da da da da da da da da ve bya bulunur; ayrıca ayrıca da ayrıca da da da da da da da da da da ve da da da da da da da da a da da da a da da da giyda da ve da yer yer almıştır.Dil ve Park Ra Duro Hun S\n",
    "#Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve 2014 yılında Panter tarafından yayınlanmıştır.Panz Pear, İngiliz yazar ve akademisyen olup, \"Sloven Ke K\" adlı eseri olarak bilinir ve \"Lin Ke \"Lad\" adlı eseri \"Lad \" adlı şarkısıyla tanınır; \"Sloven \", \"Love \" şarkısı\" şarkısı, \"Soke\" ve \" \" şarkısı \" \" \" şarkısı \" sözü\" olarak bilinir.Açe Dil tarafından yazılan \"The \", \"Gece of the \" adlı kitabı \"Kahraman\" kitabı, \"Nobel King\" ve \"Star\" adlı eseri ver\n",
    "#Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve daha sonra birçok ülkede kullanılmıştır.The Walter Walter of Fall tarafından geliştirilen ve Kuzey Amerika'da yayınlanan bir video oyunu olan bir video oyunudur ve birçok oyun ve ödül kazanmıştır.Kuzey Amerika'da doğan bir bilim insanıdır ve insan ve doğal olan ve doğal alanlarında uzmanlaşmıştır. Slovak ilinin Çağ ilçesine bağlı olan köy, tarihi Gürcü köyünün adı olan \"Kuzey Kuzey Kuzey Kıbrıs Türk Cumhuriyeti'nde yer alan ve Türk halk müziği olup, müzik ve tarihsel eserlerinin de önem\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
