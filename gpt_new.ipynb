{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e00fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7906a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f76701249f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f95aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints/best_model\", exist_ok=True)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(max_samples=50000):\n",
    "    \"\"\"Veri setini yükleyip temizler, summary listesini döner.\"\"\"\n",
    "    dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
    "    processed_texts = []\n",
    "\n",
    "    for i in tqdm(range(min(len(dataset), max_samples)), desc=\"Preprocessing data\"):\n",
    "        summary = clean_text(dataset[i][\"summary\"])\n",
    "        processed_texts.append(summary)\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6875d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedByteLevelBPE:\n",
    "    def __init__(self, merges=None, vocab=None, special_tokens=None):\n",
    "        self.merges = merges or []\n",
    "        self.vocab = vocab or {}\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _build_lookup_tables(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "\n",
    "        offset = len(self.token_to_id)\n",
    "        for i in range(256):\n",
    "            byte_token = f\"{i:03d}\"\n",
    "            self.token_to_id[byte_token] = offset + i\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.merges_set = set(tuple(m) for m in self.merges)\n",
    "\n",
    "\n",
    "    def train(self, corpus, num_merges=10000, chunk_size=10000, verbose=True):\n",
    "        global_freqs = Counter()\n",
    "\n",
    "        for i in tqdm(range(0, len(corpus), chunk_size), desc=\"Vocabulary Construction\"):\n",
    "            chunk = corpus[i:i + chunk_size]\n",
    "            text = \" \".join(chunk)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "\n",
    "            for word in words:\n",
    "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
    "                global_freqs[\" \".join(byte_tokens)] += 1\n",
    "\n",
    "        vocab = global_freqs\n",
    "        self.merges = []\n",
    "\n",
    "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
    "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        self.merges_set = set(tuple(m) for m in self.merges)\n",
    "        self._build_token_vocab()\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, vocab):\n",
    "        new_vocab = Counter()\n",
    "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def encode(self, text, dropout=0.0):\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
    "\n",
    "            while len(tokens) > 1:\n",
    "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "                valid_pairs = [p for p in pairs if p in self.merges_set and random.random() > dropout]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
    "                merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "\n",
    "                tokens = new_tokens\n",
    "\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        decoded_bytes = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.special_token_ids:\n",
    "                continue\n",
    "            try:\n",
    "                if len(token) == 6:\n",
    "                    bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
    "                else:\n",
    "                    bytes_seq = [int(token)]\n",
    "                decoded_bytes.extend(bytes_seq)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            return bytes(decoded_bytes).decode('utf-8', errors='ignore')\n",
    "        except Exception:\n",
    "            return \"Corrupted\"\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"merges\": self.merges,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"special_tokens\": self.special_tokens\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        merges = [tuple(m) for m in data[\"merges\"]]\n",
    "        return cls(merges=merges, vocab=data[\"vocab\"], special_tokens=data[\"special_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1b73292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model, load_model\n",
    "\n",
    "# Model architecture\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        context = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        generated = self.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        return self.tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        # Model yapılandırmasını ayrı bir JSON dosyasına kaydet\n",
    "        config = {\n",
    "            'vocab_size': len(self.tokenizer.token_to_id),\n",
    "            'n_embd': self.token_embedding_table.embedding_dim,\n",
    "            'block_size': self.block_size,\n",
    "            'n_layer': len(self.blocks),\n",
    "            'n_head': len(self.blocks[0].sa.heads),\n",
    "            'tokenizer_config': {\n",
    "                'merges': self.tokenizer.merges,\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Config'i ayrı bir dosyaya kaydet\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False)\n",
    "        \n",
    "        # Model ağırlıklarını safetensors formatında kaydet\n",
    "        save_model(self, filepath)\n",
    "        print(f\"Model saved to {filepath} and config to {config_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cuda'):\n",
    "        # Config dosyasını yükle\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Tokenizer'ı oluştur\n",
    "        tokenizer = OptimizedByteLevelBPE(\n",
    "            merges=[tuple(m) for m in config['tokenizer_config']['merges']],\n",
    "            vocab=config['tokenizer_config']['vocab'],\n",
    "            special_tokens=config['tokenizer_config']['special_tokens']\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Modeli başlat\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            n_embd=config['n_embd'],\n",
    "            block_size=config['block_size'],\n",
    "            n_layer=config['n_layer'],\n",
    "            n_head=config['n_head'],\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        \n",
    "        # Ağırlıkları yükle\n",
    "        load_model(model, filepath, strict=True)\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bb2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for batching\n",
    "def encode_texts(tokenizer, texts, dropout=0.0):\n",
    "    \"\"\"Her metni ayrı tokenize edip liste listesi döner.\"\"\"\n",
    "    encoded_texts = []\n",
    "    for txt in texts:\n",
    "        token_ids = tokenizer.encode(txt, dropout=dropout)\n",
    "        encoded_texts.append(token_ids)\n",
    "    return encoded_texts\n",
    "\n",
    "def get_batch(data, block_size, batch_size, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    data: 1D tensor, uzun token dizisi\n",
    "    block_size: model context window\n",
    "    batch_size: kaç tane örnek alınacak\n",
    "    \"\"\"\n",
    "    # 0..len(data)-block_size-1 arası rastgele başlangıç pozisyonları seç\n",
    "    max_start_idx = data.size(0) - block_size - 1\n",
    "    starts = torch.randint(0, max_start_idx, (batch_size,))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for start in starts:\n",
    "        input_seq = data[start : start + block_size]\n",
    "        target_seq = data[start + 1 : start + block_size + 1]\n",
    "\n",
    "        # Eğer padding yapılacaksa buraya ekle (genelde 1D tokenlarda gerek yok)\n",
    "        inputs.append(input_seq.unsqueeze(0))\n",
    "        targets.append(target_seq.unsqueeze(0))\n",
    "\n",
    "    xb = torch.cat(inputs, dim=0)   # batch_size x block_size\n",
    "    yb = torch.cat(targets, dim=0)  # batch_size x block_size\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(train_data if split == 'train' else val_data, block_size, batch_size, pad_token_id)\n",
    "            _, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-4, total_iters=10000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    batch_size = 64 \n",
    "    block_size = 256 #1024\n",
    "    \n",
    "    max_iters = 10000 #50000\n",
    "  \n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    eval_interval = 500\n",
    "    eval_iters = 200\n",
    "\n",
    "    n_embd = 768 #1024\n",
    "    n_head = 12 #16 \n",
    "    n_layer = 8 #8\n",
    "\n",
    "    save_interval = 100\n",
    "    \n",
    "    weight_decay = 1e-2\n",
    "    patience = 3\n",
    "\n",
    "    # Load and preprocess data\n",
    "    full_corpus = load_and_preprocess_data(max_samples=500)\n",
    "\n",
    "    # Initialize or load tokenizer (benim yeni mantıkla)\n",
    "    tokenizer_path = \"tokenizer.json\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE()\n",
    "        tokenizer.train(full_corpus, num_merges=3000, chunk_size=5000, verbose=True)\n",
    "        tokenizer.save_model(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Loading pretrained tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE.load_model(tokenizer_path)\n",
    "\n",
    "    pad_token_id = tokenizer.token_to_id.get('<pad>', None)\n",
    "    if pad_token_id is None:\n",
    "        # Eğer tokenizer'da <pad> yoksa elle ekle veya 0 yap\n",
    "        print(\"Tokenizer does not have <pad> token. Using 0 as pad_token_id.\")\n",
    "        pad_token_id = 0\n",
    "\n",
    "    # Tokenize each text separately (benim yeni encode_text fonksiyonum gibi)\n",
    "    def encode_text(text):\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        tokens = []\n",
    "        for word in tqdm(words, desc=\"Tokenizing\"):\n",
    "            tokens.extend(tokenizer.encode(word))\n",
    "        return tokens\n",
    "\n",
    "    tokens = []\n",
    "    for text in tqdm(full_corpus, desc=\"Encoding texts\"):\n",
    "        tokens.extend(encode_text(text))\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    # Train/val split\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    print(f\"Total tokens: {len(data)}\")\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    vocab_size = len(tokenizer.token_to_id)\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        tokenizer=tokenizer,\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    for iter in range(max_iters):\n",
    "        # Learning rate schedule (varsa)\n",
    "        lr = get_lr(iter)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        xb, yb = get_batch(train_data, block_size, batch_size, pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id)\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "\n",
    "            if iter % save_interval == 0:\n",
    "                checkpoint_path = f\"checkpoints/checkpoint_{iter}.safetensors\"\n",
    "                model.save_model(checkpoint_path)\n",
    "                print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "                model.save_model(best_model_path)\n",
    "                print(f\"New best model saved to {best_model_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "559ca6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 500/500 [00:00<00:00, 8003.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 16978.18it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9631.17it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 10428.53it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 9803.78it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 8928.73it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 9643.28it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 11406.53it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 12109.04it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 14302.09it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 11189.26it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 7890.91it/s]/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 14945.29it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 10318.32it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 18688.66it/s]\n",
      "Tokenizing: 100%|██████████| 33/33 [00:00<00:00, 11600.07it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 16063.57it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 9610.71it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 9646.40it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 11472.86it/s]\n",
      "Tokenizing: 100%|██████████| 117/117 [00:00<00:00, 9629.59it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 10832.19it/s]s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 14251.42it/s]\n",
      "Tokenizing: 100%|██████████| 29/29 [00:00<00:00, 13115.68it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 9095.26it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 12126.21it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 7050.18it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 9720.46it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 9452.58it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 11147.79it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 12141.60it/s]s]\n",
      "Tokenizing: 100%|██████████| 101/101 [00:00<00:00, 11568.76it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 9717.55it/s]\n",
      "Tokenizing: 100%|██████████| 33/33 [00:00<00:00, 9180.95it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 14645.17it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 9512.33it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 11156.52it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 13128.06it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 14231.57it/s]\n",
      "Tokenizing: 100%|██████████| 99/99 [00:00<00:00, 8550.81it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 15237.72it/s]s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 7830.91it/s]\n",
      "Tokenizing: 100%|██████████| 117/117 [00:00<00:00, 13364.20it/s]\n",
      "Tokenizing: 100%|██████████| 107/107 [00:00<00:00, 14746.35it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 7392.80it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 8893.58it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 10490.43it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 9678.78it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 8345.15it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 6361.35it/s]/s]\n",
      "Tokenizing: 100%|██████████| 101/101 [00:00<00:00, 7782.78it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 9850.21it/s]\n",
      "Tokenizing: 100%|██████████| 99/99 [00:00<00:00, 11497.93it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 15586.53it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 11082.83it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 11312.22it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 10811.63it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11380.30it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 13296.19it/s]s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 9039.02it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 15559.76it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 7763.06it/s]\n",
      "Tokenizing: 100%|██████████| 33/33 [00:00<00:00, 7945.58it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 9625.87it/s]\n",
      "Tokenizing: 100%|██████████| 99/99 [00:00<00:00, 13054.45it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 13355.03it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 10075.88it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 9814.24it/s]/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 8542.63it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 8302.34it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 9299.31it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 8141.21it/s]\n",
      "Tokenizing: 100%|██████████| 107/107 [00:00<00:00, 11583.48it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 8200.35it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 15351.25it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 13507.80it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 15451.54it/s]s]\n",
      "Tokenizing: 100%|██████████| 119/119 [00:00<00:00, 12837.83it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 8484.63it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 11682.55it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 9961.97it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 7603.31it/s]\n",
      "Tokenizing: 100%|██████████| 107/107 [00:00<00:00, 11796.00it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 17105.12it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 13589.45it/s]\n",
      "Tokenizing: 100%|██████████| 97/97 [00:00<00:00, 11468.89it/s]s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 17679.58it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 20735.76it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 9674.66it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 13956.02it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 12831.60it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 6955.05it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 8932.24it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 5933.91it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 5443.33it/s]/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 4220.71it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 4058.57it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 7434.54it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 7062.82it/s]\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 11408.74it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 8959.89it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 9529.18it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 9030.99it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 7960.93it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 119/119 [00:00<00:00, 8033.38it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11057.27it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 10611.23it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 10711.86it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 11484.35it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 8794.60it/s]\n",
      "Tokenizing: 100%|██████████| 99/99 [00:00<00:00, 8071.61it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 9559.44it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 10203.91it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 9520.29it/s]\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 10425.65it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 8353.41it/s]\n",
      "Tokenizing: 100%|██████████| 33/33 [00:00<00:00, 11180.29it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 17946.29it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 8293.15it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10261.57it/s]/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 10013.05it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 9187.02it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 10482.85it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 10153.34it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9285.57it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 10010.01it/s]\n",
      "Tokenizing: 100%|██████████| 35/35 [00:00<00:00, 8543.36it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 10757.34it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 10657.15it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 9331.39it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 9947.63it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 12286.71it/s]\n",
      "Tokenizing: 100%|██████████| 131/131 [00:00<00:00, 11565.74it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 13517.19it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 10420.33it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 8050.49it/s]\n",
      "Tokenizing: 100%|██████████| 145/145 [00:00<00:00, 12361.01it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 9952.91it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 12931.30it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 6719.44it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 12926.58it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 13432.91it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10741.85it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 10746.27it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 6546.27it/s]\n",
      "Tokenizing: 100%|██████████| 113/113 [00:00<00:00, 14158.52it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 15495.15it/s]/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 6958.86it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 10556.41it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 9115.36it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 9738.37it/s]\n",
      "Tokenizing: 100%|██████████| 35/35 [00:00<00:00, 6986.18it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 13144.99it/s]\n",
      "Tokenizing: 100%|██████████| 131/131 [00:00<00:00, 13802.25it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 6800.03it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 11086.27it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 8948.73it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 17259.17it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 9497.29it/s]\n",
      "Tokenizing: 100%|██████████| 107/107 [00:00<00:00, 5874.53it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 6443.76it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 6600.99it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11722.02it/s]/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 7825.73it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 12091.47it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 19418.07it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 7738.92it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 12772.80it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 7500.70it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 7700.30it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 11241.07it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 8351.49it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 10985.03it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 8597.68it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 9429.41it/s]\n",
      "Tokenizing: 100%|██████████| 153/153 [00:00<00:00, 17289.81it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 8417.82it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 18728.43it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 12734.64it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 9135.32it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 9064.26it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 10203.53it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 7120.27it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 10009.95it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 14591.95it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 13074.77it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 9113.16it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 10695.59it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 7446.29it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 12177.17it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 10222.44it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 8793.09it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 13049.90it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 12913.75it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 11527.69it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 9576.04it/s]\n",
      "Tokenizing: 100%|██████████| 27/27 [00:00<00:00, 9341.43it/s]\n",
      "Tokenizing: 100%|██████████| 141/141 [00:00<00:00, 11535.62it/s]]\n",
      "Tokenizing: 100%|██████████| 109/109 [00:00<00:00, 10696.00it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 16374.86it/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 14142.99it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 12283.17it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 10628.53it/s]\n",
      "Tokenizing: 100%|██████████| 99/99 [00:00<00:00, 11749.08it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 12996.43it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 4740.24it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 10761.20it/s]/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 12530.45it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 3614.80it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 8665.65it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 4017.14it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 7394.44it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 11226.29it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 10191.08it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 9166.97it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 10921.46it/s]/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 11542.57it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 9589.83it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 13447.54it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 10368.25it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 6064.22it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 12461.53it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11498.04it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 11866.43it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 9807.32it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 97/97 [00:00<00:00, 11402.04it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 9177.91it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 4824.11it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 8895.06it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 11362.62it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 12175.42it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 10991.68it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 13687.88it/s]/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10974.44it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 8439.62it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 8583.49it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 13571.87it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 13460.02it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 12867.35it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 10846.51it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 10991.89it/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 9872.52it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 11701.58it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 11994.76it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 8980.70it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 11935.36it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 14261.66it/s]\n",
      "Tokenizing: 100%|██████████| 109/109 [00:00<00:00, 9319.73it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10836.78it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 15965.14it/s]\n",
      "Tokenizing: 100%|██████████| 107/107 [00:00<00:00, 17093.53it/s]]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 9081.80it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 10834.83it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 18428.40it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 9619.96it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 9972.19it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 4307.36it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 5514.15it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 6206.76it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 7132.83it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 8612.93it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 9549.58it/s]\n",
      "Tokenizing: 100%|██████████| 115/115 [00:00<00:00, 7944.41it/s]\n",
      "Tokenizing: 100%|██████████| 95/95 [00:00<00:00, 8979.56it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 7370.36it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 7170.76it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 9536.17it/s]\n",
      "Tokenizing: 100%|██████████| 25/25 [00:00<00:00, 8809.34it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 14706.36it/s]/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 8505.44it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 14713.58it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9328.72it/s]\n",
      "Tokenizing: 100%|██████████| 109/109 [00:00<00:00, 12899.72it/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 13778.46it/s]\n",
      "Tokenizing: 100%|██████████| 35/35 [00:00<00:00, 14967.44it/s]\n",
      "Tokenizing: 100%|██████████| 95/95 [00:00<00:00, 11908.16it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 6107.22it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 11982.94it/s]/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9514.76it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 14614.30it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 11252.14it/s]\n",
      "Tokenizing: 100%|██████████| 111/111 [00:00<00:00, 15225.08it/s]\n",
      "Tokenizing: 100%|██████████| 115/115 [00:00<00:00, 16906.00it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 12526.44it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 9158.21it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 10516.49it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 10971.65it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 13746.90it/s]/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 13403.19it/s]\n",
      "Tokenizing: 100%|██████████| 35/35 [00:00<00:00, 8747.51it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 12111.11it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11039.35it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 13074.79it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 10010.83it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 11318.48it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 11468.39it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 12074.94it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 11416.26it/s]/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 4584.06it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 6955.19it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 5195.60it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 7568.82it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 6157.81it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 12609.20it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 10256.22it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 9221.81it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 11222.81it/s]/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 13390.32it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 13346.91it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 8193.12it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 8150.47it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 13590.36it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 16612.35it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 12193.97it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 4990.97it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 11359.08it/s]/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 12261.99it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 9878.82it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 7484.33it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 10782.27it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 15328.95it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 12904.99it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11951.96it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 10508.79it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 10752.29it/s]/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 9940.61it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 7842.75it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 6212.95it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 17111.32it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 11309.17it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 14022.50it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 8352.47it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 8940.03it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 12168.61it/s]/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 5480.09it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 4357.76it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 4879.81it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 5227.84it/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 5250.62it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 7327.31it/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 7288.03it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 8903.48it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 10478.72it/s]/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 10903.62it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 9216.41it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 12231.09it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 13557.20it/s]\n",
      "Tokenizing: 100%|██████████| 143/143 [00:00<00:00, 15706.54it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 11393.28it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 9216.36it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 9505.22it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 10437.61it/s]/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 11014.19it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 14820.86it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 7434.19it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 10130.31it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 13620.12it/s]\n",
      "Tokenizing: 100%|██████████| 29/29 [00:00<00:00, 9794.25it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 14795.12it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 15081.13it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 6598.24it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 16744.17it/s]/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 6999.68it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 16021.83it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10613.43it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 12783.12it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 13018.05it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 10518.24it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 15659.21it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 6233.08it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 6561.98it/s]\n",
      "Tokenizing: 100%|██████████| 95/95 [00:00<00:00, 5428.30it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 83/83 [00:00<00:00, 9390.82it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 8824.77it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 7395.07it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 8483.31it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 7899.77it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 14663.66it/s]\n",
      "Tokenizing: 100%|██████████| 97/97 [00:00<00:00, 12398.59it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 8838.97it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 7719.16it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 11227.95it/s]\n",
      "Tokenizing: 100%|██████████| 35/35 [00:00<00:00, 13308.01it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 10404.32it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 10625.53it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 14854.56it/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 9609.79it/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 10523.18it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 17984.84it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 10220.71it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 10690.66it/s]/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 10946.55it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 6871.92it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 13577.84it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 11084.99it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 10437.77it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 9153.47it/s]\n",
      "Tokenizing: 100%|██████████| 101/101 [00:00<00:00, 11607.75it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 10862.43it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 17534.92it/s]/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 7757.92it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 5447.54it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 5011.99it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 3583.06it/s]\n",
      "Tokenizing: 100%|██████████| 115/115 [00:00<00:00, 11983.13it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 5906.32it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 10972.54it/s]\n",
      "Tokenizing: 100%|██████████| 31/31 [00:00<00:00, 17659.03it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 8642.29it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 39/39 [00:00<00:00, 8211.33it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 14045.67it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 13918.11it/s]\n",
      "Tokenizing: 100%|██████████| 119/119 [00:00<00:00, 15449.83it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 11075.41it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 17169.30it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 9352.15it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10695.01it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 14812.02it/s]/s]\n",
      "Tokenizing: 100%|██████████| 11/11 [00:00<00:00, 8698.59it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 9264.79it/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 12241.42it/s]\n",
      "Tokenizing: 100%|██████████| 47/47 [00:00<00:00, 10597.37it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 8439.24it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 14746.74it/s]\n",
      "Tokenizing: 100%|██████████| 33/33 [00:00<00:00, 11663.61it/s]\n",
      "Tokenizing: 100%|██████████| 105/105 [00:00<00:00, 10723.46it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 12116.18it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 13335.44it/s]/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 7316.87it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9558.12it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11043.05it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 6932.97it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 5121.77it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 3979.70it/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 9340.42it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 8047.08it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 4337.44it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 6077.86it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 6054.60it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 11459.07it/s]\n",
      "Tokenizing: 100%|██████████| 85/85 [00:00<00:00, 8942.63it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 9235.03it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 8182.17it/s]\n",
      "Tokenizing: 100%|██████████| 79/79 [00:00<00:00, 7432.04it/s]\n",
      "Tokenizing: 100%|██████████| 71/71 [00:00<00:00, 9859.80it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 129/129 [00:00<00:00, 6765.60it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 11791.28it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 14611.56it/s]\n",
      "Tokenizing: 100%|██████████| 77/77 [00:00<00:00, 6573.34it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 6915.14it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 10226.59it/s]\n",
      "Tokenizing: 100%|██████████| 91/91 [00:00<00:00, 9310.22it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 11085.81it/s]/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11950.92it/s]\n",
      "Tokenizing: 100%|██████████| 131/131 [00:00<00:00, 7831.22it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 7133.67it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 7817.34it/s]\n",
      "Tokenizing: 100%|██████████| 25/25 [00:00<00:00, 9730.66it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 9361.38it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 3277.41it/s]\n",
      "Tokenizing: 100%|██████████| 65/65 [00:00<00:00, 3782.11it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 9959.52it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 9062.75it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 7374.30it/s]\n",
      "Tokenizing: 100%|██████████| 43/43 [00:00<00:00, 10947.86it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 7613.07it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11878.58it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 10444.24it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 8020.86it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 15414.02it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 6739.64it/s]\n",
      "Tokenizing: 100%|██████████| 61/61 [00:00<00:00, 10897.54it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 12918.27it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 8588.24it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 9435.16it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11848.97it/s]\n",
      "Tokenizing: 100%|██████████| 103/103 [00:00<00:00, 19751.89it/s]\n",
      "Tokenizing: 100%|██████████| 49/49 [00:00<00:00, 10783.40it/s]/s]\n",
      "Tokenizing: 100%|██████████| 87/87 [00:00<00:00, 14001.40it/s]\n",
      "Tokenizing: 100%|██████████| 81/81 [00:00<00:00, 12563.37it/s]\n",
      "Tokenizing: 100%|██████████| 109/109 [00:00<00:00, 12711.43it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 18038.42it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 13540.35it/s]\n",
      "Tokenizing: 100%|██████████| 75/75 [00:00<00:00, 10055.39it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 3565.63it/s]\n",
      "Tokenizing: 100%|██████████| 37/37 [00:00<00:00, 6522.20it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 45/45 [00:00<00:00, 5651.69it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 6224.01it/s]\n",
      "Tokenizing: 100%|██████████| 51/51 [00:00<00:00, 11104.68it/s]\n",
      "Tokenizing: 100%|██████████| 55/55 [00:00<00:00, 5614.86it/s]\n",
      "Tokenizing: 100%|██████████| 63/63 [00:00<00:00, 14602.99it/s]\n",
      "Tokenizing: 100%|██████████| 135/135 [00:00<00:00, 9273.81it/s]\n",
      "Tokenizing: 100%|██████████| 69/69 [00:00<00:00, 10261.57it/s]\n",
      "Tokenizing: 100%|██████████| 53/53 [00:00<00:00, 9396.72it/s]t/s]\n",
      "Tokenizing: 100%|██████████| 41/41 [00:00<00:00, 4821.85it/s]\n",
      "Tokenizing: 100%|██████████| 59/59 [00:00<00:00, 13502.70it/s]\n",
      "Tokenizing: 100%|██████████| 93/93 [00:00<00:00, 11970.49it/s]\n",
      "Tokenizing: 100%|██████████| 89/89 [00:00<00:00, 12873.51it/s]\n",
      "Tokenizing: 100%|██████████| 67/67 [00:00<00:00, 9399.55it/s]\n",
      "Tokenizing: 100%|██████████| 57/57 [00:00<00:00, 10176.88it/s]\n",
      "Tokenizing: 100%|██████████| 73/73 [00:00<00:00, 12241.49it/s]\n",
      "Encoding texts: 100%|██████████| 500/500 [00:06<00:00, 77.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 72112\n",
      "Train data size: 64900\n",
      "Val data size: 7212\n",
      "0.95M parameters\n",
      "Step 0: Train 5.6938, Val 5.6975, LR 0.000000\n",
      "Model saved to checkpoints/checkpoint_0.safetensors and config to checkpoints/checkpoint_0_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_0.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 499: Train 1.6350, Val 1.6037, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d017e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "model = Transformer.load_model(model_path, device)\n",
    "\n",
    "prompt = \"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\"\n",
    "generated = model.generate_from_prompt(\n",
    "    prompt, \n",
    "    max_new_tokens=200, \n",
    "    temperature=0.5, \n",
    "    top_k=50\n",
    ")\n",
    "    \n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
