{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e00fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "import unicodedata\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7906a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f203d3089f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f95aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints/best_model\", exist_ok=True)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(max_samples=50000):\n",
    "    \"\"\"Veri setini yükleyip temizler, summary listesini döner.\"\"\"\n",
    "    dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
    "    processed_texts = []\n",
    "\n",
    "    for i in tqdm(range(min(len(dataset), max_samples)), desc=\"Preprocessing data\"):\n",
    "        summary = clean_text(dataset[i][\"summary\"])\n",
    "        processed_texts.append(summary)\n",
    "\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6875d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "class OptimizedByteLevelBPE:\n",
    "    def __init__(self, merges: Optional[List[Tuple[str, str]]] = None,\n",
    "                 vocab: Optional[Dict[str, int]] = None,\n",
    "                 special_tokens: Optional[List[str]] = None):\n",
    "        self.merges = merges\n",
    "        self.vocab = vocab or {}\n",
    "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _build_lookup_tables(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # Özel token'lar\n",
    "        for idx, token in enumerate(self.special_tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "\n",
    "        offset = len(self.token_to_id)\n",
    "\n",
    "        # Byte token'lar\n",
    "        for i in range(256):\n",
    "            byte_token = f\"{i:03d}\"\n",
    "            self.token_to_id[byte_token] = offset + i\n",
    "\n",
    "        offset = max(self.token_to_id.values()) + 1\n",
    "\n",
    "        # Eğer vocab varsa, ekle\n",
    "        if self.vocab:\n",
    "            for token in sorted(self.vocab.keys()):\n",
    "                if token not in self.token_to_id:\n",
    "                    self.token_to_id[token] = offset\n",
    "                    offset += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
    "        self.merges_set = set(self.merges)\n",
    "\n",
    "\n",
    "    def _build_token_vocab(self):\n",
    "        \"\"\"Merge sonrası oluşan token sözlüğünü oluşturur\"\"\"\n",
    "        tokens = set()\n",
    "        for a, b in self.merges:\n",
    "            tokens.add(a)\n",
    "            tokens.add(b)\n",
    "            tokens.add(a + b)\n",
    "        tokens = sorted(tokens)\n",
    "\n",
    "        # Token ID'lerini devam ettir\n",
    "        start_id = max(self.token_to_id.values()) + 1\n",
    "        for tok in tokens:\n",
    "            if tok not in self.token_to_id:\n",
    "                self.token_to_id[tok] = start_id\n",
    "                start_id += 1\n",
    "\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        self.vocab = {k: v for k, v in self.token_to_id.items() if k not in self.special_tokens}\n",
    "\n",
    "    def train(self, corpus: List[str], num_merges: int = 10000,\n",
    "              chunk_size: int = 10000, verbose: bool = True):\n",
    "        \"\"\"BPE algoritması ile tokenizer'ı eğitir\"\"\"\n",
    "        global_freqs = Counter()\n",
    "\n",
    "        # 1. Frekansları hesapla\n",
    "        for i in tqdm(range(0, len(corpus), chunk_size), desc=\"Vocabulary Construction\"):\n",
    "            chunk = corpus[i:i + chunk_size]\n",
    "            text = \" \".join(chunk)\n",
    "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "\n",
    "            for word in words:\n",
    "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
    "                global_freqs[\" \".join(byte_tokens)] += 1\n",
    "\n",
    "        # 2. Merge işlemleri\n",
    "        vocab = global_freqs\n",
    "        self.merges = []\n",
    "\n",
    "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.merges.append(best_pair)\n",
    "\n",
    "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
    "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "\n",
    "        self.merges_set = set(self.merges)\n",
    "        self._build_token_vocab()\n",
    "        self._build_lookup_tables()\n",
    "\n",
    "    def _get_stats(self, vocab: Counter) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Sembollerin eş frekanslarını hesaplar\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Counter) -> Counter:\n",
    "        \"\"\"En sık geçen çifti birleştirir\"\"\"\n",
    "        new_vocab = Counter()\n",
    "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
    "            new_vocab[new_word] = freq\n",
    "\n",
    "        return new_vocab\n",
    "\n",
    "    def encode(self, text: str, dropout: float = 0.0) -> List[int]:\n",
    "        \"\"\"Metni token ID'lerine çevirir\"\"\"\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
    "\n",
    "            # BPE merge\n",
    "            while len(tokens) > 1:\n",
    "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "                valid_pairs = [\n",
    "                    p for p in pairs\n",
    "                    if p in self.merges_set and random.random() > dropout\n",
    "                ]\n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
    "                merged_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "\n",
    "                tokens = new_tokens\n",
    "\n",
    "            for token in tokens:\n",
    "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Token ID'lerinden orijinal metni oluşturur\"\"\"\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        decoded_bytes = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.special_token_ids:\n",
    "                continue  # özel token'ları atla\n",
    "            try:\n",
    "                # Tüm token'ı 3'er 3'er parçala\n",
    "                bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
    "                decoded_bytes.extend(bytes_seq)\n",
    "            except ValueError:\n",
    "                pass  # bilinmeyen token varsa yoksay\n",
    "\n",
    "        try:\n",
    "            return bytes(decoded_bytes).decode('utf-8', errors='replace')\n",
    "        except Exception:\n",
    "            return \"Corrupted\"\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, prefix: str):\n",
    "        \"\"\"Modeli diske kaydeder\"\"\"\n",
    "        with open(prefix, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"merges\": self.merges,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"special_tokens\": self.special_tokens\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, prefix: str):\n",
    "        with open(prefix, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        merges = [tuple(m) for m in data[\"merges\"]]\n",
    "        obj = cls(\n",
    "            merges=merges,\n",
    "            vocab=data[\"vocab\"],\n",
    "            special_tokens=data[\"special_tokens\"]\n",
    "        )\n",
    "        # Eksik yapılandırmaları tamamla\n",
    "        obj._build_token_vocab()\n",
    "        obj._build_lookup_tables()\n",
    "        return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297e546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2944, 1822, 43, 104, 105, 36, 2848, 988, 36, 1180, 36, 1031, 2214, 101, 36, 1654, 105]\n",
      "Decoded: Çin'de yapılan bir araştırmaya göre\n"
     ]
    }
   ],
   "source": [
    "# Test için tokenizer'ı kontrol edin\n",
    "tokenizer = OptimizedByteLevelBPE.load_model(\"tokenizer.json\")\n",
    "test_text = \"Çin'de yapılan bir araştırmaya göre\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)  # Orijinal metni tam olarak geri almalı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b73292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_model, load_model\n",
    "\n",
    "# Model architecture\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -self.block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, top_k)\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "        context = torch.tensor(tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n",
    "        generated = self.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        return self.tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        # Model yapılandırmasını ayrı bir JSON dosyasına kaydet\n",
    "        config = {\n",
    "            'vocab_size': len(self.tokenizer.token_to_id),\n",
    "            'n_embd': self.token_embedding_table.embedding_dim,\n",
    "            'block_size': self.block_size,\n",
    "            'n_layer': len(self.blocks),\n",
    "            'n_head': len(self.blocks[0].sa.heads),\n",
    "            'tokenizer_config': {\n",
    "                'merges': self.tokenizer.merges,\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Config'i ayrı bir dosyaya kaydet\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, ensure_ascii=False)\n",
    "        \n",
    "        # Model ağırlıklarını safetensors formatında kaydet\n",
    "        save_model(self, filepath)\n",
    "        print(f\"Model saved to {filepath} and config to {config_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, filepath, device='cuda'):\n",
    "        # Config dosyasını yükle\n",
    "        config_path = filepath.replace('.safetensors', '_config.json')\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Tokenizer'ı oluştur\n",
    "        tokenizer = OptimizedByteLevelBPE(\n",
    "            merges=[tuple(m) for m in config['tokenizer_config']['merges']],\n",
    "            vocab=config['tokenizer_config']['vocab'],\n",
    "            special_tokens=config['tokenizer_config']['special_tokens']\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Modeli başlat\n",
    "        model = cls(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            n_embd=config['n_embd'],\n",
    "            block_size=config['block_size'],\n",
    "            n_layer=config['n_layer'],\n",
    "            n_head=config['n_head'],\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        \n",
    "        # Ağırlıkları yükle\n",
    "        load_model(model, filepath, strict=True)\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for batching\n",
    "def encode_texts(tokenizer, texts, dropout=0.0):\n",
    "    \"\"\"Her metni ayrı tokenize edip liste listesi döner.\"\"\"\n",
    "    encoded_texts = []\n",
    "    for txt in texts:\n",
    "        token_ids = tokenizer.encode(txt, dropout=dropout)\n",
    "        encoded_texts.append(token_ids)\n",
    "    return encoded_texts\n",
    "\n",
    "def get_batch(data, block_size, batch_size, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    data: 1D tensor, uzun token dizisi\n",
    "    block_size: model context window\n",
    "    batch_size: kaç tane örnek alınacak\n",
    "    \"\"\"\n",
    "    # 0..len(data)-block_size-1 arası rastgele başlangıç pozisyonları seç\n",
    "    max_start_idx = data.size(0) - block_size - 1\n",
    "    starts = torch.randint(0, max_start_idx, (batch_size,))\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for start in starts:\n",
    "        input_seq = data[start : start + block_size]\n",
    "        target_seq = data[start + 1 : start + block_size + 1]\n",
    "\n",
    "        # Eğer padding yapılacaksa buraya ekle (genelde 1D tokenlarda gerek yok)\n",
    "        inputs.append(input_seq.unsqueeze(0))\n",
    "        targets.append(target_seq.unsqueeze(0))\n",
    "\n",
    "    xb = torch.cat(inputs, dim=0)   # batch_size x block_size\n",
    "    yb = torch.cat(targets, dim=0)  # batch_size x block_size\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(train_data if split == 'train' else val_data, block_size, batch_size, pad_token_id)\n",
    "            _, loss = model(X.to(device), Y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it, warmup_iters=500, max_lr=1e-4, total_iters=10000):\n",
    "    if it < warmup_iters:\n",
    "        return max_lr * it / warmup_iters\n",
    "    elif it > total_iters:\n",
    "        return 0.0\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
    "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    block_size = 256 #1024\n",
    "\n",
    "    max_iters = 10000 #50000\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    eval_interval = 100\n",
    "    eval_iters = 200\n",
    "\n",
    "    n_embd = 768 #1024\n",
    "    n_head = 12 #16\n",
    "    n_layer = 8 #8\n",
    "\n",
    "    save_interval = 500\n",
    "\n",
    "    weight_decay = 1e-2\n",
    "    patience = 3\n",
    "\n",
    "    # Load and preprocess data\n",
    "    full_corpus = load_and_preprocess_data(max_samples=5000)\n",
    "\n",
    "    # Initialize or load tokenizer (benim yeni mantıkla)\n",
    "    tokenizer_path = \"tokenizer.json\"\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        print(\"Training tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE()\n",
    "        tokenizer.train(full_corpus, num_merges=3000, chunk_size=5000, verbose=True)\n",
    "        tokenizer.save_model(tokenizer_path)\n",
    "    else:\n",
    "        print(\"Loading pretrained tokenizer...\")\n",
    "        tokenizer = OptimizedByteLevelBPE.load_model(tokenizer_path)\n",
    "\n",
    "    pad_token_id = tokenizer.token_to_id.get('<pad>', None)\n",
    "    if pad_token_id is None:\n",
    "        # Eğer tokenizer'da <pad> yoksa elle ekle veya 0 yap\n",
    "        print(\"Tokenizer does not have <pad> token. Using 0 as pad_token_id.\")\n",
    "        pad_token_id = 0\n",
    "\n",
    "    # Tokenize each text separately (benim yeni encode_text fonksiyonum gibi)\n",
    "    def encode_text(text):\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            tokens.extend(tokenizer.encode(word))\n",
    "        return tokens\n",
    "\n",
    "    tokens = []\n",
    "    for text in tqdm(full_corpus, desc=\"Encoding texts\"):\n",
    "        tokens.extend(encode_text(text))\n",
    "    data = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    # Train/val split\n",
    "    n = int(0.9 * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    print(f\"Total tokens: {len(data)}\")\n",
    "    print(f\"Train data size: {len(train_data)}\")\n",
    "    print(f\"Val data size: {len(val_data)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    vocab_size = len(tokenizer.token_to_id)\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        block_size=block_size,\n",
    "        n_layer=n_layer,\n",
    "        n_head=n_head,\n",
    "        tokenizer=tokenizer,\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    is_first = True\n",
    "    for iter in range(max_iters):\n",
    "        # Learning rate schedule (varsa)\n",
    "        lr = get_lr(iter)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        xb, yb = get_batch(train_data, block_size, batch_size, pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, train_data, val_data, block_size, batch_size, eval_iters, pad_token_id)\n",
    "            train_loss = losses['train']\n",
    "            val_loss = losses['val']\n",
    "            print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
    "\n",
    "            if(is_first != True):\n",
    "              if iter % save_interval == 0:\n",
    "                  checkpoint_path = f\"checkpoints/checkpoint_{iter}.safetensors\"\n",
    "                  model.save_model(checkpoint_path)\n",
    "                  print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "              if val_loss < best_val_loss:\n",
    "                  best_val_loss = val_loss\n",
    "                  patience_counter = 0\n",
    "                  best_model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "                  model.save_model(best_model_path)\n",
    "                  print(f\"New best model saved to {best_model_path}\")\n",
    "              else:\n",
    "                  patience_counter += 1\n",
    "                  if patience_counter >= patience:\n",
    "                      print(\"Early stopping triggered.\")\n",
    "                      break\n",
    "            else:\n",
    "              is_first = False\n",
    "            \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559ca6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 500/500 [00:00<00:00, 9806.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 500/500 [00:02<00:00, 212.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 72112\n",
      "Train data size: 64900\n",
      "Val data size: 7212\n",
      "1.24M parameters\n",
      "Step 0: Train 8.2219, Val 8.2203, LR 0.000000\n",
      "Step 100: Train 7.6751, Val 7.6669, LR 0.000020\n",
      "Model saved to checkpoints/checkpoint_100.safetensors and config to checkpoints/checkpoint_100_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_100.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 200: Train 6.6092, Val 6.5999, LR 0.000040\n",
      "Model saved to checkpoints/checkpoint_200.safetensors and config to checkpoints/checkpoint_200_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_200.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 300: Train 5.8443, Val 5.8407, LR 0.000060\n",
      "Model saved to checkpoints/checkpoint_300.safetensors and config to checkpoints/checkpoint_300_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_300.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 400: Train 5.1762, Val 5.1809, LR 0.000080\n",
      "Model saved to checkpoints/checkpoint_400.safetensors and config to checkpoints/checkpoint_400_config.json\n",
      "Saved checkpoint to checkpoints/checkpoint_400.safetensors\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n",
      "Step 499: Train 4.7221, Val 4.7265, LR 0.000100\n",
      "Model saved to checkpoints/best_model/best_model.safetensors and config to checkpoints/best_model/best_model_config.json\n",
      "New best model saved to checkpoints/best_model/best_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d017e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoints/best_model/best_model.safetensors\"\n",
    "model = Transformer.load_model(model_path, device)\n",
    "\n",
    "prompt = \"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\"\n",
    "\n",
    "generated = model.generate_from_prompt(\n",
    "    prompt, \n",
    "    max_new_tokens=256, \n",
    "    temperature=0.5, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
