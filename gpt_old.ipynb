{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4969442f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4969442f",
        "outputId": "aa71bfe8-7a1e-4eea-d3b9-64529a24b998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8279ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8279ca",
        "outputId": "bfff6444-bd8f-4eff-806c-95246c02f59e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f79940f49d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9f22cdae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f22cdae",
        "outputId": "fdfe1059-1a16-4c44-9f3b-c402ecc7e9f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Data Preprocess: 100%|██████████| 5000/5000 [00:00<00:00, 10387.84sample/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000 data processed.\n",
            "Sample data: Çin'in Sichuan Eyaletinde yer alan Sichuan Dev Panda Barınakları, dünyadaki tehlike altındaki dev panda popülasyonunun %30'unu barındıran ve 9245 km2'lik bir alana yayılan park alanıdır.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from typing import List, Generator\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset = load_dataset(\"musabg/wikipedia-tr-summarization\", split='train')\n",
        "\n",
        "def clean_text(text: str, remove_numbers: bool = False) -> str:\n",
        "    \"\"\"Türkçe metni temizle ve normalize et.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def preprocess_data(batch_size: int = 1000, max_samples: int = 5000) -> Generator[List[str], None, None]:\n",
        "    \"\"\"Veri setini temizleyip işlenebilir hale getirir. max_samples kadar veri işler.\"\"\"\n",
        "    processed_texts = []\n",
        "\n",
        "    for i in tqdm(range(0, min(len(dataset), max_samples)), desc=\"Data Preprocess\", unit=\"sample\"):\n",
        "        summary = clean_text(dataset[i][\"summary\"])\n",
        "        processed_texts.append(summary)\n",
        "\n",
        "        if len(processed_texts) >= batch_size:\n",
        "            yield processed_texts\n",
        "            processed_texts = []\n",
        "\n",
        "    if processed_texts:\n",
        "        yield processed_texts\n",
        "\n",
        "\n",
        "\n",
        "data_generator = preprocess_data(max_samples=5000)\n",
        "\n",
        "full_corpus = []\n",
        "for batch in data_generator:\n",
        "    full_corpus.extend(batch)\n",
        "\n",
        "print(f\"{len(full_corpus)} data processed.\")\n",
        "print(\"Sample data:\", full_corpus[0])\n",
        "\n",
        "text = \" \".join(full_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0ed200ce",
      "metadata": {
        "id": "0ed200ce"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "class OptimizedByteLevelBPE:\n",
        "    def __init__(self, merges: Optional[List[Tuple[str, str]]] = None,\n",
        "                 vocab: Optional[Dict[str, int]] = None,\n",
        "                 special_tokens: Optional[List[str]] = None):\n",
        "        self.merges = merges or []\n",
        "        self.vocab = vocab or {}\n",
        "        self.special_tokens = special_tokens or ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "        self._build_lookup_tables()\n",
        "\n",
        "    def _build_lookup_tables(self):\n",
        "        \"\"\"Token ve ID eşlemelerini kurar\"\"\"\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "\n",
        "        # Özel token'lar\n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.token_to_id[token] = idx\n",
        "\n",
        "        offset = len(self.token_to_id)\n",
        "\n",
        "        # Byte token'lar (000 - 255)\n",
        "        for i in range(256):\n",
        "            byte_token = f\"{i:03d}\"\n",
        "            self.token_to_id[byte_token] = offset + i\n",
        "\n",
        "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
        "        self.special_token_ids = {tok: self.token_to_id[tok] for tok in self.special_tokens}\n",
        "        self.merges_set = set(self.merges)\n",
        "\n",
        "    def _build_token_vocab(self):\n",
        "        \"\"\"Merge sonrası oluşan token sözlüğünü oluşturur\"\"\"\n",
        "        tokens = set()\n",
        "        for a, b in self.merges:\n",
        "            tokens.add(a)\n",
        "            tokens.add(b)\n",
        "            tokens.add(a + b)\n",
        "        tokens = sorted(tokens)\n",
        "\n",
        "        # Token ID'lerini devam ettir\n",
        "        start_id = max(self.token_to_id.values()) + 1\n",
        "        for tok in tokens:\n",
        "            if tok not in self.token_to_id:\n",
        "                self.token_to_id[tok] = start_id\n",
        "                start_id += 1\n",
        "\n",
        "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
        "        self.vocab = {k: v for k, v in self.token_to_id.items() if k not in self.special_tokens}\n",
        "\n",
        "    def train(self, corpus: List[str], num_merges: int = 10000,\n",
        "              chunk_size: int = 10000, verbose: bool = True):\n",
        "        \"\"\"BPE algoritması ile tokenizer'ı eğitir\"\"\"\n",
        "        global_freqs = Counter()\n",
        "\n",
        "        # 1. Frekansları hesapla\n",
        "        for i in tqdm(range(0, len(corpus), chunk_size), desc=\"Vocabulary Construction\"):\n",
        "            chunk = corpus[i:i + chunk_size]\n",
        "            text = \" \".join(chunk)\n",
        "            words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
        "\n",
        "            for word in words:\n",
        "                byte_tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")] + [\"</w>\"]\n",
        "                global_freqs[\" \".join(byte_tokens)] += 1\n",
        "\n",
        "        # 2. Merge işlemleri\n",
        "        vocab = global_freqs\n",
        "        self.merges = []\n",
        "\n",
        "        for merge_step in tqdm(range(num_merges), desc=\"BPE Merging\"):\n",
        "            pairs = self._get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
        "            vocab = self._merge_vocab(best_pair, vocab)\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "            if verbose and (merge_step % 1000 == 0 or merge_step == num_merges - 1):\n",
        "                print(f\"Merge {merge_step + 1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
        "\n",
        "        self.merges_set = set(self.merges)\n",
        "        self._build_token_vocab()\n",
        "        self._build_lookup_tables()\n",
        "\n",
        "    def _get_stats(self, vocab: Counter) -> Dict[Tuple[str, str], int]:\n",
        "        \"\"\"Sembollerin eş frekanslarını hesaplar\"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "        return pairs\n",
        "\n",
        "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Counter) -> Counter:\n",
        "        \"\"\"En sık geçen çifti birleştirir\"\"\"\n",
        "        new_vocab = Counter()\n",
        "        pattern = re.compile(rf'(?<!\\S){re.escape(pair[0])} {re.escape(pair[1])}(?!\\S)')\n",
        "\n",
        "        for word, freq in vocab.items():\n",
        "            new_word = pattern.sub(pair[0] + pair[1], word)\n",
        "            new_vocab[new_word] = freq\n",
        "\n",
        "        return new_vocab\n",
        "\n",
        "    def encode(self, text: str, dropout: float = 0.0) -> List[int]:\n",
        "        \"\"\"Metni token ID'lerine çevirir\"\"\"\n",
        "        words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
        "        token_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            tokens = [f\"{b:03d}\" for b in word.encode(\"utf-8\")]\n",
        "\n",
        "            # BPE merge\n",
        "            while len(tokens) > 1:\n",
        "                pairs = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
        "                valid_pairs = [\n",
        "                    p for p in pairs\n",
        "                    if p in self.merges_set and random.random() > dropout\n",
        "                ]\n",
        "                if not valid_pairs:\n",
        "                    break\n",
        "\n",
        "                best_pair = min(valid_pairs, key=lambda p: self.merges.index(p))\n",
        "                merged_token = best_pair[0] + best_pair[1]\n",
        "\n",
        "                new_tokens = []\n",
        "                i = 0\n",
        "                while i < len(tokens):\n",
        "                    if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == best_pair:\n",
        "                        new_tokens.append(merged_token)\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_tokens.append(tokens[i])\n",
        "                        i += 1\n",
        "\n",
        "                tokens = new_tokens\n",
        "\n",
        "            for token in tokens:\n",
        "                token_ids.append(self.token_to_id.get(token, self.special_token_ids[\"<unk>\"]))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Token ID'lerinden orijinal metni oluşturur\"\"\"\n",
        "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
        "        decoded_bytes = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if token in self.special_token_ids:\n",
        "                continue  # özel token'ları atla\n",
        "            try:\n",
        "                if len(token) == 6:  # örn: \"196195\" gibi birleşmiş token\n",
        "                    bytes_seq = [int(token[i:i+3]) for i in range(0, len(token), 3)]\n",
        "                else:\n",
        "                    bytes_seq = [int(token)]\n",
        "                decoded_bytes.extend(bytes_seq)\n",
        "            except ValueError:\n",
        "                pass  # bilinmeyen token veya özel karakter varsa yoksay\n",
        "\n",
        "        try:\n",
        "            return bytes(decoded_bytes).decode('utf-8', errors='ignore')\n",
        "        except Exception:\n",
        "            return \"Corrupted\"  # bozulmuş veri varsa boş döndür\n",
        "\n",
        "\n",
        "    def save_model(self, prefix: str):\n",
        "        \"\"\"Modeli diske kaydeder\"\"\"\n",
        "        with open(prefix, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\n",
        "                \"merges\": self.merges,\n",
        "                \"vocab\": self.vocab,\n",
        "                \"special_tokens\": self.special_tokens\n",
        "            }, f, ensure_ascii=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, prefix: str):\n",
        "        \"\"\"Kaydedilmiş tokenizer modelini yükler\"\"\"\n",
        "        with open(prefix, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        merges = [tuple(m) for m in data[\"merges\"]]\n",
        "        return cls(\n",
        "            merges=merges,\n",
        "            vocab=data[\"vocab\"],\n",
        "            special_tokens=data[\"special_tokens\"]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cdb88069",
      "metadata": {
        "id": "cdb88069"
      },
      "outputs": [],
      "source": [
        "tokenizer = OptimizedByteLevelBPE()\n",
        "\n",
        "#tokenizer.train(\n",
        "#    full_corpus,\n",
        "#    num_merges=3000,\n",
        "#    chunk_size=5000,\n",
        "#    verbose=True\n",
        "#)\n",
        "#tokenizer.save_model('turkish_bpe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2af8bd4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2af8bd4a",
        "outputId": "6fda36a3-67f2-48a4-9dcb-74960a7db6a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "şçöğü\n"
          ]
        }
      ],
      "source": [
        "tokenizer.load_model('turkish_bpe_model.json')\n",
        "\n",
        "# Metni encode/decode etme\n",
        "encoded = tokenizer.encode(\"şçöğü\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(decoded)  # \"merhaba dünya\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d149295",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d149295",
        "outputId": "a44bfc54-5b9b-4b83-fbae-b218e5e26acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing: İstanbul'da şehir içi ulaşım çok karmaşık\n",
            "Success!\n",
            "\n",
            "Testing: Pijamalı hasta yağız şoföre çabucak güvendi\n",
            "Success!\n",
            "\n",
            "Testing: Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde\n",
            "Success!\n"
          ]
        }
      ],
      "source": [
        "test_cases = [\n",
        "    \"İstanbul'da şehir içi ulaşım çok karmaşık\",\n",
        "    \"Pijamalı hasta yağız şoföre çabucak güvendi\",\n",
        "    \"Fahiş fiyatlarla mücadele ederken güğümsü renkler içinde\"\n",
        "]\n",
        "\n",
        "for _text in test_cases:\n",
        "    print(f\"\\nTesting: {_text}\")\n",
        "    encoded = tokenizer.encode(_text)\n",
        "    decoded = tokenizer.decode(encoded)\n",
        "    print(\"Success!\" if decoded == _text else \"Failed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8492c871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8492c871",
        "outputId": "85ecbb64-1f66-4984-c66c-51d5254fc71e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoding with BPE: 100%|██████████| 355077/355077 [00:02<00:00, 129658.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total token: 1470241\n",
            "Train data size: 1323216\n",
            "Val data size: 147025\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def encode_text_with_bpe_ids(bpe_obj, text):\n",
        "    tokens = []\n",
        "    words = re.findall(r'\\S+|\\s+', text)\n",
        "    for word in tqdm(words, desc=\"Encoding with BPE\"):\n",
        "        tokens.extend(bpe_obj.encode(word))\n",
        "    return tokens\n",
        "\n",
        "tokens = encode_text_with_bpe_ids(tokenizer, text)\n",
        "data = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Total token: {len(data)}\")\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Val data size: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a6be6a07",
      "metadata": {
        "id": "a6be6a07"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "30df6a27",
      "metadata": {
        "id": "30df6a27"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "55cc2778",
      "metadata": {
        "id": "55cc2778"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "12a0b666",
      "metadata": {
        "id": "12a0b666"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, n_embd, block_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "443e9a48",
      "metadata": {
        "id": "443e9a48"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 2 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * n_embd, n_embd),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "68b268cc",
      "metadata": {
        "id": "68b268cc"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "d5b2d298",
      "metadata": {
        "id": "d5b2d298"
      },
      "outputs": [],
      "source": [
        "class Transformers(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=512, block_size=256, n_layer=6, n_head=8, tokenizer=None):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.block_size = block_size\n",
        "        self.tokenizer = tokenizer  # Injected tokenizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=30):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -self.block_size:]\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, top_k)\n",
        "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat([idx, next_token], dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def generate_from_prompt(self, prompt, max_new_tokens=100, temperature=1.0, top_k=30):\n",
        "        assert self.tokenizer is not None, \"Tokenizer must be provided\"\n",
        "        idx = torch.tensor([self.tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
        "        out = self.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        return self.tokenizer.decode(out[0].tolist())\n",
        "    \n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_path, tokenizer, **kwargs):\n",
        "        model = cls(tokenizer=tokenizer, **kwargs)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        return model\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "88b1f758",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "\n",
        "grad_clip = 1.0\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 3\n",
        "\n",
        "eval_iters = 200\n",
        "n_embd = 768\n",
        "n_head = 12\n",
        "n_layer = 8\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "vocab_size = len(tokenizer.token_to_id)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b92235",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38.395652 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = Transformers(vocab_size=vocab_size, n_embd=n_embd, block_size=block_size, n_layer=n_layer, n_head=n_head)\n",
        "model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fc4b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "def get_lr(it, warmup_iters=500, max_lr=1e-3, total_iters=5000):\n",
        "    if it < warmup_iters:\n",
        "        return max_lr * it / warmup_iters\n",
        "    elif it > total_iters:\n",
        "        return 0.0\n",
        "    else:\n",
        "        decay_ratio = (it - warmup_iters) / (total_iters - warmup_iters)\n",
        "        return max_lr * 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Learning rate scheduler\n",
        "    lr = get_lr(iter)\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr\n",
        "\n",
        "    # Değerlendirme ve log\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "\n",
        "        print(f\"Step {iter}: Train {train_loss:.4f}, Val {val_loss:.4f}, LR {lr:.6f}\")\n",
        "      \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(\"The new model is better than the old model. The best model has been updated.\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Eğitim adımı\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "ea3f867c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_14770/767988937.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve sonrasında Manisa'ya yerleşerek İstanbul'u ele geçirme seçeneklerini sağlamıştır. İranlı asker ve siyasetçi Ali Paşa, İran'da doğdu ve İran'da hayatını kaybetti; ayrıca Maliye Bakanlığı ve Karşılaştırma Enstitüsü gibi durumlar \n"
          ]
        }
      ],
      "source": [
        "model = Transformers.from_pretrained(\"best_model_wiki.pt\", tokenizer=tokenizer,\n",
        "                                     vocab_size=vocab_size, n_embd=n_embd,\n",
        "                                     block_size=block_size, n_layer=n_layer, n_head=n_head)\n",
        "\n",
        "text = model.generate_from_prompt(\"Çin'de yapılan bir araştırmaya göre Çin Seddi'nin yapımı tam 100 yıl sürmüştür ve\", max_new_tokens=256, temperature=0.5, top_k=30)\n",
        "print(text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
